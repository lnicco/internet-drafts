{
  "magic": "E!vIA5L86J2I",
  "timestamp": "2020-09-07T13:47:41.898669+00:00",
  "repo": "quiclog/internet-drafts",
  "labels": [
    {
      "name": "design",
      "description": "",
      "color": "1d76db"
    },
    {
      "name": "editorial",
      "description": "",
      "color": "0e8a16"
    },
    {
      "name": "high-level-schema",
      "description": "",
      "color": "7fd836"
    },
    {
      "name": "quic-http3-fields",
      "description": "",
      "color": "d6354a"
    }
  ],
  "issues": [
    {
      "number": 1,
      "id": "MDU6SXNzdWU0MjkyODkxNjc=",
      "title": "Allow flexible fields definitions",
      "url": "https://github.com/quiclog/internet-drafts/issues/1",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "design",
        "high-level-schema"
      ],
      "body": "The fields that are logged for each individual event depend on the context of usage of the format.\r\n\r\nE.g., if you split your logs per connection-id yourself, you do not need to log the connection-id for each event.\r\nHowever, if you do not log only QUIC data, but also ICMP/TCP info (e.g., the in-network measurement use-case), you need an additional field \"protocol-type\".\r\n\r\nProposal:\r\nAllow common fields to be set in each \"trace\" header.\r\n\r\n```\r\n{\r\n     \"common_fields\": [ \"connection_id\": \"0xdeadbeef\", \"protocol-type\": \"QUIC\" ],  \r\n     \"field_headers: [\"timestamp\", \"category\", \"type\", \"trigger\", \"data\"],  \r\n     \"events\": [ ... ]  \r\n}\r\n```\r\n\r\nIf you do not log a field, you just leave it out of both common_fields and field_headers. \r\n",
      "createdAt": "2019-04-04T13:46:52Z",
      "updatedAt": "2019-10-14T09:31:32Z",
      "closedAt": "2019-10-14T09:31:32Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Present in draft-01 by allowing fields in either common_fields or in event_fields.",
          "createdAt": "2019-10-14T09:31:32Z",
          "updatedAt": "2019-10-14T09:31:32Z"
        }
      ]
    },
    {
      "number": 2,
      "id": "MDU6SXNzdWU0MjkzNTQzMDY=",
      "title": "Streaming format",
      "url": "https://github.com/quiclog/internet-drafts/issues/2",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "design",
        "high-level-schema"
      ],
      "body": "Plain JSON is not entirely streamable... it requires its fields to be closed properly (by ] and }) at the end.\r\n\r\ne.g.,\r\n```\r\n\"events\": [\r\n           { \"key\": \"val\" }\r\n```\r\nWill fail, but \r\n```\r\n\"events\": [\r\n           { \"key\": \"val\" }\r\n]\r\n```\r\nwill succeed. \r\n\r\nHowever, one could employ a streaming JSON parser (e.g., http://oboejs.com/) and ignore unclosed fields at the end that way. \r\nFor the way the format is currently defined, an implementation would then write the \"header\" of the qlog file, and then it could stream individual events that are just appended to the file. \r\nIf the file isn't properly closed, that's not a problem: the streaming parser user just ignores those errors.\r\nHowever, this breaks compatibility with the built-in parsers in many stdlibs and the browsers themselves.\r\nIt would also be possible to write a simple postprocessing tool that simply appends the necessary \"closing\" characters to a qlog file if it isn't complete, though that adds another step in a pipeline... \r\n\r\nThere are also various JSON-subformats that address this problem (see https://en.wikipedia.org/wiki/JSON_streaming), but they also do not seem to be supported in the standard JSON parsers for many platforms...\r\n\r\n**Given all this, my personal preference is to stay with just the normal JSON format and tools are recommended to use a streaming parser.** \r\n\r\n\r\nExample of how the browser's built-in JSON parser does not support special JSON:\r\n![2019-04-04 17_02_41](https://user-images.githubusercontent.com/2240689/55566226-8df5ff80-56fb-11e9-9a60-ab0fe703cf27.png)\r\n\r\nExample of how a streaming parser (oboe) does handle this properly:\r\n![proof](https://user-images.githubusercontent.com/2240689/55569216-77eb3d80-5701-11e9-86af-2b1a5b79ea2f.png)\r\n\r\n\r\n\r\n",
      "createdAt": "2019-04-04T15:47:41Z",
      "updatedAt": "2020-09-05T16:15:36Z",
      "closedAt": "2020-09-05T16:15:36Z",
      "comments": []
    },
    {
      "number": 3,
      "id": "MDU6SXNzdWU0Mjk3MTAwNDc=",
      "title": "Aggregated metrics",
      "url": "https://github.com/quiclog/internet-drafts/issues/3",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "design"
      ],
      "body": "Example: in-network logger aggregates over a period of time and sends back summarized data in 1 go (e.g., average RTT in past 10s, measured over 10k packets).\r\n\r\nThis can be supported in a variety of ways.\r\n\r\nThe easiest would probably be a new EVENT type that just contains the aggregated metrics.\r\nHowever, this depends on which types of aggregated data you want to pass. \r\n\r\ne.g., saying: median RTT was 50ms over the past 5s and we saw 1k packets in that time is fine\r\n\r\ne.g., saying: median RTT was 50ms across these 5 connections identified by these 4-tuples, is not really adhering to the semantics of the original setup.\r\n\r\nWe need more input from the people doing aggregated use cases to see how this data would be used and which types of metadata is needed to make an informed decision. ",
      "createdAt": "2019-04-05T11:30:41Z",
      "updatedAt": "2020-09-07T13:18:00Z",
      "closedAt": "2020-09-07T13:18:00Z",
      "comments": []
    },
    {
      "number": 4,
      "id": "MDU6SXNzdWU0Mjk3MjQzNzc=",
      "title": "Readability vs file size",
      "url": "https://github.com/quiclog/internet-drafts/issues/4",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "design"
      ],
      "body": "We would like to keep the file (semi-) human readable.\r\nThis means more advanced techniques like listing all fields in an ENUM up-front and then referencing to them by index is not optimal.\r\n\r\nHowever, when repeating strings, we might want to limit the length of individual fields. \r\ne.g., TRANS instead of TRANSPORT, APP instead of APPLICATION, RX instead of RECEIVE, etc. \r\n\r\nCurrent numbers (with readable strings):\r\n- qlog version without whitespace is about 4x the size of the binary .qtr file for the quic-trace example file (3.5MB to 823KB)\r\n- qlog version with whitespace is 10.6MB. However, this should matter little, since if manually reviewing large logs, you'll probably use a text editor that can auto-format the json from the version without whitespace\r\n\r\n4x size difference is a lot, but better than the direct .json protobuf transform, which clocks in at 14MB. ",
      "createdAt": "2019-04-05T12:11:32Z",
      "updatedAt": "2020-09-05T16:19:14Z",
      "closedAt": "2020-09-05T16:19:14Z",
      "comments": []
    },
    {
      "number": 5,
      "id": "MDU6SXNzdWU0Mjk3MzAwOTY=",
      "title": "Decide upon a language to define the schema definition",
      "url": "https://github.com/quiclog/internet-drafts/issues/5",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "We need to indicate various things, such as field types (int, string, ...), whether fields are required or optional, give example values for fields, etc.\r\n\r\nWe could use normal JSON-schema for this, but this is quite verbose...\r\nWe could also use TypeScript, though this is non-standard...\r\nOther RFCs are known to use their own specific languages for this (i.e., see TLS 1.3's type definitions), but maybe there is something workable already out there. \r\n\r\nCurrently, we use TypeScript format, since this is used in the Quicker prototype qlog implementation directly + is easy enough to parse for newcomers. ",
      "createdAt": "2019-04-05T12:26:48Z",
      "updatedAt": "2020-09-05T16:19:46Z",
      "closedAt": "2020-09-05T16:19:46Z",
      "comments": []
    },
    {
      "number": 6,
      "id": "MDU6SXNzdWU0Mjk3NzczOTI=",
      "title": "Define the semantics of RX and TX for NETWORK vantage point",
      "url": "https://github.com/quiclog/internet-drafts/issues/6",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "design",
        "high-level-schema"
      ],
      "body": "For CLIENT and SERVER, the difference between RECEIVE (RX) and TRANSMIT (TX) is obvious. Not so for an in-network observer (or, e.g., a proxy server), where these terms make less sense...\r\n\r\nSome options:\r\n- Type is NETWORK_CLIENT and NETWORK_SERVER (instead of NETWORK)\r\n- add separate \"flow\" field indicating if we use CLIENT or SERVER semantics (currently in the draft)\r\n- add separate metadata field indicating which 5-tuple is the conceptual \"client\" and which is the \"server\" and use RX/TX based on that\r\n- Don't fix this and let the tooling layer figure it out (if packet nr 6 is a client TX and a RX in the NETWORK trace, the network is from the viewpoint of the SERVER)\r\n\r\nBroader discussion: does it make sense to log packets as PACKET_TX and _RX here? how about instead a PACKET event (similar to how wireshark does it). However: this doesn't make sense for (stateful) proxies that do act as both client+server when ~transforming the traffic (e.g., Facebook's load balancer). \r\n",
      "createdAt": "2019-04-05T14:15:11Z",
      "updatedAt": "2019-10-14T09:30:47Z",
      "closedAt": "2019-10-14T09:30:47Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Solved in draft-01 by using separate _sent and _received (or equivalent) events for clarity + using vantage_point and their \"flow\" field.",
          "createdAt": "2019-10-14T09:30:47Z",
          "updatedAt": "2019-10-14T09:30:47Z"
        }
      ]
    },
    {
      "number": 7,
      "id": "MDU6SXNzdWU0Mjk4MjIyODc=",
      "title": "Use cases for the TRIGGER field ",
      "url": "https://github.com/quiclog/internet-drafts/issues/7",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "design",
        "high-level-schema"
      ],
      "body": "To have the TRIGGER as a top-level field, there need to be good use-cases and people willing to use this in their tools. \r\n\r\nSince the TRIGGER will only be useful for specific events (e.g., PACKET_RETRANSMIT can be due to several loss-detection related situations) and it's value might in some cases also just be deduced from the context of surrounding log messages, it is debatable it will have much use in practice. \r\n\r\nAn alternate approach could be to log it as part of the DATA field of specific events, instead of as a top-level field. ",
      "createdAt": "2019-04-05T15:48:06Z",
      "updatedAt": "2019-10-14T09:36:11Z",
      "closedAt": "2019-10-14T09:36:11Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Feedback from among others @nibanks indicates that adding the TRIGGER as an optional member of the DATA is probably the better option down the road.\r\n\r\nWe could think about extending that: any event_field could conceptually be added to data. This would be useful for other event_fields as well, that are not the same for all events (which would be in common_fields) but also don't need to be logged for each event (event_fields). Maybe something like dynamic_fields? and then you do data.dynamic to fetch them? (e.g., data.dynamic.trigger). This is nice an flexible, but a potential nightmare to support properly in tooling...",
          "createdAt": "2019-07-31T13:47:10Z",
          "updatedAt": "2019-07-31T13:47:10Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Fixed in #23. Triggers are now properties of the data field with hints in the draft as to their values in separate contexts. \r\n\r\nDecided not to do the \"dynamic_fields\" approach for now to keep complexity manageable. ",
          "createdAt": "2019-10-14T09:36:11Z",
          "updatedAt": "2019-10-14T09:36:11Z"
        }
      ]
    },
    {
      "number": 8,
      "id": "MDU6SXNzdWU0Mjk4Mzg0ODU=",
      "title": "Allow index-based referencing for all event field names",
      "url": "https://github.com/quiclog/internet-drafts/issues/8",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "design",
        "high-level-schema"
      ],
      "body": "Currently, we have a concept that you can have a \"groups_id\" object in \"common_fields\". \r\nIf you then have a \"group_id\" in your \"events_field\", the value for that with each event is an index into the \"groups_id\" field, to prevent replication of complex fields.\r\n\r\nWe could make this more general, applicable to each field name.\r\nE.g., if you know up-front which CATEGORY values you support, you could do something like:\r\n\r\n```\r\n{\r\n    \"common_fields\": {\r\n        \"group_id\": \"127ecc830d98f9d54a42c4f0842aa87e181a\",\r\n        \"ODCID\": \"127ecc830d98f9d54a42c4f0842aa87e181a\",\r\n        \"CATEGORY\": [\"PACKET_RX\", \"DATA_FRAME_NEW\"]\r\n        \"protocol_type\":  \"QUIC_HTTP3\",\r\n        \"reference_time\": \"1553986553572\"\r\n    },\r\n    \"event_fields\": [\r\n        \"delta_time\",\r\n        \"CATEGORY\",\r\n        \"EVENT_TYPE\",\r\n        \"TRIGGER\",\r\n        \"DATA\"\r\n    ],\r\n    \"events\": [[\r\n            2,\r\n            \"TRANSPORT\",\r\n            0,\r\n            \"LINE\",\r\n            [...]\r\n        ],[\r\n            7,\r\n            \"APPLICATION\",\r\n            1,\r\n            \"GET\",\r\n            [...]\r\n        ],\r\n        ...\r\n    ]\r\n}\r\n```\r\n\r\nWe would then have a general rule:\r\n\r\n> If the field is present as a value of type array in \"common_fields\" AND the field-name is present in \"event_fields\", the value per-event MUST be treated as an index into the \"common_fields\" entry, rather than taken as a direct value. \r\n\r\n(\"groups_id\" would then also be renamed to \"group_id\" in \"common_fields\")\r\n\r\nThis would allow smaller file sizes (and less string writing overhead) for applications that have a static list of e.g., CATEGORY or EVENT_TYPE up front.\r\nDownside 1 is that it complicates the tools reading the file (but only a bit imo).\r\nDownside 2 is that is complicates humans reading the file (so it depends on the use case).\r\n   (either way, it's easy to go from 1 to the other with a simple script)\r\n\r\n-----------------------------------------------------------------------------------\r\n\r\nThis concept could be extended to make it a fully self-describing format.\r\nIn other words, we could also describe the fields in the DATA for known events up-front and replace those entries with in-order arrays of the values instead of key-value object definitions.\r\n\r\nVery high-level concept (probably needs proper description of the fields etc.):\r\n```\r\n\"data_fields\" : {\r\n      \"TRANSPORT+PACKET_SENT\" : [\r\n         \"frame_type\",\r\n         \"packet_number\",\r\n         \"frames\"\r\n     ]\r\n}\r\n...\r\n[ 57, \"TRANSPORT\", \"PACKET_SENT\", \"TRIGGER\", [\"STREAM\", 15, [...]]]\r\n```\r\nTaking this all to the extreme, you could have a fully self-describing format that lists all known events (and potentially some values, similar to QPack's static table) up-top and then each entry just uses indexes + potentially a few raw values. However, I'm personally not of the opinion this added complexity is worth it. \r\n\r\n",
      "createdAt": "2019-04-05T16:27:53Z",
      "updatedAt": "2019-10-14T09:29:52Z",
      "closedAt": "2019-10-14T09:29:52Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "In the interest of keeping things simple and because there are few situations where this has turned up in the meantime, I've decided for now to only allow this for \"group_id\". Further fields that benefit from this can be added per-instance. \r\n\r\nNote that Chrome's netlog format does this, and it severely compromises the user's ability to understand and grep those files, despite them using .json as a substrate as well.",
          "createdAt": "2019-10-14T09:29:52Z",
          "updatedAt": "2019-10-14T09:29:52Z"
        }
      ]
    },
    {
      "number": 9,
      "id": "MDU6SXNzdWU0MzA1MDE2ODM=",
      "title": "How descriptive should EVENT_TYPE names be?",
      "url": "https://github.com/quiclog/internet-drafts/issues/9",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "design",
        "quic-http3-fields"
      ],
      "body": "For example:\r\nPACKET_RECEIVED        vs       1RTT_PACKET_RECEIVED\r\nFRAME_NEW                vs        ACK_FRAME_NEW\r\n\r\nFor the leftmost entries, one would add a \"type\" field to the \"DATA\" value, e.g., \r\n```\r\n{\r\n     \"type\": \"1RTT\"\r\n}\r\n```\r\n\r\nThe shorter form makes that we have a much less large amount of different EVENT_TYPEs, but also makes it a bit harder to parse for human readers + harder to quickly filter for tools. \r\nThe longer form is much more explicit, but requires much more definition up-front and a proliferation of different EVENT_TYPEs.\r\n\r\nWe could also break consistency. i.e., the original qlog used PACKET_RECEIVED with an explicit type in the DATA, but used ACK_FRAME_NEW for individual frames.\r\n\r\nCurrently, we use the short-form, since this is most similar to quic-trace and keeps it consistent if we want to log frames both in their own events and again when sending a packet. \r\n\r\nExtra edge-case: Errors\r\nIf you go for extreme short-form, you would just have a single ERROR EVENT_TYPE for each CATEGORY, and define the error type in the DATA.\r\nHowever, for easier manual debugging, tracking the specific type of error directly in the EVENT_TYPE is arguably easier. Maybe an exception should be made for errors? ",
      "createdAt": "2019-04-08T15:01:36Z",
      "updatedAt": "2019-10-07T19:46:27Z",
      "closedAt": "2019-10-07T19:46:26Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "In draft-01, we made the conscious choice to limit the number of events as much as possible and make most event data based on the Frame definitions that already existed for packet_sent and packet_received. Combined with proper naming of properties (e.g., packet_type instead of type) this enables fast parsing while removing the need for separate events for each possible signal.",
          "createdAt": "2019-10-07T19:46:26Z",
          "updatedAt": "2019-10-07T19:46:26Z"
        }
      ]
    },
    {
      "number": 10,
      "id": "MDU6SXNzdWU0MzA5NTQyMzE=",
      "title": "Numbers in JSON",
      "url": "https://github.com/quiclog/internet-drafts/issues/10",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "editorial",
        "quic-http3-fields"
      ],
      "body": "Typically, integers in JavaScript and most JSON implementations are limited to 2^53-1.\r\nThis gives problems, as the VLIE values in the QUIC/H3 specs are 2^62-1.\r\n\r\nTwo options:\r\n- Allow bigger values in the JSON. Tools MUST use a JSON parser that can deal with this and a JavaScript engine that supports BigInt (currently limited to Chromium: https://caniuse.com/#search=bigint)\r\n- Encode all VLIE fields as strings. Tools have to deal with this themselves (most will probably just take the shortcut of assuming actual values will be < 2^53 and just use the JavaScript \"Number\" type). This is best for a wide tooling implementation area in browsers. \r\n\r\nCurrently, the draft uses option 2. \r\n\r\n",
      "createdAt": "2019-04-09T13:00:45Z",
      "updatedAt": "2020-09-05T16:19:32Z",
      "closedAt": "2020-09-05T16:19:32Z",
      "comments": [
        {
          "author": "jlaine",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Firefox and Edge (obviously) also suport BigInt according to caniuse. The only significant outlier is Safari.\r\n\r\nHowever you're right, JSON serialization / parsing isn't there yet:\r\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/BigInt#Use_within_JSON",
          "createdAt": "2019-08-23T10:57:13Z",
          "updatedAt": "2019-08-23T10:59:25Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "NONE",
          "body": "An alternative would be to define a completely different interface.\r\n\r\nJSON is an odd choice because what generally happens here is that you have a stream of events.  Constructing that as a JSON array is awkward as it interacts poorly with typical JSON processing pipelines.  You might use JSON text sequences, but that is an odd line.\r\n\r\nCSV has some nice properties: you define the first column as the event type and remaining fields dependent on the type.  Then each record is delineated by something easy to produce (CRLF) and fields are easily recoverable.  Everything is a string then and you can define processing for number fields.",
          "createdAt": "2019-11-19T04:00:59Z",
          "updatedAt": "2019-11-19T04:00:59Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "As pointed out by @marten-seemann, the JSON spec itself does not limit the numbers to 2^53-1, it is just the implementations.\r\n\r\nAs such, for the purposes of the qlog spec, and if we stay with JSON, we can simply require clients to be able to deal with larger numbers in one of several ways (e.g., either detect and discard, detect and notify user, ignore, use parser that can handle up to 64 bit).\r\n\r\nI am thinking of switching to option 1 (from the first post in this issue) for draft-02. \r\n\r\n\r\n",
          "createdAt": "2020-01-19T10:45:33Z",
          "updatedAt": "2020-01-19T10:45:33Z"
        }
      ]
    },
    {
      "number": 11,
      "id": "MDU6SXNzdWU0MzQ2NzkxMTA=",
      "title": "Allow raw logging of packets and/or frames",
      "url": "https://github.com/quiclog/internet-drafts/issues/11",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [
        "design",
        "quic-http3-fields"
      ],
      "body": "As mentioned by @kazuho on the mailing list, it would be useful to (re-)introduce the ability to log raw packet and/or frame contents, probably has a hex-encoded string. \r\n\r\nProbably easiest to add an additional field like this:\r\n```\r\n{\r\n    \"stream_type\": \"ACK\",\r\n    \"raw_hex\": \"0892e340dbaa354f800239dddc7be78406fe3726bea050bb8c56ab36\",\r\n    ...\r\n}\r\n```",
      "createdAt": "2019-04-18T09:37:52Z",
      "updatedAt": "2019-10-14T09:27:43Z",
      "closedAt": "2019-10-14T09:27:43Z",
      "comments": [
        {
          "author": "kazuho",
          "authorAssociation": "NONE",
          "body": "Thank you for opening the issue.\r\n\r\nCan we also omit the \"stream_type\" attribute, because that would be obvious from the first byte of the binary? So something like just `{\"raw_hex\":\"...\"}` or just the hex string itself.",
          "createdAt": "2019-04-18T23:58:00Z",
          "updatedAt": "2019-04-18T23:58:00Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Omitting the stream_type is indeed possible.\r\nDropping the \"raw_hex\" would require a move to an array (i.e., [ ]) rather than an object (i.e., { }) literal.\r\nThis is something we might want to allow for issue #8 as well, so that might fit. \r\n\r\nJust to be sure what we're talking about:\r\nWe would have a mixed JSON file format with some events (e.g., recovery-related stuff) being logged in full, and the packet/frame level events being logged as binary hex strings for post-processing, right? See the example below.\r\n\r\n```\r\n\"events\": {\r\n    [48, \"TRANSPORT\", \"PACKET_RECEIVED\", \"DEFAULT\", [\"08277abefc43c25eca0892e340dbaa354f800239dddc7be78406fe3726bea050bb8c56ab36\"] ],\r\n    [49, \"TRANSPORT\", \"FRAME_RECEIVED\", \"DEFAULT\", [\"0892e340dbaa354f800239dddc7be78406fe3726bea050bb8c56ab36\"] ],\r\n    [50, \"RECOVERY\", \"METRIC_UPDATE\", \"ACK_RECEIVED\", {\"min_rtt\": 50, \"smoothed_rtt\": 62} ],\r\n}\r\n```\r\nDoes that fit with what you had in mind? (I have now included both full packet and separate frame logs, obviously we could also just do the packet only)\r\n",
          "createdAt": "2019-04-19T09:57:21Z",
          "updatedAt": "2019-04-19T09:58:45Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Lacking further follow-up on this, I've added \"raw\" or \"raw_encrypted\"/\"raw_decrypted\" fields where appropriate to draft-01. I do not want to require tools to deal with situations where only the \"raw\" fields are present (i.e., having to include a full compliant QUIC and H3 parser in each tool), so you'd have to write a separate transformer that takes the raw stuff and transforms it into \"proper qlog\" before putting it in a tool, but I feel it's a good compromise personally.",
          "createdAt": "2019-10-14T09:27:43Z",
          "updatedAt": "2019-10-14T09:27:43Z"
        }
      ]
    },
    {
      "number": 13,
      "id": "MDU6SXNzdWU0NzE3NzQ4MDQ=",
      "title": "Invalid Assumptions in packet_sent triggers",
      "url": "https://github.com/quiclog/internet-drafts/issues/13",
      "state": "CLOSED",
      "author": "nibanks",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "The `packet_sent`'s `triggers` field makes the assumptions that a packet is sent because a previous packet is being retransmitted. For instance, in winquic a connection has a queue/set of what data needs to be sent. When data is (suspected) lost, the data in the packet is added back to the queue. Similarly for PTO, we mark an outstanding packet as lost, and if we don't have any, queue a PING frame.\r\n\r\nThe send logic just grabs data from the queue/set and builds up packets to be sent. There is no direction relationship between different packets.\r\n\r\nSo, IMO, triggers are the reason data is queued to be sent, not actually sent. What is actually sent will depend on the entire state of the send queue at the time the send logic actually executes.\r\n\r\nFor example, assume you have two outstanding packets that end up getting marked as lost:\r\n\r\n```\r\n  PktNum=1 { STREAM ID=1, Offset=0, Length=100 }\r\n  PktNum=2 { STREAM ID=1, Offset=100, Length=100 }\r\n```\r\n\r\nBoth are marked as lost. Around the same time, the app queues another 100 bytes on stream 1 to be sent. Then another packet ends up getting sent:\r\n\r\n```\r\n  PktNum=55 { STREAM ID=1, Offset=0, Length=300 }\r\n```",
      "createdAt": "2019-07-23T15:45:02Z",
      "updatedAt": "2019-10-07T17:03:31Z",
      "closedAt": "2019-10-07T17:03:31Z",
      "comments": [
        {
          "author": "nibanks",
          "authorAssociation": "NONE",
          "body": "As a follow up, I believe the packet_lost and packet_acknowledged events should be in the transport section. Also, the packet_retransmit should be removed.",
          "createdAt": "2019-07-23T15:48:54Z",
          "updatedAt": "2019-07-23T15:48:54Z"
        }
      ]
    },
    {
      "number": 14,
      "id": "MDU6SXNzdWU0NzE3Nzk5MTc=",
      "title": "Payload for packet_dropped",
      "url": "https://github.com/quiclog/internet-drafts/issues/14",
      "state": "CLOSED",
      "author": "nibanks",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "First, it looks like there is no payload for this event. Is that expected? It's a hard problem. Practically, there is only one case in which you drop a packet, post decryption success, and that is because it's a duplicate packet number. Other than that, all other drop events would occur before a packet is decrypted. If it can't be decrypted you don't know the packet number, which would likely be the most interesting payload of this event. So, therefor it likely isn't too useful in having the packet number as payload.\r\n\r\nSo, in absence of including the packet number as payload, it might just be worth having a \"reason\" which is a string. That's what winquic has already at least.",
      "createdAt": "2019-07-23T15:54:46Z",
      "updatedAt": "2019-10-07T17:03:30Z",
      "closedAt": "2019-10-07T17:03:30Z",
      "comments": []
    },
    {
      "number": 15,
      "id": "MDU6SXNzdWU0NzI4MTg2NjI=",
      "title": "Specify time units used for ack_delay",
      "url": "https://github.com/quiclog/internet-drafts/issues/15",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "And probably for other non-timestamp time values (like RTT)\r\n\r\nOptions:\r\n- Force people to use the resolution set in the \"configuration\"\r\n- Choose a fixed resolution (always milli or always micro)\r\n- Allow people to indicate resolution inside .data of each event \r\n- Combination: default is milli, add config parameter to specify, allow overrides in the .data, etc.\r\n\r\nThanks @jlaine for reporting.",
      "createdAt": "2019-07-25T11:43:19Z",
      "updatedAt": "2019-10-14T09:24:55Z",
      "closedAt": "2019-10-14T09:24:55Z",
      "comments": [
        {
          "author": "jlaine",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I'd say the first option \"force people to use the resolution set in configuration\".",
          "createdAt": "2019-08-23T10:54:57Z",
          "updatedAt": "2019-08-23T10:54:57Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Fixed by 3c09877 through the \"first option\"",
          "createdAt": "2019-10-14T09:24:55Z",
          "updatedAt": "2019-10-14T09:24:55Z"
        }
      ]
    },
    {
      "number": 16,
      "id": "MDU6SXNzdWU0NzMwNTQ1NDU=",
      "title": "Support partial logs",
      "url": "https://github.com/quiclog/internet-drafts/issues/16",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "@nibanks mentioned that the winquic implementation does logging in a circular buffer. If this runs out of space, the earliest logs (e.g., the ip addresses, initial connection ids, etc. might have been dropped).\r\n\r\nWe can't really force tools to support this, but potentially we can add text in the draft so people know they should take this into account.",
      "createdAt": "2019-07-25T20:26:05Z",
      "updatedAt": "2019-10-14T09:24:28Z",
      "closedAt": "2019-10-14T09:24:28Z",
      "comments": []
    },
    {
      "number": 17,
      "id": "MDU6SXNzdWU0NzMwNTg0NzU=",
      "title": "Simplify / fix group_id usage",
      "url": "https://github.com/quiclog/internet-drafts/issues/17",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Right now, the group_id concept is -very- flexible. It can be in common_fields (e.g., 1 trace per connection) but it can also be in event_fields (combining several connections into 1 trace). That already puts quite a burden on tools to support this different approaches.\r\n\r\nThen, another problem comes up if you would have the same group_id across multiple traces (e.g., trace 1 has some events for group_id x, but trace 2 has some group_id x events as well.)\r\n\r\nNote: This concept was mainly added to support the spindump use case (https://github.com/EricssonResearch/spindump), CC @jariarkko. There, a network intermediary logs (aggregated) data on many connections and even protocols. It would be tedious for that setup to split everything out into separate traces. \r\n\r\nPossible solutions I currently see:\r\n- Only allow 1 of the options (e.g., group_id only in common_fields or only in event_fields). I'm not a big fan of this (common only is inflexible, event_fields only has much overhead)(also: common can be seen as a special case of event_fields, so that could be the implementation target)\r\n- Disallow the same group_id across different traces: I think this makes a lot of sense, my preference\r\n- Discard the whole group_id concept alltogether (in practice, this would lead to many different approaches in different tools. basically the same as event_fields only, only no standard way of calling the thing)\r\n\r\nAdditional suggestion: rename group_id to \"luid\" (locally unique identifier)\r\n\r\nThanks to @nibanks for reporting this",
      "createdAt": "2019-07-25T20:35:32Z",
      "updatedAt": "2020-09-05T16:21:06Z",
      "closedAt": "2020-09-05T16:21:06Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Thinking about this some more and having implemented splitting traces on group_id in quicvis, I feel allowing group_ids across traces is still the best way to go. \r\n\r\nIf you're logging on multiple network intermediaries (so multiple vantage points) at once and then merge those logs, you will always have the same group_id split over multiple traces. However, each of those traces SHOULD then represent a different vantage_point. So the real restriction should be: cannot spread events from the same vantage point across different traces within the same qlog file. Put differently: each trace should contain only events from a single vantage point. \r\n\r\nAs such, we might rename group_id to flow_id instead, since that makes the semantics a bit clearer. That would say \"this event belongs to flow with flow_id x, as observed from entity y\".\r\n\r\nI am trying to think of a reason why you would want to combine events from different vantage points into the same trace, but can't seem to find a use case. Either way, that would require changing up how we define vantage_point now, since it's per-trace and not part of common_fields or event_fields. \r\n\r\nAny thoughts @nibanks? ",
          "createdAt": "2019-10-02T12:01:53Z",
          "updatedAt": "2019-10-02T12:01:53Z"
        },
        {
          "author": "nibanks",
          "authorAssociation": "NONE",
          "body": "Are there really people signed up to trace multiple different vantage points and put them all in the same file? I don't know about other companies, but getting logs from more than one machine all into the same file is practically impossible for the Windows scenario. Would it be so bad that the tools need to parse a file per vantage point?\r\n\r\nI want qlog to succeed, but the more complicated it it, the less the chance I see that of becoming a reality. IMO, this is a place where simplicity should win.",
          "createdAt": "2019-10-02T14:05:51Z",
          "updatedAt": "2019-10-02T14:09:23Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "1) About the use case of having multiple traces in 1 file, I'm not directly thinking of the Windows scenario or using this in production, but more about use cases such as research, education and case studies. There, it's handy to be able to group everything needed for a single \"context\" in a single file to be shared and interpreted easily. For production, automated gathering or aggregation in separate datastores is certainly also possible. The current setup does not prevent you loading separate files, 1 per vantage point, either.  \r\n\r\n2) I'm not sure how only allowing a single trace from a single vantage point per file would help the original problem that it's difficult for tool developers to deal with group_ids occurring across traces... client and server will still have events with the same group_id (e.g., ODCID)\r\n\r\n As such, I'm not 100% sure what you're proposing? Do you want to do away with the \"traces\" array and replace it with a single \"trace\" per qlog file? Or do you want to do away with group_id at the \"event_fields\" level, requiring each individual trace to only contain events from a single connection?\r\n\r\nIn the latter case, I think that would actually be an obstacle to adoption, since currently several implementers are simply logging all events on the server in a single trace, tagged with ODCID for later splitting. This is generally much simpler than generating a single file (or trace) per connection on the server. \r\n\r\nIf you want to do away with group_id completely, that would leave out a whole bunch of other use cases, e.g., in-network observers like spindump (https://github.com/EricssonResearch/spindump, CC @jariarkko, @ihlar). This might be good enough for the QUIC use case (though barely), but not if qlog would grow to a more flexible format. \r\n\r\nFor draft-01, I've decided to keep the setup as-is, since there are users employing group_ids already (e.g., quant). I did specify the intended uses a bit more and am certainly open to more discussion on this design. Will you be in Singapore, @nibanks?",
          "createdAt": "2019-10-14T08:58:25Z",
          "updatedAt": "2019-10-14T08:58:25Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "the `group_id` field was not seeing much use in practice and as discussed above had some issues.\r\n\r\nIt has been considerably simplified for draft02, from f5db7cdc8cd0cf37bfe5f1b0b4c54fc56ffc5f28 onward. ",
          "createdAt": "2020-09-05T16:21:06Z",
          "updatedAt": "2020-09-05T16:21:06Z"
        }
      ]
    },
    {
      "number": 19,
      "id": "MDU6SXNzdWU0OTAxODExOTk=",
      "title": "Make it possible to tie push_id to stream",
      "url": "https://github.com/quiclog/internet-drafts/issues/19",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Thanks to @jlaine for reporting",
      "createdAt": "2019-09-06T07:46:24Z",
      "updatedAt": "2019-10-09T19:24:59Z",
      "closedAt": "2019-10-09T19:24:58Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Was added as \"associated_push_id\" to http.stream_type_set in dda4374878d6fa3aabe9406bd1f7f8706ac59c80",
          "createdAt": "2019-10-09T19:24:58Z",
          "updatedAt": "2019-10-09T19:24:58Z"
        }
      ]
    },
    {
      "number": 20,
      "id": "MDU6SXNzdWU0OTMyNjg4Njg=",
      "title": "Better qpack support",
      "url": "https://github.com/quiclog/internet-drafts/issues/20",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Some comments from @lpardue on qpack support:\r\n\r\n> but how do you correlate a qpack header block event (or whatever you want to call it) to the header frame that it was carried in? maybe simply that header block event contains a stream_id for correlation\r\n\r\n> other option is add raw_header_block to the HeaderFrame event data\r\n\r\nThough that would still require additional events for encoder/decoder instructions, no?\r\n\r\n> a qpack event that can consist of encoder instructions, decoder instructions and/or header block\r\n\r\nSo this seems the better option then, combined with the stream_id and expectation that the qpack events are logged in the same order as HeaderFrame's... (though not sure how important the ordering is personally).\r\n\r\nOpen questions:\r\n- What about seeing what's in the dynamic table (or initial static table? or is that always the same?)? specific dynamic_table_updated event or... ?\r\n\r\n\r\n",
      "createdAt": "2019-09-13T11:12:21Z",
      "updatedAt": "2019-10-08T14:50:35Z",
      "closedAt": "2019-10-08T14:50:35Z",
      "comments": [
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "> What about seeing what's in the dynamic table (or initial static table? or is that always the same?)? specific dynamic_table_updated event or... ?\r\n\r\nSome of this comes around to qlog design ethos - are you logging the message exchange objects or their effects or both (or some, depending on event types and deployment preference)?\r\n",
          "createdAt": "2019-09-13T11:18:25Z",
          "updatedAt": "2019-09-13T11:18:25Z"
        }
      ]
    },
    {
      "number": 21,
      "id": "MDU6SXNzdWU1MDE0NDY2MDA=",
      "title": "Make event names more consistent",
      "url": "https://github.com/quiclog/internet-drafts/issues/21",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Change event names to the trend of \"metrics_updated\" instead of \"metric_update\".\r\nThis is what we use for \"packet_sent\" and \"packet_received\" etc. and it's nicer to have this everywhere. ",
      "createdAt": "2019-10-02T12:03:27Z",
      "updatedAt": "2019-10-04T10:41:15Z",
      "closedAt": "2019-10-04T10:41:15Z",
      "comments": []
    },
    {
      "number": 22,
      "id": "MDU6SXNzdWU1MDIxMzAxNzQ=",
      "title": "Mark events by their importance",
      "url": "https://github.com/quiclog/internet-drafts/issues/22",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Not all events are equally useful in a debugging/tooling scenario.\r\nMark events according to order of usefulness/expectedness.\r\n\r\nFor example:\r\n- Core\r\n- Base\r\n- Extra",
      "createdAt": "2019-10-03T15:15:45Z",
      "updatedAt": "2019-10-04T10:41:15Z",
      "closedAt": "2019-10-04T10:41:15Z",
      "comments": []
    },
    {
      "number": 23,
      "id": "MDU6SXNzdWU1MDI1MjU5NTE=",
      "title": "Make triggers behave like mixins",
      "url": "https://github.com/quiclog/internet-drafts/issues/23",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Since no-one is implementing triggers as top-level fields and it was always unclear how to best approach them, we should punt them to optional properties of the \"data\" field instead. This allows their flexibility without their overhead. \r\n\r\nSee also issue #7 ",
      "createdAt": "2019-10-04T09:20:09Z",
      "updatedAt": "2019-10-04T10:41:15Z",
      "closedAt": "2019-10-04T10:41:15Z",
      "comments": []
    },
    {
      "number": 24,
      "id": "MDU6SXNzdWU1MDM1NTYyMTM=",
      "title": "Replace specific events with a single encompassing event",
      "url": "https://github.com/quiclog/internet-drafts/issues/24",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "We want to reduce the total amount of events as much as possible.\r\n\r\nEspecially specific events for things happening in reaction to the receipt of a specific frame in a packet (e.g., ACK, MAX_DATA, etc.) can be removed, since they can usually be inferred from that frame. Initially we had separate events for these (e.g., \"packet_acknowledged\" or \"flow_control_updated\" but those were rarely used in addition to packet_received events + many implementations do not defer frame handling from reception.\r\n\r\nOne notable exception is @nibank's msquic, which does not log all frames in a received packet, but rather does log only specific events (e.g., packet_acknowledged). One of the reasons is because he feels logging each packet in full does not scale. Another reason for this pattern might be that an implementation does not wish to log all types of frames OR conversely, does not want to log packet-level information at all but only very select frames. \r\n\r\nTo support this use case and still keep a low amount of event types, I will add a \"frame_parsed\" encompassing event. This will log the frame with its associated data, but without the encompassing packet-level data. This prevents re-defining semantics for many events. The downside is that you sometimes might want to log e.g., \"packet_acknowledged\" a long time after frame receipt. In that case, you would pretend you're parsing the frame only then. I feel this is a good trade-off to make here though. ",
      "createdAt": "2019-10-07T16:20:47Z",
      "updatedAt": "2019-10-07T17:03:31Z",
      "closedAt": "2019-10-07T17:03:31Z",
      "comments": []
    },
    {
      "number": 25,
      "id": "MDU6SXNzdWU1MDQ1MjcwNDA=",
      "title": "Additional triggers and info for dropped packets",
      "url": "https://github.com/quiclog/internet-drafts/issues/25",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "The current text lists about 8 reasons for dropping packets.\r\nMicrosoft's implementation lists 60+ individual reasons (via @nibanks)\r\n\r\nSome of those:\r\n\r\n> LogDrop(\"Retry sent to server\");\r\nLogDrop(\"Already received server response\");\r\nLogDrop(\"No room for ODCID\");\r\nLogDrop(\"Invalid ODCID\");\r\nLogDrop(\"InitialToken alloc failed\");\r\nLogDrop(\"OrigCID alloc failed\");\r\nLogDrop(\"Max deferred datagram count reached\");\r\nLogDrop(\"Key no longer accepted\");\r\nLogDrop(\"SH packet during version negotiation\");\r\nLogDrop(\"Too short for HP\");\r\nLogDrop(\"Packet number too big\");\r\nLogDrop(\"Payload length less than encryption tag\");\r\nLogDrop(\"Generate new packet keys\");\r\nLogDrop(\"Decryption failure\");\r\nLogDrop(\"Invalid SH Reserved bits values\");\r\nLogDrop(\"Invalid LH Reserved bits values\");\r\nLogDrop(\"Duplicate packet number\");\r\nLogDrop(\"Key no longer accepted (batch)\");\r\nLogDrop(\"Failed to compute HP mask\");\r\nLogDrop(\"Different remote address\");\r\nLogDrop(\"Too small for Packet->Invariant\");\r\nLogDrop(\"LH no room for DestCID\");\r\nLogDrop(\"Zero length DestCID\");\r\nLogDrop(\"LH no room for SourceCID\");\r\nLogDrop(\"SH no room for DestCID\");\r\nLogDrop(\"DestCID don't match\");\r\nLogDrop(\"SourceCID don't match\");\r\nLogDrop(\"Greater than allowed max CID length\");\r\nLogDropWithValue(\"Invalid client/server packet type\", Packet->LH->Type);\r\nLogDrop(\"Invalid LH FixedBit bits values\");\r\nLogDrop(\"Long header has invalid token length\");\r\nLogDropWithValue(\"Long header has token length larger than buffer length\", TokenLengthVarInt);\r\nLogDrop(\"Long header has invalid payload length\");\r\nLogDropWithValue(\"Long header has length larger than buffer length\", LengthVarInt);\r\nLogDropWithValue(\"Long Header doesn't have enough room for packet number\",\r\nLogDrop(\"Invalid SH FixedBit bits values\");\r\nLogDrop(\"Non-initial packet not matched with a Connection\");\r\nLogDrop(\"Retry received after initial\");\r\n\r\nI feel that we don't need to list things in this level of detail in the qlog spec (the \"trigger\" field allows any text anyway). However, maybe some guidance text on this would be helpful and maybe a few more suggested triggers would be interesting. \r\n\r\n\r\n",
      "createdAt": "2019-10-09T09:29:23Z",
      "updatedAt": "2019-10-09T09:29:23Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 26,
      "id": "MDU6SXNzdWU1MDcwOTMyMTQ=",
      "title": "well-known URI might include an extension",
      "url": "https://github.com/quiclog/internet-drafts/issues/26",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, we just use the ODCID directly as an identifier, without a \".qlog\" extension. \r\nIt might be interesting to include the extension, but I don't really have a good view on the pros and cons. ",
      "createdAt": "2019-10-15T08:38:34Z",
      "updatedAt": "2020-09-01T19:13:48Z",
      "closedAt": "2020-09-01T19:13:48Z",
      "comments": []
    },
    {
      "number": 27,
      "id": "MDU6SXNzdWU1MDcxMDcwNzU=",
      "title": "0-RTT is a bit ambiguous in -01",
      "url": "https://github.com/quiclog/internet-drafts/issues/27",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "using transport.parameters_set there are two parameters to signal 0-RTT:\r\n- resumption_allowed\r\n- early_data_enabled\r\n\r\nAs pointed out by @jlaine, these are a bit ambiguous, as 0-RTT can either be used for the current connection or enabled for a future connection.\r\n\r\nSolution 1:\r\n- Rename parameters to resumption_accepted and early_data_accepted\r\n- Add new events: session_ticket_sent/received (with early_data_enabled?:boolean member)\r\n\r\nSolution 2:\r\n- Rename parameters to resumption_accepted and early_data_accepted\r\n- Add new parameters: resumption_offered and early_data_offered",
      "createdAt": "2019-10-15T09:03:47Z",
      "updatedAt": "2019-10-15T09:03:47Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 28,
      "id": "MDU6SXNzdWU1MDkwNTI3Mzg=",
      "title": "Lacking a way to indicate ALPN list for client",
      "url": "https://github.com/quiclog/internet-drafts/issues/28",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, we can log a list of supported ALPN values for the server in \"server_listening\" and log the finally selected ALPN in \"parameters_set\". However, we lack a way to log the list of ALPN values the client supports (and offers the server).\r\n\r\nOptions:\r\n- add `alpn_values?: Array<string> // ALPN values offered by the client / received by the server. Use parameters_set to log the actually selected alp` to \"connection_started\"\r\n- make \"alpn\" in \"parameters_set\" an array instead of a string\r\n\r\nI personally prefer the 1st option, since the second doesn't match the semantics of \"set\" (since it would be emitted twice) and logging the negotiation options should be optional in a \"Base\" (while the final value is in a \"Core\" event).\r\n\r\nCC @jlaine ",
      "createdAt": "2019-10-18T12:35:44Z",
      "updatedAt": "2020-03-08T11:13:28Z",
      "closedAt": null,
      "comments": [
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "The first option is fine. It is nice to distinguish between proposal and results.\r\nI could see for example:\r\n```\r\n\"alpn_values\": [ \"h3-27\", \"hq-27\", \"h3-25\", \"hq-25\" ],\r\n```\r\nAnd later:\r\n```\r\n\"alpn\": \"h3-27\",\r\n```\r\nBut there a few issues. For example, what happens if the qlog entry contains both `alpn_values` and `alpn`?",
          "createdAt": "2020-03-08T04:31:02Z",
          "updatedAt": "2020-03-08T04:33:46Z"
        },
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "I would prefer something like `proposed_alpn` instead of `alpn_values`, to emphasize that this is a proposal. This also has a nice way to solve the `proposed_alpn` vs `alpn` issues. For example, if a server logs:\r\n```\r\n\"proposed_alpn\": [ \"h3-27\", \"hq-27\", \"h3-25\", \"hq-25\" ],\r\n\"alpn\": \"h3-27\",\r\n```\r\nThat can be clear understood as \"the client proposed these 4 values, and the server selected the 1sr one\"\r\n",
          "createdAt": "2020-03-08T04:34:50Z",
          "updatedAt": "2020-03-08T04:35:24Z"
        },
        {
          "author": "jlaine",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I like the idea of strictly distinguishing offer values and the negotiated one. Alternative possible names (no strong feelings about this):\r\n\r\n```\r\n\"alpn_offer\": [\"a\", \"b\", \"c\"]\r\n\"alpn_answer\": \"a\"\r\n```",
          "createdAt": "2020-03-08T11:13:13Z",
          "updatedAt": "2020-03-08T11:13:28Z"
        }
      ]
    },
    {
      "number": 29,
      "id": "MDU6SXNzdWU1MjAzNTE4MjY=",
      "title": "Utf-8",
      "url": "https://github.com/quiclog/internet-drafts/issues/29",
      "state": "CLOSED",
      "author": "mocsy",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Draft 01 doesn't specify character encoding, but the ts files work with utf-8, don't they?\r\nI propose to standardize that choice as well.",
      "createdAt": "2019-11-09T07:35:27Z",
      "updatedAt": "2020-09-05T16:12:20Z",
      "closedAt": "2020-09-05T16:12:20Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Excellent point, I agree the encoding should be defined. ",
          "createdAt": "2019-11-09T07:38:34Z",
          "updatedAt": "2019-11-09T07:38:34Z"
        }
      ]
    },
    {
      "number": 30,
      "id": "MDU6SXNzdWU1MjQ4MjY3Nzg=",
      "title": "consider moving to a binary format",
      "url": "https://github.com/quiclog/internet-drafts/issues/30",
      "state": "CLOSED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "I just started working on implementing qlog in quic-go. Maybe it's because I'm still fairly unfamiliar with qlog, but I feel like encoding things in JSON leads to some awkward hacks. Examples of these are:\r\n* A lot of numbers are encoded as strings, e.g. stream offset or packet numbers. I assume this is because JSON doesn't properly handle uint64s (or does it?).\r\n* IP addresses are encoded as strings. If that means they're supposed to be encoded in the human-readable encoding (with . and :), that's ambiguous for IPv6 addresses. Really, IP addresses should be a byte array.\r\n* (Raw) packet data is supposed to be hex encoded, which greatly increases the log size.\r\n* Some fields are defined as enums, whereas other fields that just have a few options are encoded as strings. Examples are the `stream_side` (\"sending\" or \"receiving\") and `stream_type` (\"unidirectional\" or \"bidirectional\"), which are both string fields.\r\n\r\nI'm not sure if I like trick to save bytes on the `events` by first defining the `event_fields` and then using a list instead of an object to encode the `events`. To me, this feels more like a hack to work around the shortcomings of JSON, namely the repetition of the field labels when using objects.\r\nAs far as I can see, a binary encoding scheme would be able to provide a type-safe representation here without repeating the field labels (and blowing up the file size), as long as it's possible to define some `common_fields` for a connection.\r\n\r\nA protobuf-based logging format (This is just a suggestion. Protobufs are the thing I'm most familiar with, maybe there are better choices out there.) would resolve the encoding ambiguities I listed above, because we'd be able to make use of a strong typing system, which would allow us to completely eliminate the use of `string`s (except for places where things actually are strings, e.g. CONNECTION_CLOSE reason phrases). Furthermore, it would greatly simplify implementing qlog: Just fill in the corresponding fields in the Protobuf messages, call `Marshal()`, and you're done. No need to manually define dozens of logging structs and make sure they're correctly serialized into qlog's flavor of JSON.",
      "createdAt": "2019-11-19T07:30:57Z",
      "updatedAt": "2020-09-05T16:22:44Z",
      "closedAt": "2020-09-05T16:22:44Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Talking about this with @nibanks, he would primarily like this for larger traces (he has logs of several 100s of megabytes) and for integration with other tools (like https://docs.microsoft.com/en-us/windows-hardware/test/wpt/windows-performance-analyzer).\r\n\r\nHe suggests https://diamon.org/ctf/ as one possible format (though, at first glance, this doesn't have a JavaScript parser somewhere). ",
          "createdAt": "2020-01-07T16:05:08Z",
          "updatedAt": "2020-01-07T16:05:08Z"
        },
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "There is related experience with DNS log formats. In particular, look at the CBOR encoding of DNS logs proposed in RFC 8618, https://datatracker.ietf.org/doc/rfc8618/. They started from PCAP, but there was a practical issue with managing huge PCAP files. The first attempt was to just try compress the binary, but they ended up with a more structured approach. The logical syntax follows the \"natural\" repetitions in the data, managing to get for example DNS names encoded just once, and then represented by indices in the tables of names. Then they encode the \"syntactically organized\" data in CBOR (binary JSON), and they apply compression on top of that.\r\n\r\nThe main value of the logical syntax comes when processing logs. For example, I observed a factor 50 performance gain between doing DNS statistics directly on the PCAP and doing the same statistics on the logical CBOR data, due to both reduced IO with shorter data, and more compact code following logical references.\r\n\r\nI suspect there is something similar hiding in the Quic traces.",
          "createdAt": "2020-02-26T01:05:34Z",
          "updatedAt": "2020-02-26T01:05:34Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "@huitema that's some very interesting stuff that I wasn't aware of yet, thanks!",
          "createdAt": "2020-02-26T09:51:43Z",
          "updatedAt": "2020-02-26T09:51:43Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Talking about it some more with @nibanks, he states:\r\n\r\n> I'd prefer something that is light-weight and doesn't depend on yet another thing (protobuf). Or something that exists and is light weight to implement a parser for from scratch\r\n\r\n@LPardue did some initial tests with CBOR and found the file size gains not to really outweigh compressed JSON. \r\n\r\nI am currently experimenting with a few binary scheme options to get a first feel for potential file size and (de)serialization gains. That should give us some additional data to work from. ",
          "createdAt": "2020-03-17T15:24:30Z",
          "updatedAt": "2020-03-17T15:24:30Z"
        },
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "To be clear I am no CBOR expert. All I did for my serializing code was substitute out serde_json for serde_cbor and compare the resulting output. CBOR shaved off 10% of identity encoding, gzipped-json shaved off about 40%.\r\n\r\nAFAIK It is possible to profile CBOR to be more efficient (e.g. https://tools.ietf.org/html/draft-raza-ace-cbor-certificates-04) but that is beyond my skillset.\r\n",
          "createdAt": "2020-03-17T23:06:52Z",
          "updatedAt": "2020-03-17T23:06:52Z"
        },
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "I am quite familiar with the work on using CBOR to record DNS traces in RFC 8618. The captures were originally in PCAP, but PCAP gets very large files. They looked at a set of variations:\r\n\r\n|  Format      | File size | Comp. | Comp. size |   RSS | User time |\r\n| --------- | --------- | ----- | ---------- | ----- | --------- |\r\n| PCAP        |    661.87 | snzip |     212.48 |  2696 |      1.26 |\r\n|             |           | lz4   |     181.58 |  6336 |      1.35 |\r\n|             |           | gzip  |     153.46 |  1428 |     18.20 |\r\n|             |           | zstd  |      87.07 |  3544 |      4.27 |\r\n|             |           | xz    |      49.09 | 97416 |    160.79 |\r\n|             |           |       |            |       |           |\r\n| JSON simple |   4113.92 | snzip |     603.78 |  2656 |      5.72 |\r\n|             |           | lz4   |     386.42 |  5636 |      5.25 |\r\n|             |           | gzip  |     271.11 |  1492 |     73.00 |\r\n|             |           | zstd  |     133.43 |  3284 |      8.68 |\r\n|             |           | xz    |      51.98 | 97412 |    600.74 |\r\n|             |           |       |            |       |           |\r\n| Avro simple |    640.45 | snzip |     148.98 |  2656 |      0.90 |\r\n|             |           | lz4   |     111.92 |  5828 |      0.99 |\r\n|             |           | gzip  |     103.07 |  1540 |     11.52 |\r\n|             |           | zstd  |      49.08 |  3524 |      2.50 |\r\n|             |           | xz    |      22.87 | 97308 |     90.34 |\r\n|             |           |       |            |       |           |\r\n| CBOR simple |    764.82 | snzip |     164.57 |  2664 |      1.11 |\r\n|             |           | lz4   |     120.98 |  5892 |      1.13 |\r\n|             |           | gzip  |     110.61 |  1428 |     12.88 |\r\n|             |           | zstd  |      54.14 |  3224 |      2.77 |\r\n|             |           | xz    |      23.43 | 97276 |    111.48 |\r\n|             |           |       |            |       |           |\r\n| PBuf simple |    749.51 | snzip |     167.16 |  2660 |      1.08 |\r\n|             |           | lz4   |     123.09 |  5824 |      1.14 |\r\n|             |           | gzip  |     112.05 |  1424 |     12.75 |\r\n|             |           | zstd  |      53.39 |  3388 |      2.76 |\r\n|             |           | xz    |      23.99 | 97348 |    106.47 |\r\n|             |           |       |            |       |           |\r\n| JSON block  |    519.77 | snzip |     106.12 |  2812 |      0.93 |\r\n|             |           | lz4   |     104.34 |  6080 |      0.97 |\r\n|             |           | gzip  |      57.97 |  1604 |     12.70 |\r\n|             |           | zstd  |      61.51 |  3396 |      3.45 |\r\n|             |           | xz    |      27.67 | 97524 |    169.10 |\r\n|             |           |       |            |       |           |\r\n| Avro block  |     60.45 | snzip |      48.38 |  2688 |      0.20 |\r\n|             |           | lz4   |      48.78 |  8540 |      0.22 |\r\n|             |           | gzip  |      39.62 |  1576 |      2.92 |\r\n|             |           | zstd  |      29.63 |  3612 |      1.25 |\r\n|             |           | xz    |      18.28 | 97564 |     25.81 |\r\n|             |           |       |            |       |           |\r\n| CBOR block  |     75.25 | snzip |      53.27 |  2684 |      0.24 |\r\n|             |           | lz4   |      51.88 |  8008 |      0.28 |\r\n|             |           | gzip  |      41.17 |  1548 |      4.36 |\r\n|             |           | zstd  |      30.61 |  3476 |      1.48 |\r\n|             |           | xz    |      18.15 | 97556 |     38.78 |\r\n|             |           |       |            |       |           |\r\n| PBuf block  |     67.98 | snzip |      51.10 |  2636 |      0.24 |\r\n|             |           | lz4   |      52.39 |  8304 |      0.24 |\r\n|             |           | gzip  |      40.19 |  1520 |      3.63 |\r\n|             |           | zstd  |      31.61 |  3576 |      1.40 |\r\n|             |           | xz    |      17.94 | 97440 |     33.99 |\r\n\r\nYou can see that there are some differences between various algorithms. JSON clearly gets bigger sizes there than the binary alternatives, even after compression. But the biggest differences come from switching from what they call \"simple\" to what they call \"block\".\r\n\r\nThe simple alternative is pretty similar to the current Qlog. Each DNS transaction is represented by a corresponding record in JSON, CBOR, Avro or protobuf. In contrast, the \"block\" format starts by building tables of objects seen in multiple records: table of DNS names, table to record values, etc. Then the individual PCAP records are represented by \"block records\" which instead of listing DNS names simply list the index of the name in the table of names. You can think of that as a \"logical compression\", and it does reduces the size of the recording by a factor 10x. After that, they can still apply compression.\r\n\r\nThe real beauty of the block format comes when processing the data in back end programs. Compare:\r\n```\r\nuncompress < pcap.xz | process-pcap\r\n```\r\nTo:\r\n```\r\nuncompress < cbor.xz | process-cbor\r\n```\r\nIn the cbor alternative, there are about 10 times fewer data piped into the analysis program than in the pcap alternative. That's a much lower IO load. On top of that, since the cbor data is structured in blocks, parsing and processing is much easier, resulting in a much lower CPU load. In a project that I was involved with, replacing process-pcap by process-cbor made us run 40 times faster!\r\n\r\nAlso note that there are no practical differences between the various binary alternatives. yes, +- 10% here or there, but compared to a factor 40 that's really in the noise.",
          "createdAt": "2020-03-18T00:58:46Z",
          "updatedAt": "2020-03-18T00:58:46Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Thanks a lot for that @huitema. Doing something similar to the \"block format\" would be trivial for qlog as well. However, it mismatches with how I thought general purpose compression works in my head... don't those algorithms also build that type of lookup-table on the fly? I will test with manual block formats as well and see what that gives.\r\n\r\nAnother interesting ref from @martinthomson https://tools.ietf.org/html/draft-mattsson-tls-cbor-cert-compress-00",
          "createdAt": "2020-03-18T11:10:58Z",
          "updatedAt": "2020-03-18T11:10:58Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "So I've been doing some tests of my own to figure out the best approach to this for qlog.\r\n\r\nI've created converter scripts (see https://github.com/quiclog/pcap2qlog/tree/binary/src/converters) that use a lookup table/dictionary instead of repeating values, one that cbor encodes the files and a (rudimentary) protobuf schema. The dictionary is currently fully dynamic and stored inside the resulting file, but this can obviously be improved by having a static shared dictionary with a dynamic part for just the field values (much like QPACK and Chrome's NetLog). \r\n\r\nI've then also looked at various compression schemes (https://github.com/quiclog/pcap2qlog/blob/binary/src/scripts/comparisons/compare.sh) (xz, gzip, brotli, zstd, lz4), focusing mainly on the schemes most often seen on the web for on-the-fly compression (gzip 6 and brotli 4).\r\n\r\nFull results can be found at https://gist.github.com/rmarx/49bb14f83157d9fe59fb40e7c05b1f3f, a bit nicer representation in the following image (sizes for traces in which a 500MB or 100MB file were downloaded from the lsquic public endpoint). The blue value is the reference point for the percentages, green is the \"best in class\" for that row:\r\n\r\n![2020-04-22 11_27_11-results xlsx - Excel](https://user-images.githubusercontent.com/2240689/79965403-410ed380-848c-11ea-9d66-b1a90bd4bc72.png)\r\n\r\n\r\nMain takeaways for me:\r\n1. protobuf is the smallest, but not by a huge margin compared to dictionary+cbor, especially not when compression is used.\r\n2. compression alone saves massively, even on the original JSON file or direct CBOR version of that\r\n3. protobuf without compression is still quite large (23% of original), so I'd reckon you'd always use compression for storage/transmission anyway? \r\n\r\nNext to these tests, we also ran a survey among QUIC experts (implementers and researchers), on which we got replies from 28 participants (thanks everyone!). Part of the survey was to ask how important they felt features like \"fast (de)serialization, small file size, flexibility (e.g., easily add new event types), grep-ability\" were. The full results will be posted soon (are part of a publication we're preparing), but the gist of it is:\r\n\r\n![2020-04-22 11_37_40-QUIC and HTTP_3 Debugging Survey - March 2020 - Google Forms](https://user-images.githubusercontent.com/2240689/79966456-c050d700-848d-11ea-9113-d9c2a230d8ab.png)\r\n\r\nMy interpretation:\r\n1. Flexibility is the major selling point across the board. I personally believe moving to something like protobuf sort of robs us from that (much more difficult to add new event types as you have to update the schema)\r\n2. Most don't care too much about (de)serialization performance\r\n3. Small file size was regarded as important, but again not as much as flexibility. \r\n4. grep-ability was also considered an important feature by many\r\n5. easy integration is also a major point and this would be easier with something like protobuf\r\n\r\nFinally, we also talked to Facebook (cc @mjoras), who have been deploying qlog at scale, logging over 30 billion qlog events per day. Compared to their earlier binary format, qlog is about 2-3x larger and takes 50% longer to serialize. Yet, this is quite manageable on the server side, where they log full-string JSON events to a centralized service. On the client, they do find the file-size to be prohibitive to upload granular full qlogs (containing all events they'd like). Yet, Matt was also adamant that they'd rather keep the flexibility of the JSON format than move to a more inflexible binary one. They were considering utilizing compression and writing a custom JSON (de)serializer, optimized for qlog, to help deal with some of the overhead. \r\n\r\n----------------------------------\r\n\r\nSo, presented with those results, my standpoint today is still to keep using JSON as the basis for qlog. I would propose to add the \"dictionary\" setup to the spec though, as an optional optimized mode and also recommend tools to support that (not sure about a default static dictionary at this point though). Furthermore, I'd recommend using cbor if file size is important. \r\n\r\nCompanies that do need more optimizations can write their own protobuf (or equivalent) schema (which I've shown is possible) and then write a post-processor to go to proper JSON qlog for shared tooling. \r\n\r\nStill, feedback on all this is more than welcome of course! @marten-seemann, @martinthomson, @huitema, @LPardue, @nibanks, @mjoras\r\n\r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2020-04-22T09:50:22Z",
          "updatedAt": "2020-04-22T09:50:22Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "If we use cbor, does that mean that we can get rid of the `event_fields`? Having implemented a both an encoder as well as a parsing tool, this complicated things quite a bit for me (over just encoding an event as a normal JSON object).",
          "createdAt": "2020-04-22T09:56:02Z",
          "updatedAt": "2020-04-22T09:56:02Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "With the latest commit linked above (https://github.com/quiclog/internet-drafts/commit/eb59e69aa0f92031d8a2377575b7328429440061), I feel this issue has been resolved.\r\n\r\nqlog has not moved to a binary format by default, but is now much easier to serialize as one/to define a binary schema for. Some of the reasoning behind that has also been included in the qlog document. ",
          "createdAt": "2020-09-05T16:22:44Z",
          "updatedAt": "2020-09-05T16:22:44Z"
        }
      ]
    },
    {
      "number": 31,
      "id": "MDU6SXNzdWU1MzA1NzkzNjI=",
      "title": "Typo in path response frame definition",
      "url": "https://github.com/quiclog/internet-drafts/issues/31",
      "state": "CLOSED",
      "author": "mpiraux",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "```\r\n### PathResponseFrame\r\n\r\n~~~\r\nclass PathResponseFrame{\r\n  frame_type:string = \"patch_response\";\r\n\r\n  data?:string;\r\n}\r\n~~~\r\n```\r\n\r\n`patch_response` should be `path_response`.",
      "createdAt": "2019-11-30T13:30:20Z",
      "updatedAt": "2020-09-07T13:29:02Z",
      "closedAt": "2020-09-07T13:29:02Z",
      "comments": []
    },
    {
      "number": 32,
      "id": "MDU6SXNzdWU1NDYzMzIyODQ=",
      "title": "Mention JSON earlier",
      "url": "https://github.com/quiclog/internet-drafts/issues/32",
      "state": "CLOSED",
      "author": "dtikhonov",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "The fact that qlog is in JSON format is not mentioned until section 3.3.4 of the main logging schema draft.  This should be stated earlier: before any JSON examples are given.",
      "createdAt": "2020-01-07T15:02:10Z",
      "updatedAt": "2020-09-05T16:04:17Z",
      "closedAt": "2020-09-05T16:04:17Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Fixed in f5db7cdc8cd0cf37bfe5f1b0b4c54fc56ffc5f28",
          "createdAt": "2020-09-05T16:04:17Z",
          "updatedAt": "2020-09-05T16:04:17Z"
        }
      ]
    },
    {
      "number": 33,
      "id": "MDU6SXNzdWU1NDYzMzM5OTQ=",
      "title": "Well-known URI: uppercase or lowercase",
      "url": "https://github.com/quiclog/internet-drafts/issues/33",
      "state": "CLOSED",
      "author": "dtikhonov",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Whether ODCID in the well-known URI is uppercase or lowercase should be specified.",
      "createdAt": "2020-01-07T15:05:21Z",
      "updatedAt": "2020-09-01T19:13:38Z",
      "closedAt": "2020-09-01T19:13:38Z",
      "comments": []
    },
    {
      "number": 34,
      "id": "MDU6SXNzdWU1NDk5MDA3MzI=",
      "title": "Consolidate repeated padding frames?",
      "url": "https://github.com/quiclog/internet-drafts/issues/34",
      "state": "CLOSED",
      "author": "agrover",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Padding frames contain no information, but when eyeballing a qlog for the start of a connection, they swamp more interesting things.\r\n\r\nIt's a bit of a cheat, given that qlog otherwise is 1:1 between packet contents and logging, but I was wondering if a more compact representation of repeated padding frames might be nice.\r\n\r\nThis is less about qlog file size -- I'm assuming repeated padding entries in json would compress amazingly -- more about human readability.\r\n\r\nI see pros and cons, but wanted to raise it as an issue. Cheers.",
      "createdAt": "2020-01-15T00:45:26Z",
      "updatedAt": "2020-09-07T13:28:39Z",
      "closedAt": "2020-09-07T13:28:39Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "I'm very confused at the moment, because I was sure I had already added this to the editor's draft for version -02, but... apparently not? \r\n\r\nSo yes, I definitely think this is a good idea. At one point, Facebook was logging 1 padding frame for each byte of padding and that was horrendous. \r\n\r\nI thought about just adding a \"length\" field to the padding frame, indicating the amount of bytes padded. Would you agree that's the correct approach? \r\n\r\n",
          "createdAt": "2020-01-15T07:29:55Z",
          "updatedAt": "2020-01-15T07:29:55Z"
        },
        {
          "author": "hawkinsw",
          "authorAssociation": "NONE",
          "body": "> I'm very confused at the moment, because I was sure I had already added this to the editor's draft for version -02, but... apparently not?\r\n> \r\n> So yes, I definitely think this is a good idea. At one point, Facebook was logging 1 padding frame for each byte of padding and that was horrendous.\r\n> \r\n> I thought about just adding a \"length\" field to the padding frame, indicating the amount of bytes padded. Would you agree that's the correct approach?\r\n\r\nThat sounds like a pretty good idea to me. It also seems to \"mirror\" the approach that Wireshark takes?\r\n![Screenshot from 2020-01-15 03-52-38](https://user-images.githubusercontent.com/8715530/72419310-911e2180-374a-11ea-8520-1b8f99a34a12.png)\r\n",
          "createdAt": "2020-01-15T08:53:33Z",
          "updatedAt": "2020-01-15T08:53:33Z"
        },
        {
          "author": "agrover",
          "authorAssociation": "NONE",
          "body": "> I thought about just adding a \"length\" field to the padding frame, indicating the amount of bytes padded. Would you agree that's the correct approach?\r\n\r\nSounds good to me!",
          "createdAt": "2020-01-16T18:48:52Z",
          "updatedAt": "2020-01-16T18:48:52Z"
        }
      ]
    },
    {
      "number": 35,
      "id": "MDU6SXNzdWU1NTE3NTE3NjI=",
      "title": "Add guidance to server developers",
      "url": "https://github.com/quiclog/internet-drafts/issues/35",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "@marten-seemann asked how to best approach logging from a server's perspective, given that things like version negotiation and stateless retry are not inherently tied to a single connection. We should add some informative guidance on how to best approach this to the spec, depending on how much state you're willing to keep around \r\n\r\nSome options:\r\n\r\n1. low state: keep a separate qlog file for the entire server. This logs vneg, retry, etc.. Then, when a connection is truly accepted, start a new .qlog for the individual connection, containing all events thereafter. The server.qlog can then also contain an event signalling the acceptance of a new connection for later cross-linking between the files.\r\n2. low state: keep a single huge qlog file for everything, using the \"group_id\" field to allow later de-multiplexing into separate connections (I believe quant does this atm)\r\n3. stateful: if you already track vneg/retry and link them up with the final connection, you can output them in the per-connection qlog file as well\r\n\r\nMaybe also shortly talk about some of the trade-offs in each option. Also talk about how to approach server-level events like server_listening and packet_dropped in separate scenarios. ",
      "createdAt": "2020-01-18T10:33:53Z",
      "updatedAt": "2020-01-18T10:34:36Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 36,
      "id": "MDU6SXNzdWU1NTE3NTUxNTg=",
      "title": "Require specific encoding for string fields",
      "url": "https://github.com/quiclog/internet-drafts/issues/36",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "For example, NewTokenFrame doesn't really need a length field if the encoding of the token is specified (e.g., if it is hex-encoded, byte-length is 2x token.length).\r\n\r\nWe already indicate hex-encoding at other places in the text (e.g., for version), maybe it's a good idea to enforce this across the board (then also specificy whether it should have 0x prefix or not etc. + examples).\r\n\r\nThanks to @marten-seemann for reporting",
      "createdAt": "2020-01-18T11:08:45Z",
      "updatedAt": "2020-09-05T16:23:16Z",
      "closedAt": "2020-09-05T16:23:16Z",
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "This also applies to the NEW_CONNECTION_ID frame.",
          "createdAt": "2020-01-18T14:43:29Z",
          "updatedAt": "2020-01-18T14:43:29Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Could this be as easy as saying that all byte-values are hex-encoding (omitting the 0x) somewhere in the introduction? Then this would apply to connection IDs, Retry tokens, NEW_TOKEN tokens, stateless reset tokens, etc.",
          "createdAt": "2020-07-08T09:01:57Z",
          "updatedAt": "2020-07-08T09:01:57Z"
        }
      ]
    },
    {
      "number": 37,
      "id": "MDU6SXNzdWU1NTE3NTc4MjA=",
      "title": "missing HANDSHAKE_DONE frame",
      "url": "https://github.com/quiclog/internet-drafts/issues/37",
      "state": "CLOSED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2020-01-18T11:35:17Z",
      "updatedAt": "2020-01-18T15:31:42Z",
      "closedAt": "2020-01-18T15:31:42Z",
      "comments": []
    },
    {
      "number": 39,
      "id": "MDU6SXNzdWU1NTE3ODA4OTQ=",
      "title": "Be more consistent in numbers vs strings for (potentially) large numbers (varints)",
      "url": "https://github.com/quiclog/internet-drafts/issues/39",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Now, we only make varints strings if it's likely they will go over 2^53 (JSON's / JavaScript's number limit). \r\n\r\nFor example, this means error codes are just numbers, even though they are varints, as it's unlikely to see a very large error code. However, for fields that are greased (e.g., see https://github.com/quicwg/base-drafts/pull/3360) they could be larger and a number no longer suffices.\r\n\r\nSee also dcil and scil in PacketHeader, which could be numbers but are now strings.\r\n\r\nMore in general: maybe it's best to simply allow both for all number fields and have the tools figure it out? ",
      "createdAt": "2020-01-18T15:07:33Z",
      "updatedAt": "2020-09-07T13:19:00Z",
      "closedAt": "2020-09-07T13:19:00Z",
      "comments": [
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "For me it would be very unnatural to treat Quic integers (varint encoded) as strings instead of numbers. Not impossible of course, one could always add quotes. But very unnatural.\r\n",
          "createdAt": "2020-02-26T00:54:29Z",
          "updatedAt": "2020-02-26T00:54:29Z"
        },
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "An approach some people are using for extension experimentation IS to pick large values for frame type, settings, stream types, etc. ",
          "createdAt": "2020-05-25T13:03:08Z",
          "updatedAt": "2020-05-25T13:03:08Z"
        },
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "@LPardue people do pick large numbers for experimentation, but the numbers are typically 2 bytes:\r\n```\r\nFrame types:\r\n    ack_frequency = 0xAF,\r\n    time_stamp = 757\r\nTP:\r\n    test_large_chello = 3127,\r\n    enable_loss_bit_old = 0x1055,\r\n    enable_loss_bit = 0x1057,\r\n    min_ack_delay = 0xDE1A,\r\n    enable_time_stamp = 0x7157\r\n```\r\nEven if we add another byte for versioning, traditional JSON will work just fine.",
          "createdAt": "2020-05-25T14:39:55Z",
          "updatedAt": "2020-05-25T14:39:55Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "The plan is to switch away from JSON/TypeScript notation in the draft and use explicit type annotations (e.g., uint32, uint64, etc.).\r\n\r\nThen I'd add something stating that, if using a JSON format, you should decide yourself between a) encoding 64-bit values as strings or b) hope they won't be larger than 2^53 and log them as numbers. Tools would be strongly advised to support both string and number variants of these fields. ",
          "createdAt": "2020-05-25T15:02:24Z",
          "updatedAt": "2020-05-25T15:03:02Z"
        },
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "and PRIORITY_UPDATE is using `0x1CCB8BBF1F0700` :)\r\n\r\nIt will be more difficult to make a general purpose library that can do what Robin suggests on the serialization side. It might also make it hard on the deserialization side in my specific implementation. \r\n\r\nThat said, I really dislike the arbitrary schema inconstencies that exist today. So on balance I look forward to the proposed approach.",
          "createdAt": "2020-05-25T15:19:41Z",
          "updatedAt": "2020-05-25T15:19:41Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "If only we knew someone from the H3 Priorities team that could help mitigate that weirdly large Frame identifier... ",
          "createdAt": "2020-05-25T15:25:30Z",
          "updatedAt": "2020-05-25T15:25:30Z"
        }
      ]
    },
    {
      "number": 40,
      "id": "MDU6SXNzdWU1NTE4Njk2MDE=",
      "title": "packet_size doesn't belong in the PacketHeader, but PacketType does",
      "url": "https://github.com/quiclog/internet-drafts/issues/40",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "In my interpretation, the `PacketHeader` is a QUIC packet header. Therefore, it should contain the QUIC packet type.\r\nHowever, the `packet_size` (as opposed to the `payload_length`, which is the length of the QUIC payload e.g. in a coalesced packet), is a property of the UDP packet, and therefore should a property of the `packet_sent` / `packet_received` event.",
      "createdAt": "2020-01-19T05:27:36Z",
      "updatedAt": "2020-01-19T10:31:18Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "While I -think- I agree with the general sentiment, this is a major departure from the current setup as implemented in most qlog setups. Additionally, many qvis visualizations actively use these fields.\r\n\r\nI propose to keep the issue and PR open until when I can update qvis to deal with this change, so that I don't forget to do just that. Definitely before draft-02 lands of course.\r\n\r\nAdditionally, `packet_size` is intended to mean the entire size of the QUIC packet (header + payload), not the UDP datagram size (which can span multiple coalesced QUIC packets as you indicate). I felt the separate packet_size was needed to get an estimate of the header size, since that can differ quite a bit depending on the chosen encodings, pn-length etc. That does bring up the question if we need a `datagram_size/length` outside of the `datagram_*` events as well (though I personally would say not).",
          "createdAt": "2020-01-19T10:31:18Z",
          "updatedAt": "2020-01-19T10:31:18Z"
        }
      ]
    },
    {
      "number": 42,
      "id": "MDU6SXNzdWU1NTE5MDYyODU=",
      "title": "Think about usage in proxies",
      "url": "https://github.com/quiclog/internet-drafts/issues/42",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "If qlog is used on a proxy, the question becomes what vantage_point it should use for its traces, since it's now simultaneously accepting and opening connections, and thus behaves as both a client and a server.\r\n\r\nOne approach would be to have (two) separate traces with separate vantage_points, but if there is a 1-to-1 mapping between client -> proxy -> origin connections, it -might- make sense to log everything into one trace (though I would need to reflect more on this). \r\n\r\nIn this latter case, it might make sense to add a \"role\" or \"vantage_point\" indication to events like `connection_started`.\r\n\r\nThanks to @hawkinsw for reporting.\r\nMaybe @LPardue has some comments, given his experience with the proxy-ing use case? ",
      "createdAt": "2020-01-19T11:08:11Z",
      "updatedAt": "2020-01-19T11:08:11Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 43,
      "id": "MDU6SXNzdWU1NTI3ODIwOTc=",
      "title": "Add connection_closed or connection_dropped event",
      "url": "https://github.com/quiclog/internet-drafts/issues/43",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, we rely on packet_* with a CONNECTION_CLOSE frame, but that's not always enough. E.g., the server can decide to drop a connection after a long timeout without sending a CONNECTION_CLOSE. Or, we might want additional information of when a connection is effectively dropped completely (according to @marten-seemann: is supposed to happen 3 PTOs after it is retired)\r\n\r\nMaybe a connection_closed event with a trigger field suffices? Should this be importance Base or Extra?",
      "createdAt": "2020-01-21T10:23:18Z",
      "updatedAt": "2020-01-21T10:23:18Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 44,
      "id": "MDU6SXNzdWU1NTI3ODM1MDc=",
      "title": "Revise design of dual-endpoint events",
      "url": "https://github.com/quiclog/internet-drafts/issues/44",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, we have some events that are used both for indicating changes in the local as well as the remote endpoint. An example is parameters_set, which logs both connection params we set locally, as the ones we get from the other side. parameters_set uses an \"owner\" field to disambiguate between these two cases. \r\n\r\nHowever, other events with similar purpose, like connection_id_updated, use another approach (src_ vs dst_ prefixes). We should decide on a singular consistent approach (currently leaning towards \"owner\" field myself, as it is the most flexible)",
      "createdAt": "2020-01-21T10:25:48Z",
      "updatedAt": "2020-01-21T10:25:48Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 45,
      "id": "MDU6SXNzdWU1NTI3ODQ2MDM=",
      "title": "Reduce importance of connection_id_updated",
      "url": "https://github.com/quiclog/internet-drafts/issues/45",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, it is a Core event.\r\nHowever, as pointed out by @marten-seemann, not all implementations track when the remote endpoint changes their CID (e.g., looks when they first receive a packet with a new CID). In those cases, they might decide to take the log filesize hit by just logging the CID for each incoming PacketHeader.\r\n\r\nAs such, the connection_id_updated should probably be a Base event, with guidance on when to use which option.",
      "createdAt": "2020-01-21T10:27:41Z",
      "updatedAt": "2020-09-07T13:24:26Z",
      "closedAt": "2020-09-07T13:24:26Z",
      "comments": []
    },
    {
      "number": 46,
      "id": "MDU6SXNzdWU1NTM2MzIzNjY=",
      "title": "Updates for draft-25",
      "url": "https://github.com/quiclog/internet-drafts/issues/46",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "For now, there's just one I'm aware of:\r\n\r\nparameters_set.idle_timeout was renamed to .max_idle_timeout",
      "createdAt": "2020-01-22T15:54:15Z",
      "updatedAt": "2020-01-24T12:38:33Z",
      "closedAt": null,
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "There was also HANDSHAKE_DONE, but we added that already.",
          "createdAt": "2020-01-22T17:44:02Z",
          "updatedAt": "2020-01-22T17:44:02Z"
        },
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "maybe this was flagged a while back but `HTTP3EventType:dependency_update`, since it looks like there will be no dpendency-based prioritization in HTTP/3 core probably want to remove it",
          "createdAt": "2020-01-24T12:06:38Z",
          "updatedAt": "2020-01-24T12:07:26Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "@LPardue I think that's a specific issue for the TypeScript definitions that's not in the qlog draft itself? I've fixed it for ts here: https://github.com/quiclog/qlog/commit/cf4af5b227289fb32cde9dc9e39ee6a963a08384",
          "createdAt": "2020-01-24T12:38:25Z",
          "updatedAt": "2020-01-24T12:38:33Z"
        }
      ]
    },
    {
      "number": 47,
      "id": "MDU6SXNzdWU1NTQ3MTk0ODQ=",
      "title": "Revisit the category for generic events",
      "url": "https://github.com/quiclog/internet-drafts/issues/47",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "We have a number of \"generic\" events in the draft (section 7, General error, warning and debugging definitions). These currently have their own categories: error, warning, info, debug, verbose, simulation. This is a bit awkward, since most of these only have a single event type. \r\n\r\nIt might be a good idea to group these under a single category, e.g., \"generic\" or \"general\" or \"textual\" or... (or maybe 2: generic and simulation). Another option would be to define a new event field called `log_level` next to category (but that would increase overhead, as now we're essentially squatting on the category to provide that). \r\n\r\nThe purpose of these events (sans simulation) is to allow one to replace the default textual logging with qlog completely (e.g., everything you now send to stdout/stderr with printf() would go into these kinds of events).\r\n\r\nThanks to @LPardue for reporting.",
      "createdAt": "2020-01-24T12:52:37Z",
      "updatedAt": "2020-08-06T00:27:00Z",
      "closedAt": null,
      "comments": [
        {
          "author": "kazu-yamamoto",
          "authorAssociation": "NONE",
          "body": "I would vote for \"generic\" or \"general\".",
          "createdAt": "2020-08-06T00:27:00Z",
          "updatedAt": "2020-08-06T00:27:00Z"
        }
      ]
    },
    {
      "number": 48,
      "id": "MDU6SXNzdWU1NTYxMDAyMzI=",
      "title": "Reference the JSON specification",
      "url": "https://github.com/quiclog/internet-drafts/issues/48",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently JSON is mentioned, but not officially referenced.\r\nMake it clear that formats unspecified in the qlog draft should be taken from JSON spec (e.g., that booleans should be spelled `true` and `false`)\r\n\r\nthanks @hawkinsw for reporting",
      "createdAt": "2020-01-28T10:01:15Z",
      "updatedAt": "2020-09-05T16:23:51Z",
      "closedAt": "2020-09-05T16:23:51Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Was fixed in f5db7cdc8cd0cf37bfe5f1b0b4c54fc56ffc5f28",
          "createdAt": "2020-09-05T16:23:51Z",
          "updatedAt": "2020-09-05T16:23:51Z"
        }
      ]
    },
    {
      "number": 49,
      "id": "MDU6SXNzdWU1NTk2NjMwNTQ=",
      "title": "More fine-grained connection states",
      "url": "https://github.com/quiclog/internet-drafts/issues/49",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Per @huitema:\r\n\r\n` I would split the active state between \"start\" and \"confirmed\", and the handshake start on the server side between \"received a request\" and \"address validated\".`\r\n\r\n```\r\nThe server will go through the \"anti dos mitigation\" phase until the client's address is validated. That's important, because the server behavior in that state is restricted. Once it has sent all the handshake packets, the server goes to a \"false start\" phase in which it can send 1-RTT packets but (should) not receive. And then once it receives the client finished and sends the \"handshake done\", it moves to a confirmed state, at which point it can deal with migration and key update.\r\n\r\nClient is kind of the same. It goes from initiating to handshake, then to an \"almost ready\" phase after sending the \"client finished\" and getting the 1rtt keys. But it will only become really active once it receives the \"handshake done\" or an equivalent.\r\n```",
      "createdAt": "2020-02-04T11:40:06Z",
      "updatedAt": "2020-04-13T09:55:26Z",
      "closedAt": null,
      "comments": [
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "Defining connection states is hard, but here is what I would suggest:\r\n\r\n1) Client side, up to the \"ready\" state:\r\n* attempted (Initial sent, no handshake keys received yet)\r\n* handshake (handshake keys received)\r\n* almost ready (1RTT keys received, but handshake done not received yet)\r\n* ready (or active) (handshake done received from server, or equivalent)\r\n2) Server side:\r\n* received (initial received)\r\n* validated (client address has been verified)\r\n* handshake (handshake packets received from client, handshake in progress)\r\n* false start (1RTT keys write received, but handshake not complete yet)\r\n* ready (handshake complete, handshake done sent)\r\n3) Both sides:\r\n* draining (close connection packet sent or received, waiting for some time)\r\n* disconnected (done with this connection)\r\n\r\n",
          "createdAt": "2020-03-08T23:24:39Z",
          "updatedAt": "2020-03-08T23:24:39Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I like the addition of `validated`. That can be really useful when debugging the early handshake stages.\r\n\r\n> * almost ready (1RTT keys received, but handshake done not received yet)\r\n> * ready (or active) (handshake done received from server, or equivalent)\r\n\r\nThis could be `handshake_completed` and `handshake_confimred`. Both of them would work for client as well as the server.\r\n\r\n> false start (1RTT keys write received, but handshake not complete yet)\r\n\r\nNot sure if we need this. We already have a `key_updated` event.",
          "createdAt": "2020-04-12T12:08:56Z",
          "updatedAt": "2020-04-12T12:10:01Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Here's my proposal:\r\n\r\n```\r\nenum ConnectionState {\r\n    validated, // only for the server, when the client's IP has been validated\r\n    handshake_completed, // TLS handshake successful\r\n    handshake_confirmed, // handshake confirmed, see sec. 4.1.2 of the QUIC-TLS draft\r\n    draining, // CONNECTION_CLOSE sent\r\n    closed // connection actually fully closed, memory freed\r\n}\r\n```\r\n\r\n@rmarx, @huitema What do you think?",
          "createdAt": "2020-04-12T12:17:13Z",
          "updatedAt": "2020-04-12T12:17:13Z"
        },
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "@marten-seemann I think that's too coarse. You have to consider scenarios in which Initial packets are exchanged for some time before handshake keys are available, e.g., for post quantum, and scenarios in which handshake packets are exchanged for some time before 1RTT keys are available, e.g., client auth. Also, per 4.1.2, handshake confirmed on the server happens at exactly the same time as handshake confirmed. That's why I use a \"false start\" state for the server.\r\n\r\nWe are discussing logging options here. Having detailed logging options does not hurt, there is no point in being too parsimonious. If you only want to log a subset of them, that's a fine implementation choice, but that should not prevent precise logging for those who want it.",
          "createdAt": "2020-04-12T17:25:09Z",
          "updatedAt": "2020-04-12T17:25:09Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Also, per 4.1.2, handshake confirmed on the server happens at exactly the same time as handshake confirmed.\r\n\r\nTrue. This event would be kind of redundant for the server. It's also kind of redundant redundant for the client, since this is the time when the client drops Handshake keys.\r\n\r\n> @marten-seemann I think that's too coarse. You have to consider scenarios in which Initial packets are exchanged for some time before handshake keys are available, e.g., for post quantum, and scenarios in which handshake packets are exchanged for some time before 1RTT keys are available, e.g., client auth. [...] That's why I use a \"false start\" state for the server.\r\n\r\nFirst of all, I don't understand the name \"false start\". My point in https://github.com/quiclog/internet-drafts/issues/49#issuecomment-612604685 was that we already have an event for this: it's the [key updated](https://quiclog.github.io/internet-drafts/draft-marx-qlog-event-definitions-quic-h3.html#name-key_updated) event.\r\n\r\nNow it seems like both \"false start\", `handshake_comfirmed` and `handshake_completed` are all redundant, since they all accompanied by key generation / key discarding events. Not sure what to make of that...",
          "createdAt": "2020-04-13T09:55:26Z",
          "updatedAt": "2020-04-13T09:55:26Z"
        }
      ]
    },
    {
      "number": 50,
      "id": "MDU6SXNzdWU1NjAyMDI4Mjc=",
      "title": "general observations",
      "url": "https://github.com/quiclog/internet-drafts/issues/50",
      "state": "OPEN",
      "author": "xquery",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "The following is a 'bag' of general comments ... perhaps not so actionable but I did not find a mail list to send these kind of general observations.\r\n\r\n**scope**\r\n\r\nSome readers may try to contrast/compare this effort with higher level http logging formats (eg. NCSA, common log format and friends) ... might be worth reducing their confusion by adding some context eg. qlog sits near pcap.\r\n \r\nYou may consider explicitly constraining scope to h2/h3 and degrade gracefully for use with other protocols as a side effect of good design instead of assert broad applicability.\r\n\r\nThere is a statement of compliance attempted in 'tooling section' but this document defines a high level schema eg. I would have expected the only statement of compliance achievable (in this document) is the correct validation of schema against instance data.\r\n\r\nUnlike pcap (which is defined in terms of an api) qlog (so far) is defined in the form of a schema - hence I was looking for a clear definition of optional vs not required ... my expectation was for the core of qlog to be as small as possible.\r\n\r\nI would separate out protocol definition (endpoint) ... I am sure there exists a good IETF example of this ... but I am too lazy to find and will point you to a W3C set of specs as example https://www.w3.org/TR/2013/REC-sparql11-overview-20130321/\r\n\r\n**json**\r\n\r\nYou might consider adding some section on json convention/style (assert snake_case, etc) eg. remarking on challenges of using json for representing log data (limitations with datetime, decimal, no comments - all come to mind).\r\n\r\nYou are defining a high level schema (which for me implies no dependency on a concrete format like json) but as you have used json throughout to illustrate relationships/structure - I was looking (maybe missed it) for a non normative json schema definition.\r\n\r\n**keys**\r\n\r\nerror is buried in the text, should be normatively defined.\r\n\r\nI dislike the term 'vantage_point' ... I understand the requirements but maybe considering other terms like 'src' and 'target' are more appropriate.  \r\n\r\n**values**\r\n\r\nhave you considered defining a time unit in some human readable datetime (including tz notation ex.iso-8601 et al https://www.w3.org/International/core/2005/09/timezone.html)  \r\n\r\n**transforms**\r\nHave you considered demonstrating how transforms might work \r\nI like the way csv spec goes about this https://www.w3.org/2013/csvw/wiki/Main_Page\r\n\r\n**general** \r\n\r\nIt is unclear how easy for a database to index qlog formatted json. \r\n\r\nI think the section on 'Practical Use' might consider how compressed json compares to a binary format.\r\n\r\n",
      "createdAt": "2020-02-05T08:01:56Z",
      "updatedAt": "2020-02-05T08:01:56Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 51,
      "id": "MDU6SXNzdWU1NjE2NDczMTY=",
      "title": "QLOGDIR environment variable",
      "url": "https://github.com/quiclog/internet-drafts/issues/51",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Maybe the main schema should specify something like SSLKEYLOGFILE that implementations SHOULD adhere to. \r\n\r\nThough it needs to be a directory, not 1 file, since we probably don't want to dump all qlogs into 1 single file as we do with the SSL keys. \r\n\r\nA good option might be QLOGDIR.\r\n\r\nWe should also specify how to write/name individual files in that directory (or at least list options)\r\n\r\nTODO: find out where SSLKEYLOGFILE Is specified exactly (seems a bit difficult to google).\r\n\r\nCC @bagder @xquery",
      "createdAt": "2020-02-07T13:47:15Z",
      "updatedAt": "2020-09-01T19:13:31Z",
      "closedAt": "2020-09-01T19:13:31Z",
      "comments": [
        {
          "author": "bagder",
          "authorAssociation": "NONE",
          "body": "I don't think `SSLKEYLOGFILE` is specified anywhere and I don't know exactly how it came to exist, but I know that Firefox and Chrome with Wireshark have supported it a fairly long time and curl does too since a while back. It's just very convenient to have several applications agree on how to do this. I would very much like to have curl support the qlog variable as well.",
          "createdAt": "2020-02-07T13:50:23Z",
          "updatedAt": "2020-02-07T13:50:23Z"
        },
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "This wfm, I need a parameter to pass to control behaviour anyway and this avoids me having to make my own",
          "createdAt": "2020-02-07T15:17:49Z",
          "updatedAt": "2020-02-07T15:17:49Z"
        }
      ]
    },
    {
      "number": 52,
      "id": "MDU6SXNzdWU1NjYxMTQxNjQ=",
      "title": "mandatory new field in key_updated compromises security",
      "url": "https://github.com/quiclog/internet-drafts/issues/52",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "On a production system you probably don't want to log TLS secrets, even if you qlog (some of the) connections. The `new` field in the `key_updated` event therefore should not be mandatory. \r\n\r\nI'm not sure I understand the `old` field either. If you're logging 1-RTT key updates and their sequence numbers, the key would already be written to the qlog, so there's no need to export it again. Or am I missing something?\r\n\r\nMaybe it would be a good idea to keep key material to the SSLKEYLOGFILE and not even offer an option to write them to qlog?",
      "createdAt": "2020-02-17T07:49:23Z",
      "updatedAt": "2020-02-17T11:05:46Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Additionally, we should add an \"owner\" field to the `key_update` event.\r\n\r\nNow, difference between client/server keys is made with the trigger and also the KeyType: this should be made more consistent with the other events. See also #44. \r\n\r\nAn endpoint would then emit separate events for client and server key updates, which *should* work event if key calculation is delayed (though not 100% sure yet). \r\n",
          "createdAt": "2020-02-17T10:27:58Z",
          "updatedAt": "2020-02-17T11:05:46Z"
        }
      ]
    },
    {
      "number": 53,
      "id": "MDU6SXNzdWU1NjY5MTU2NjY=",
      "title": "Provide clearer usage advice",
      "url": "https://github.com/quiclog/internet-drafts/issues/53",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "At the moment, it's not entirely clear how qlog is \"supposed to be used\".\r\n\r\nFor example, it's not clear to implementers why some fields (e.g., quic version) are duplicated across `connection_started` and also the PacketHeader.\r\n\r\nWe should provide some examples of what things look like if you implement \"all of qlog\" and what should happen if you only implement the Core events (i.e., some fields in the Core events can be skipped if you're using the Base or Extra events instead). This is also important info for tool implementers. \r\n\r\nThis also depends on the use case: tracing while debugging vs production-level logging. ",
      "createdAt": "2020-02-18T14:05:55Z",
      "updatedAt": "2020-02-18T14:07:11Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 54,
      "id": "MDU6SXNzdWU1NzExOTQ3MDA=",
      "title": "Allow more wire-image indicators",
      "url": "https://github.com/quiclog/internet-drafts/issues/54",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, we abstract out a lot of the on-the-wire specifics.\r\n\r\nA good example is in the STREAM frame: there, the length and offset fields are optional, depending on how the frame should be handled. Simply having the same fields optional in qlog doesn't convey quite the same semantics: did the implementation simply not log them or were they not present from the start? \r\n\r\nMore importantly though: if no length is set, the frame extends to the end of the packet, so it does have a length, which you'd probably want to log (that's the way it's currently designed), but so you loose the info that the length field wasn't encoded on the wire. \r\n\r\nThis is just one example of similar problems across the board. I rather like the simplicity of the current setup, but we should have ways to signal the explicit wire image as well. \r\n\r\ncc @huitema",
      "createdAt": "2020-02-26T09:36:21Z",
      "updatedAt": "2020-02-26T09:36:21Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 55,
      "id": "MDU6SXNzdWU1NzExOTU3ODk=",
      "title": "More fields should be optional in ConnectionCloseFrame",
      "url": "https://github.com/quiclog/internet-drafts/issues/55",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, everything is required, which isn't optimal.\r\n\r\nAdditionally, qvis should use more of the present information. \r\n\r\ncc @huitema",
      "createdAt": "2020-02-26T09:38:09Z",
      "updatedAt": "2020-02-26T09:38:09Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 56,
      "id": "MDU6SXNzdWU1NzExOTg5Njk=",
      "title": "Allow logging of partial raw data",
      "url": "https://github.com/quiclog/internet-drafts/issues/56",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, we use the `raw` field in multiple events to allow logging of the raw data.\r\n\r\nIt would be interesting to allow logging for only the first x bytes, which can help in debugging.\r\n\r\nSome options:\r\n\r\n1. New `raw_partial` field, containing the partial hex-encoded data\r\n2. New `raw_length` field, indicating the length of the `raw` field. If it doesn't match the `length` parameter (or equivalent), we know `raw` is truncated\r\n3. No new fields: we can derive `raw`'s length from the size of the string. Then use similar logic as 2) to detect truncation\r\n4. No new fields, end `raw` field with `...` at the end of the string. Perfectly human readable, bit of a hassle when decoding the bytes to e.g., ASCII + less optimal for a potential binary format\r\n\r\nPersonally, I would prefer 3. We will strongly advise AGAINST logging `raw` in typical usage scenarios anyway: it's only to be used for core debugging. In these latter cases, the person debugging probably knows the stack a bit more deeply and understands that it's logging just partial raw values. \r\n\r\ncc @huitema",
      "createdAt": "2020-02-26T09:43:23Z",
      "updatedAt": "2020-02-26T09:43:23Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 57,
      "id": "MDU6SXNzdWU1NzEyMDA1NDc=",
      "title": "Add IP addresses to datagram_* events",
      "url": "https://github.com/quiclog/internet-drafts/issues/57",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "We currently extracted these to other events, assuming they wouldn't change or only change sparingly at specific times, handled by other events (e.g., when doing migration).\r\n\r\nHowever, this is not always the case. For example, @huitema mentioned:\r\n\r\n> NAT rebinding, probes of packets before migration\r\n\r\nFor these cases, it would be interesting to have (optional) IP from/to addresses in the datagram_* events as well. ",
      "createdAt": "2020-02-26T09:45:54Z",
      "updatedAt": "2020-02-26T09:45:54Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 58,
      "id": "MDU6SXNzdWU1NzEyNTQ1NDg=",
      "title": "Consider splitting up parameters_set",
      "url": "https://github.com/quiclog/internet-drafts/issues/58",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, the parameters related events aggregate all sorts of parameters, doesn't matter where they come from. Good example is transport:parameters_set, which contains mostly data that's transported via TLS (Transport params, alpn), but also some other fields (like QUIC version). \r\n\r\nSome people (@huitema, @marten-seemann) have advocated splitting up this event into others. I personally don't see the benefit of that, so this issue is to track outside arguments and proposed solutions.",
      "createdAt": "2020-02-26T11:09:41Z",
      "updatedAt": "2020-02-26T11:47:53Z",
      "closedAt": null,
      "comments": [
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "One problem with the current design is that it couples QUIC's transport parameters with the handshake. Some of that data is provided by TLS to QUIC, but other things like ALPN are not used by the QUIC transport. \r\n\r\nA more natural model could be a series of handshake events, as suggested by others.",
          "createdAt": "2020-02-26T11:47:53Z",
          "updatedAt": "2020-02-26T11:47:53Z"
        }
      ]
    },
    {
      "number": 59,
      "id": "MDU6SXNzdWU1NzEyNzIxMTI=",
      "title": "Add more TLS-specifics",
      "url": "https://github.com/quiclog/internet-drafts/issues/59",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, we have few options for logging a lot of TLS related data (e.g., cipher suites, supported groups, (E)SNI, ...). \r\n\r\nNeed to figure out a generic form for TLS (extensions) data so it can be logged using qlog as well. \r\n\r\nNote: this is getting in the \"new TCP+TLS+HTTP/2 schema\" territory ",
      "createdAt": "2020-02-26T11:37:15Z",
      "updatedAt": "2020-02-26T11:37:55Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 60,
      "id": "MDU6SXNzdWU1Nzc0NTA0NzY=",
      "title": "packet_dropped header_decrypt_error should be header_parse_error",
      "url": "https://github.com/quiclog/internet-drafts/issues/60",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Header decryption can never fail: it's just an XOR with a byte mask (header protection is not authenticated). You'd drop a packet though if header parsing fails, so maybe we can rename the trigger?",
      "createdAt": "2020-03-08T05:51:31Z",
      "updatedAt": "2020-03-08T05:51:31Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 62,
      "id": "MDU6SXNzdWU1Nzc1Mzk2MjY=",
      "title": "Logging of Retry and Version Negotiation packets",
      "url": "https://github.com/quiclog/internet-drafts/issues/62",
      "state": "OPEN",
      "author": "huitema",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Initial, Handshake, 0-RTT and 1-RTT packets have a payload composed of a set of frames. Retry and Version Negotiation packets don't. The payload of retry packets is a retry token; the version negotiation packets contain a list of proposed versions. I wonder what the proper logging should be.\r\n\r\nStaying close to the transport spec would get:\r\n```\r\n[118740, \"TRANSPORT\", \"PACKET_RECEIVED\", { \r\n    \"packet_type\": \"version\",\r\n    \"header\": { \r\n        \"packet_number\": \"0\",\r\n        \"packet_size\": 43,\r\n        \"payload_length\": 20,\r\n        \"scid\": \"96f7ef87d6603a79\",\r\n        \"dcid\": \"130b28a505315b13\" },\r\n    \"proposed_versions\": [ \"ff00001b\", \"ff000019\", \"8aca3a8a\" ]}]\r\n\r\n[113660, \"TRANSPORT\", \"PACKET_RECEIVED\", {\r\n    \"packet_type\": \"retry\",\r\n    \"header\": {\r\n        \"packet_number\": \"0\",\r\n        \"packet_size\": 80,\r\n        \"payload_length\": 32,\r\n        \"scid\": \"fa49f056d84c11c1\",\r\n        \"dcid\": \"5bdfe3c33300b8b8\" },\r\n    \"retry_token\": \"0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef\" }]\r\n```\r\nDoes this look right?",
      "createdAt": "2020-03-08T18:01:06Z",
      "updatedAt": "2020-03-08T18:01:06Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 64,
      "id": "MDU6SXNzdWU1ODA0Mjk3OTU=",
      "title": "Properly specify stateless reset, retry and migration",
      "url": "https://github.com/quiclog/internet-drafts/issues/64",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "These concepts were in flux when -01 was made and are now much more settled.\r\n\r\nAny thoughts on how to best approach this are more than welcome.",
      "createdAt": "2020-03-13T07:50:57Z",
      "updatedAt": "2020-03-28T10:24:20Z",
      "closedAt": "2020-03-28T10:24:20Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "For stateless reset, I can see two options:\r\n\r\n1. make it a new PacketType and then simply use `packet_received` / `packet_sent`\r\n2. make it a new event type with custom semantics\r\n\r\nAt least one implementer has indicated a preference for (1), since their stack only intends to support the core events. \r\n",
          "createdAt": "2020-03-13T08:05:59Z",
          "updatedAt": "2020-03-13T08:05:59Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "For stateless reset, both works for me. Most important thing is to get a way to log it at all, since this would be an interesting signal in production.",
          "createdAt": "2020-03-13T09:49:16Z",
          "updatedAt": "2020-03-13T09:49:16Z"
        }
      ]
    },
    {
      "number": 65,
      "id": "MDU6SXNzdWU1ODA0MzkwMTU=",
      "title": "Change data_moved to the transport category",
      "url": "https://github.com/quiclog/internet-drafts/issues/65",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, `data_moved` is in the HTTP category, but it's (application) protocol agnostic and can be used for other application-layer protocols as well.\r\n\r\ncc @marten-seemann ",
      "createdAt": "2020-03-13T08:11:40Z",
      "updatedAt": "2020-04-10T17:34:54Z",
      "closedAt": null,
      "comments": [
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "strong agree!",
          "createdAt": "2020-04-10T17:34:54Z",
          "updatedAt": "2020-04-10T17:34:54Z"
        }
      ]
    },
    {
      "number": 66,
      "id": "MDU6SXNzdWU1ODM1Njc3NDk=",
      "title": "relax ordering requirement",
      "url": "https://github.com/quiclog/internet-drafts/issues/66",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "The spec currently says:\r\n> Events in each individual trace MUST be logged in strictly ascending timestamp order (though not necessarily absolute value, for the \"delta_time\" setup). Tools are NOT expected to sort all events on the timestamp before processing them.\r\n\r\nThis poses problems for multi-threaded implementations that implement a streaming encoder. Since qlogs take up a lot of memory for long-lived / active connections, implementing a streaming encoder is the only way to run qlog in production, without risking to overflow the host's memory.\r\n\r\nIf you have a multithreaded application, it's not possible to guarantee that the events will be strictly ordered by timestamp.",
      "createdAt": "2020-03-18T08:51:22Z",
      "updatedAt": "2020-03-18T11:04:50Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "I've been thinking about this while implementing an automated sort() in qvis to prevent this from going wrong (kazu also had this problem).\r\n\r\nIn and of itself, it's not necessary to -log- in order I think. But it saves a lot of time in (especially web-based) tooling if you don't have to sort all logs prior to using them. So we could say something like:\r\n\r\n> Events SHOULD be logged in ascending timestamp order. Tools are not expected to do sorting. If an implementation cannot guarantee ordered logs, they can use a postprocessing step to order the logs if their tool of choice does not offer automatic sorting.\r\n\r\nThis is similar to wireshark's approach IIUC (https://www.wireshark.org/docs/wsug_html_chunked/AppToolsreordercap.html)\r\n\r\nThough I am not 100% sure that sort() solves all problems... we've had some issues with wireshark recently where packets captured on different interfaces (but ending up in the same pcap) had quite a bit of timestamp differences and re-ordering on the timestamp field would be obviously wrong. Now, this is probably an artifact of the internal tcpdump implementation etc. and QUIC/H3 stacks should be much less affected. However, given that qlog is envisioned as a wider format (and because I can envision people splicing pcap-to-qlog with direct-qlog events from the application), I can't simply dismiss this. \r\n\r\n",
          "createdAt": "2020-03-18T09:03:28Z",
          "updatedAt": "2020-03-18T09:03:28Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "The whole point of using a streaming encoder is to not have to load the whole qlog file into memory at the same time. A post-processing step (which would have to happen when the whole qlog is written) would require me to do exactly that, so that's not an option.",
          "createdAt": "2020-03-18T09:07:12Z",
          "updatedAt": "2020-03-18T09:07:12Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "I'm thinking more of an \"offline\" post-processing step. e.g., either in-bulk by a separate server/process as a cron-job or immediately prior to loading into a tool that doesn't support ordering itself. \r\n\r\nFor example, qvis currently detects if traces are un-ordered and does a sort() if that's the case. You could see that as a \"post processing step\" built in to the tooling itself. This is plenty fast enough for smaller traces (say up to 100MB) but can be slower for larger ones (though those aren't optimal in qvis either way).\r\n\r\nI think the whole point will be a bit moot in practice, since I would expect tools to have this built-in in practice, but that doesn't mean I would want to require this in the text. \r\n\r\nMaybe @nibanks has some insight into how windows ETW handles this type of thing? ",
          "createdAt": "2020-03-18T09:13:10Z",
          "updatedAt": "2020-03-18T09:13:29Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "That would be possible, but kind of ugly, since it would mean that a QUIC implementation cannot export semi-valid qlog on its own, and is reliant on external programs to fix the exported data. ",
          "createdAt": "2020-03-18T09:20:25Z",
          "updatedAt": "2020-03-18T09:20:25Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "So I'm not sure what the solution would be then unless you'd force the tools to sort the logs themselves. Is that what you're advocating for? ",
          "createdAt": "2020-03-18T11:04:50Z",
          "updatedAt": "2020-03-18T11:04:50Z"
        }
      ]
    },
    {
      "number": 69,
      "id": "MDU6SXNzdWU1ODkxNDQzOTY=",
      "title": "PTO is per PN-space",
      "url": "https://github.com/quiclog/internet-drafts/issues/69",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, we log `pto_count` in `metrics_updated`. However, we can have multiple PTO's running across multiple PN spaces and there is no proper way to correlate them with a PN space in metrics_updated.\r\n\r\nThis also impacts the `loss_timer*` event types. \r\n\r\nPossible resolutions:\r\n- Add pn_space attribute to `metrics_updated`\r\n- Split PTO count into a separate event (potentially merge with loss_timer?)\r\n\r\nTODO: reason more about how these metrics are tied to PN spaces in general (are there others than PTO that are duplicated)\r\n\r\ncc @marten-seemann ",
      "createdAt": "2020-03-27T13:45:29Z",
      "updatedAt": "2020-03-28T10:29:31Z",
      "closedAt": "2020-03-28T10:29:31Z",
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> * Add pn_space attribute to metrics_updated\r\n\r\nMost other metrics in the `metrics_updated` event are independent of the PN space, so that would be weird.\r\n\r\n> * Split PTO count into a separate event (potentially merge with loss_timer?)\r\n\r\nI'd prefer to do that. Not sure if it makes sense to merge is with the loss timer events, since we also need to be able to log a reset of the PTO count to 0 (which doesn't require setting of a timer).",
          "createdAt": "2020-03-27T13:51:04Z",
          "updatedAt": "2020-03-27T13:51:04Z"
        }
      ]
    },
    {
      "number": 72,
      "id": "MDU6SXNzdWU1ODk1NTEzNzU=",
      "title": "Allow listing which events were supported by the logger",
      "url": "https://github.com/quiclog/internet-drafts/issues/72",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "When looking at a qlog at this time, if there is a certain event type missing, you don't know if it's because those events never occurred or if the implementation simply doesn't support/log that event type. \r\n\r\nThis is especially troublesome if you want to selectively enable/disable certain types to reduce overhead when you're doing targeted \"live\" debugging on a deployed system.\r\n\r\nThe simplest solution I can see is to add a new field to `configuration`, for example:\r\n\r\n> supportedEvents:Array\\<string\\> // each string is category:event_type (e.g., transport:packet_sent)\r\n\r\nSome bikeshedding can help to decide if we need two separate fields (supportedCategories separately for example) or if we can cut out the category in here completely (as there is little overlap in event names atm).\r\n\r\nThis would also be interesting for doing a fast \"is this trace compatible with this visualization\"-check in qvis and others tools. \r\n\r\nThanks to @mjoras for reporting.",
      "createdAt": "2020-03-28T10:36:52Z",
      "updatedAt": "2020-03-29T10:53:44Z",
      "closedAt": null,
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "This only captures a subset of the possible changes. For example, you wouldn't be able to tell if a  qlog implementation already supported the recently added `stateless_reset` packet type or not.\r\n\r\nIn the end, I think it makes more sense to add some kind of versioning scheme to your qlog file (which is orthogonal to the `qlog_version`), and would not be interpreted by tools.",
          "createdAt": "2020-03-29T08:53:36Z",
          "updatedAt": "2020-03-29T08:53:36Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "I'm not sure I follow you here...\r\nIf the implementation doesn't support stateless_reset, it won't log them either in the first place, so them being absent from the `supportedEvents` list is the same.\r\n\r\nThis is not really intended to show what an implementation \"supports\" but more what is has \"enabled\". The situation is that as a user, you get a random qlog file in front of you, on which you need to do analysis. You're looking for the occurrence of a certain event type, but it isn't there. Is that because the event type wasn't being logged for this log (e.g., mvfst wants to selectively log only -some- of the supported event types to keep overhead down) or because it actually never occurred during the connection (e.g., signalling the bug you're looking for).\r\n\r\nCould you expand on the orthogonal versioning scheme? For your example of the `stateless_reset`, that would just be `draft-02` going forward from my viewpoint. \r\n\r\n\r\n\r\n",
          "createdAt": "2020-03-29T10:53:44Z",
          "updatedAt": "2020-03-29T10:53:44Z"
        }
      ]
    },
    {
      "number": 74,
      "id": "MDU6SXNzdWU1ODk2MDg4Nzg=",
      "title": "Add packet_number to frames_processed",
      "url": "https://github.com/quiclog/internet-drafts/issues/74",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "With recent discussions on that it's sometimes expensive to log parsed frames in packet_received/packet_sent directly, it would be interesting to make frames_processed a bit more usable in practice.\r\n\r\nGiving it a packet_number allows it to be more easily linked to a specific packet and thus can be combined with packet_sent/received.\r\n\r\nThough, maybe we should rather go for a frame_created/frame_parsed equivalent to H3, as _processed only implies _parsed at this point. In that vein, maybe make it multiple frameS instead of just 1? ",
      "createdAt": "2020-03-28T16:19:26Z",
      "updatedAt": "2020-09-07T13:37:28Z",
      "closedAt": "2020-09-07T13:37:28Z",
      "comments": []
    },
    {
      "number": 75,
      "id": "MDU6SXNzdWU1OTAyMDI5MDU=",
      "title": "Add way to log contents of version negotiation packet",
      "url": "https://github.com/quiclog/internet-drafts/issues/75",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, there is no way to do this.\r\n\r\nPossible approaches:\r\n1. add `supported_versions:Array<string>` to `parameters_set` (similar to ALPN in #28)\r\n2. add `supported_versions:Array<string>` to the `packet_*` events (similar to how we do stateless_reset\r\n\r\nI personally have a preference for nr. 2, since it's consistent with how other packets are handled. ALPN is after all indeed a parameter, not a full packet. \r\n\r\ncc @mpiraux",
      "createdAt": "2020-03-30T11:09:53Z",
      "updatedAt": "2020-04-14T09:37:13Z",
      "closedAt": null,
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I'd argue that we need both. The supported versions is a parameter of the server that's interesting to know independent of if version negotiation was ever performed or not.\r\n\r\nFor the client, it's important to have a way to log the contents of a version negotiation packet, so adding it to the `packet_received` event makes sense.",
          "createdAt": "2020-04-14T09:37:13Z",
          "updatedAt": "2020-04-14T09:37:13Z"
        }
      ]
    },
    {
      "number": 76,
      "id": "MDU6SXNzdWU1OTU2NzI3NDI=",
      "title": "packet_buffered should have a packet_size field",
      "url": "https://github.com/quiclog/internet-drafts/issues/76",
      "state": "CLOSED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "This field would be really helpful for debugging. Without a packet size, there's no way to correlate a buffered packet to a packet that is later dequeued from the buffer.",
      "createdAt": "2020-04-07T08:01:26Z",
      "updatedAt": "2020-07-22T07:48:08Z",
      "closedAt": "2020-07-22T07:48:08Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Good point.\r\n\r\nReminder to me: maybe mention in the comments that it's primarily useful when you're buffering because keys were not available yet (e.g., during handshake).\r\n\r\nAlso: related to #40 ",
          "createdAt": "2020-04-08T09:07:03Z",
          "updatedAt": "2020-04-08T09:07:20Z"
        }
      ]
    },
    {
      "number": 78,
      "id": "MDU6SXNzdWU1OTg0MDUzODI=",
      "title": "revisit connection_state_updated",
      "url": "https://github.com/quiclog/internet-drafts/issues/78",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "I see the following problems with the `connection_state_updated` event:\r\n1. `attempted` seems to be a duplicate of `connection_started` for the client (i.e. the two events would always be emitted at the same time)\r\n2. `reset` is impossible to log. You send a stateless reset if you lost state for a connection (after a crash or reboot, or if a packet was misrouted to the wrong backend server). In any case, there\u2019s no connection state to update by definition.\r\n3. `handshake` seems to be a duplicate of connection_started for both sides.\r\n\r\nI think these events could be removed without losing any information in the log.\r\n\r\nI suggest adding a new event for [confirmation of the handshake](https://quicwg.org/base-drafts/draft-ietf-quic-tls.html#name-handshake-confirmed). Maybe it would make sense to rename `active` to `handshake_completed` and add a `handshake_confirmed`?",
      "createdAt": "2020-04-12T04:01:04Z",
      "updatedAt": "2020-08-02T10:37:53Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Potential duplicate of #49, should be handled together. \r\n\r\n(meaning that, yes, this should be handled for -02)",
          "createdAt": "2020-04-12T11:31:59Z",
          "updatedAt": "2020-04-12T11:31:59Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "After some thinking, I feel it might worthwhile to see if we can transform `connection_started` into a `parameters_set` equivalent for the `connectivity` category.",
          "createdAt": "2020-04-12T12:11:05Z",
          "updatedAt": "2020-04-12T12:11:05Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Adding to my list, `keepalive` should probably not be a connection state. The QUIC specification doesn't  say a lot about keep-alives other than you can send a packet once in a while if you have nothing else to send.\r\nMaybe it's enough to set this as a `trigger` on the `packet_sent` event?",
          "createdAt": "2020-04-12T12:16:04Z",
          "updatedAt": "2020-04-12T12:16:04Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "@marten-seemann just reported on slack that a separate `connection_closed` event would be interesting to log the reason the connection was closed (for which we're now either relying on the CONNECTION_CLOSE frame contents or an (internal_)error event). \r\n\r\nIt seem like adding a `reason` field to `connection_state_updated` would work there too.\r\nAlternatively, this could be part of the envisioned `parameters_set` connectivity event that would replace both separate `connection_started` and `connection_close` semantics, though I'd feel a bit uneasy putting a `reason` field in `parameters_set` (semantics seem off).  \r\n\r\nBest to take a step back and look at this from a higher viewpoint to see if making all this generic gives us more benefits than simply having separate events for everything. \r\n",
          "createdAt": "2020-04-20T15:45:58Z",
          "updatedAt": "2020-04-20T15:46:10Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Here's my suggestion for a better `connection_state_updated` definition:\r\n```\r\nenum ConnectionState {\r\n    client_address_verified, // when the server considers the client's source address verified\r\n    handshake_complete, // handshake completed\r\n    handshake_confirmed, // handshake confirmed\r\n    draining, // CONNECTION_CLOSE sent\r\n    closed // connection actually fully closed, memory freed\r\n}\r\n```\r\n\r\nI added a `client_address_verified` to my earlier suggestion, since with https://github.com/quicwg/base-drafts/pull/3924, servers can make this decision based on multiple different criteria.\r\n\r\n@rmarx What do you think?",
          "createdAt": "2020-07-27T04:18:33Z",
          "updatedAt": "2020-07-27T04:18:33Z"
        },
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "I like splitting `active` into `handshake_complete` and `handshake_confirmed` as @marten-seemann is suggesting above, and I also like `client address verified` as an intermediate server state. On the client side, I would like to see logging of the transition between `started` and `heard_from_the_server`, which is pretty much the client equivalent of `client_address_verified`.\r\n\r\nOn the other hand, all these states can be trivially deduced from observing the packet flow. @rmarx is the connection_state event really useful?",
          "createdAt": "2020-08-02T00:25:57Z",
          "updatedAt": "2020-08-02T00:25:57Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "I have to re-read the draft myself first, but a merge of Marten's proposal (with a \"started\" state or similar put back in) and Christian's suggestion of `heard_from_server` seems sensible at first glance.\r\n\r\nWith regards to \"do we need this event at all\"... it depends. If you're logging all packets: potentially not, though that might hide implementation bugs (state transfer too early/late). However, as @marten-seemann has also said wrt a separate `connection_closed` event (instead of simply watching the `connection_state_updated` for this) and the discussion on needing a `packet_acked` event (#107): when writing tools that try to quickly deduce these state transitions from a trace, it can be useful to have these \"duplicate\" events as well. I don't think I'd write many tools that fully rely on only these myself, and I wouldn't mark them as \"core\" events in the spec, but I can see why some people like to keep them in and have a preference for this myself as well (up to a certain point...)",
          "createdAt": "2020-08-02T10:37:53Z",
          "updatedAt": "2020-08-02T10:37:53Z"
        }
      ]
    },
    {
      "number": 79,
      "id": "MDU6SXNzdWU1OTg0NzE3MTI=",
      "title": "Add support for connection migration",
      "url": "https://github.com/quiclog/internet-drafts/issues/79",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Now that this is a bit more mature, it should be added to draft-02.\r\n\r\nVia @marten-seemann:\r\n> So I would assume that would best be modeled using a `path_probed` event (which has ips, ports, CID), and then a `path_confirmed` and a `path_abandoned` event\r\n\r\nSomewhat ties into #49 and #78 (and all other connectivity events really...).\r\nAlso look into how this is modeled in qlog for multipath by UCL (@mpiraux)",
      "createdAt": "2020-04-12T12:13:33Z",
      "updatedAt": "2020-04-12T12:13:33Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 80,
      "id": "MDU6SXNzdWU1OTg4MDAyNTQ=",
      "title": "Remove trigger fields from individual events",
      "url": "https://github.com/quiclog/internet-drafts/issues/80",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Triggers are now said to be a global optional property of all \"data\" fields in an event.\r\n\r\nYet, some events (like `key_retired` and `key_updated` still include them. Make sure they don't.\r\n\r\ncc @marten-seemann ",
      "createdAt": "2020-04-13T10:07:15Z",
      "updatedAt": "2020-09-07T10:27:46Z",
      "closedAt": "2020-09-07T10:27:46Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "There were more of these than I expected, but all have been removed now. ",
          "createdAt": "2020-09-07T10:27:46Z",
          "updatedAt": "2020-09-07T10:27:46Z"
        }
      ]
    },
    {
      "number": 85,
      "id": "MDU6SXNzdWU2MDM2ODIzNzM=",
      "title": "split up events",
      "url": "https://github.com/quiclog/internet-drafts/issues/85",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "This is a high-level observation based on my experience of 1. adding qlog support for quic-go and 2. writing some tooling to sift through a pile of logs.\r\n\r\nWhen adding qlog support for quic-go, I already found it confusing that qlog bundles things that have little to do with each other into the same event. Examples for these are:\r\n* The values in `parameters_set` event in `transport` that are not part of the QUIC transport parameters.\r\n* The `pto_count` in `metrics_updated`.\r\n* The `connection_state_updated` event. This is the worst in this list.\r\n\r\nIn my implementation, I worked around this by exposing an API that exposes different functions, which then would encode the same qlog event (with the respective fields set). For example, I have a `qlog.UpdatedPTOCount()` and a `qlog.UpdatedMetrics()`, which both emit a `metrics_updated` event.\r\n\r\nWriting my own qlog parser, I'm now encountering the same problem again. For example, one interesting thing to monitor on a production system would be the reason why a connection was closed (to see if there are any PROTOCOL_VIOLATIONS, for example). It would make filtering much easier if there was a `connection_closed` event, instead of having to look at optional fields in all the `connection_state_updated` events emitted over the lifetime of the connection. A similar argument applies to the PTO count (Connections that experience a lot of PTO events might be interesting to look at. Connections that collect many RTT samples are not).\r\n\r\nFrom my discussions with @rmarx (and I hope that I'm paraphrasing him correctly here), the main argument for having fewer events was that it makes things easier to implement. I hope that I laid out my argument here that the opposite is the case.",
      "createdAt": "2020-04-21T03:43:01Z",
      "updatedAt": "2020-07-08T08:48:23Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "So to me, this is a core question that I'd like to resolve before finalizing draft-02.\r\n\r\nMy viewpoint has indeed always been that too many individual events and definitions would lead people to support only a minor subset. As it is, even with heavy coalescing, we have a large amount of events today and many implementations only implement a subset of those. \r\n\r\nThat being said, I can see the appeal of splitting things out again, yet that would mean doing that at all layers (e.g., you can't have everything split out at `transport` but keep everything aggregated in `http` (or am I wrong about that?)).\r\n\r\nThere is another aspect of \"backwards compatibility\" with existing implementations that are being re-used (e.g., Lucas' Rust crate, qvis). I'm not sure if people are willing to make major changes at this point. On the other end: we probably shouldn't get bogged down by that at this point yet.\r\n\r\nSo I'd like some more input on this from other active implementers/contributors: @lpardue, @huitema, @jlaine, @nibanks, @mpiraux\r\n\r\n\r\n\r\n",
          "createdAt": "2020-04-21T08:41:07Z",
          "updatedAt": "2020-04-21T08:41:07Z"
        },
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "The crate is easy to update, I don't have many consumers of it right now and I do not have concerns about backwards compatibility. As long as qlog is versioned (which it is) I have no qualms about breaking changes. \r\n\r\nIn summary, qlog crate is not a concern in the decision you make",
          "createdAt": "2020-04-21T15:26:22Z",
          "updatedAt": "2020-04-21T15:26:22Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "After further discussion on slack, I think @marten-seemann's overall viewpoint is:\r\n\r\n- Events can be grouped, but only if they (a) logically belong together and/or (b) share the same fields\r\n\r\nThis leads to the following concrete example changes:\r\n- In the example of `connection_state_updated`, if there is a `reason` for the `connection_close` (see #78), that reason won't be present for `connection_attempted` and so it makes sense to split up those events.\r\n- In the example of `metrics_updated`, almost everything happens as the result of receiving an ACK, but `pto_count` does not (due to timer firing), so they should be split up as well. (tangent: max_ack_delay is a constant and should probably be in a `parameters_set` recovery equivalent)\r\n\r\nTentative conclusion: we don't need to split up everything as it stands now, just go through the events and make sure they adhere to the general principle above. I tend to agree with that. \r\n\r\nA prime candidate for re-evaluation is `transport:parameters_set`\r\n\r\n",
          "createdAt": "2020-07-08T08:38:47Z",
          "updatedAt": "2020-07-08T08:48:23Z"
        }
      ]
    },
    {
      "number": 86,
      "id": "MDU6SXNzdWU2MTA2NDIwNTU=",
      "title": "Need to add support for the token in an Initial packet",
      "url": "https://github.com/quiclog/internet-drafts/issues/86",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Probably needs to be in the PacketHeader struct",
      "createdAt": "2020-05-01T08:41:49Z",
      "updatedAt": "2020-05-01T08:41:49Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 88,
      "id": "MDU6SXNzdWU2MjM4MTk0NDk=",
      "title": "How to log transport parameters restored for 0-RTT",
      "url": "https://github.com/quiclog/internet-drafts/issues/88",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "When initiating a 0-RTT connection, the client restores transport parameters that it remembers from the last connection. Currently, it has to use the `parameters_set` event for this, with `owner` equal to `server`.\r\n\r\nAs I've already argued in #85, the `parameters_set` event would benefit from being split up. Taking this issue into consideration, maybe `parameters_sent`, `parameters_received` and `parameters_restored` would be a good fit?",
      "createdAt": "2020-05-24T08:24:27Z",
      "updatedAt": "2020-05-24T08:24:27Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 89,
      "id": "MDU6SXNzdWU2MjQxMzIzNTg=",
      "title": "Make traditional JSON the default, provide two optimization options",
      "url": "https://github.com/quiclog/internet-drafts/issues/89",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, we utilize what I'd call a \"csv\" optimization, where the `time, category, event name and data fields` omit their keys (these are defined once in `event_fields`, much like csv columns are defined on top) and only log their values. This saves some bytes, but is non-trivial to parse (need to remember the order of the columns to know which value represents what) and to serialize (serializers typically don't like this type of thing).\r\n\r\nThis also breaks a bit with traditional JSON, which just lists the key names for every field as well. Using normal JSON would make it easier to serialize and parse, but increase the file size. \r\n\r\nThere is a third optimized option, which is to replace all keys (and potentially also values) with indices into a lookup table/dictionary. This was discussed in #30 and, especially in combination with compression, seems like a good optimization option. \r\n\r\nSo, to summarize, we have three options:\r\n1. csv column optimization (current default and only supported)\r\n2. pure JSON\r\n3. dictionary optimization\r\n\r\nThe proposal is to switch qlog to nr 2 as default, and present nr 1 and 3 as optional optimizations (which can also be done post-hoc and offline). There would not be a given static dictionary (at least not in draft-02), with the dynamic dictionary always included in the qlog. Implementations are of course free to provide a built-in static dictionary themselves.\r\n\r\nTools would be required to only support nr 2, with optional support for 1 and 3. qvis would support all 3. \r\n\r\nCompression (gzip, brotli or other) would be also described but not mandated (qvis already supports gzip and brotli atm). \r\n\r\ncc @marten-seemann @LPardue\r\n\r\nI also know @martinthomson had strong opinions on this, so it would be interesting to hear from you as I'd like to get this right for -02 now. ",
      "createdAt": "2020-05-25T08:22:55Z",
      "updatedAt": "2020-09-05T16:15:21Z",
      "closedAt": "2020-09-05T16:15:20Z",
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I'd prefer to only have only 2. The choice of JSON as a logging format already means that we don't care about file size. I see little point in introducing piecemeal improvements to JSON's inefficiency to create what is essentially a new file format.\r\n\r\nIn addition to 2 I'd like to see a binary format (I know I've been nagging with this for a long time) that was built with efficiency considerations from the ground up.",
          "createdAt": "2020-05-25T08:28:11Z",
          "updatedAt": "2020-05-25T08:28:11Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Moving to only/mainly 2 does make a binary format a lot easier I think, as things can be more directly mapped and interchanged (at least from what I know now of protobufs). \r\n\r\nOne option would be to say: qlog draft is format agnostic (but uses JSON as examples because it's readable) and the events can be presented in other formats as well (i.e., qlog just defines the fields and their types per event, not their eventual \"on the wire\" serialization). Tools should provide conversions between other forms and JSON if they want to support alternate forms. Then, anyone can just make e.g., a protobuf format based on that. \r\n\r\nI am planning to move away from TypeScript format for the event definitions anyway, as it's needed to define e.g., the difference between 64-bit numbers, smaller numbers and strings (see #39). That requires a custom mapping to TypeScript/JSON anyway, so a protobuf (or similar) mapping fits in that logic too. ",
          "createdAt": "2020-05-25T09:41:36Z",
          "updatedAt": "2020-05-25T09:41:36Z"
        },
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "I think there's a good chance I could switch the `qlog` crate to 2) without an API change. Addding an API method would allow user selection, and be non-breaking in the first case.\r\n\r\nIt seems nice if logs could self-identify the serialization format, otherwise you're relying on tools to do heuristics. This could be done with file extensions, or qlog parameters, or file magic. It's probably hard to get complete coverage but something might be a quick win.",
          "createdAt": "2020-05-25T13:09:30Z",
          "updatedAt": "2020-05-25T13:09:46Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I was wondering if CBOR should be the standard format. The representation is very similar to JSON, but it provides better typing support:\r\n1. it can unambiguously represent uint64s (so #39 would be solved)\r\n2. it supports byte strings, eliminating the need for byte to string conversions (as remarked in #30). Those happen quite often, e.g. for connection IDs, retry token, stateless reset tokens etc.\r\n\r\nI'm not sure human readability should be a goal. For my part, I've never felt the urge to look at a raw qlog file, mostly thanks to amazing tools like qvis. I don't expect this situation to shift in favor of viewing raw files in the future, if at all, more tools will make it even less necessary to do so.\r\nThat being said, CBOR could easily be converted to a human-readable file format, should the need to manually inspect a trace arise.\r\n\r\nThere are also moderate file size savings, but those wouldn't be the main motivation here. CBOR of course doesn't provide any compression of map keys / values, so I'd expect CBOR qlogs to still be quite wasteful. Not sure if there's a schema-less serialization format that can use something like a dictionary to compress commonly used identifiers.",
          "createdAt": "2020-05-26T03:40:57Z",
          "updatedAt": "2020-05-26T03:44:55Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Finished this transformation in f5db7cdc8cd0cf37bfe5f1b0b4c54fc56ffc5f28.\r\n\r\nDraft 02 will use default JSON as prime format, with NDJSON for streaming. CBOR is discussed as an option, but provides few benefits over normal JSON when paired with compression. \r\n\r\nThe text now also includes (limited) discussion and reasoning on the choice of JSON as the default format, per tests mainly discussed in #30. \r\n",
          "createdAt": "2020-09-05T16:15:20Z",
          "updatedAt": "2020-09-05T16:15:20Z"
        }
      ]
    },
    {
      "number": 90,
      "id": "MDU6SXNzdWU2MjQ3MTE5Mjg=",
      "title": "Add events indicating reasons for stalls",
      "url": "https://github.com/quiclog/internet-drafts/issues/90",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "For example, an application might have more data ready to send, but cannot due to Flow Control or Congestion Control or Anti-Amplification limits. \r\n\r\nThese are currently only observable for the Flow Control case and only if the sender sends *_BLOCKED frames (which is optional). \r\n\r\nI'm not sure in which category this type of event would belong though... maybe look at msquic's handling (they have this type of event for sure). \r\n\r\ncc @scw00",
      "createdAt": "2020-05-26T08:50:30Z",
      "updatedAt": "2020-05-26T08:56:56Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 91,
      "id": "MDU6SXNzdWU2MjQ3MTQ4MzM=",
      "title": "Add mechanism for tracking coalesced packets",
      "url": "https://github.com/quiclog/internet-drafts/issues/91",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, this can be done implicitly by looking at `datagrams_sent` and `datagrams_received` events, but: a) these are very chatty if enabled for the entire connection and b) it's still implicit.\r\n\r\nOne proposed option (cc @marten-seemann) would be to add an optional `datagram_id` field to the `packet_*` and `datagrams_*` events that allows explicit correlation. This id would be an implementation-specific thing though, since QUIC itself does not define it (can be as easy as an implementation incrementing a counter).\r\n\r\nAnother point from Marten:\r\n> Note that this would also allow to correlate packets that processed at different times: assume that one packet contained in a datagram can be processed immediately, but it is coalesced with a packet of an encyrption level that the endpoint doesn\u2019t have keys for, and needs to buffer first\r\n\r\nIn that case, we probably also need to add the datagram_id to the `packet_buffered` event",
      "createdAt": "2020-05-26T08:54:28Z",
      "updatedAt": "2020-05-26T09:07:47Z",
      "closedAt": null,
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "And to `packet_dropped`.",
          "createdAt": "2020-05-26T09:07:47Z",
          "updatedAt": "2020-05-26T09:07:47Z"
        }
      ]
    },
    {
      "number": 94,
      "id": "MDU6SXNzdWU2NDIzNTgwMDc=",
      "title": "Add support for retry token and integrity tag",
      "url": "https://github.com/quiclog/internet-drafts/issues/94",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Related to #86 , probably also best at home in the PacketHeader struct.\r\n\r\nProposal:\r\n- use a single `token` field for both retry and initial tokens\r\n- use a single `token_length` field for initial (explicit) and retry (implicit), `integrity_tag` field for retry",
      "createdAt": "2020-06-20T11:16:37Z",
      "updatedAt": "2020-07-08T08:58:20Z",
      "closedAt": null,
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I don\u2019t think we need to log the integrity tag. We also don\u2019t log the AEAD tag for normal packets.",
          "createdAt": "2020-06-20T12:37:16Z",
          "updatedAt": "2020-06-20T12:37:16Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "It would probably be interesting to log if the token is a Retry token or a NEW_TOKEN token.\r\nFor the client, this bit of information is trivially available, for the server it is after it unprotects the token (assuming unprotecting succeeds), as servers are required to be able to distinguish between Retry and NEW_TOKEN tokens.",
          "createdAt": "2020-07-08T08:58:20Z",
          "updatedAt": "2020-07-08T08:58:20Z"
        }
      ]
    },
    {
      "number": 95,
      "id": "MDU6SXNzdWU2NTMwODU3NTI=",
      "title": "remove the time_units configuration",
      "url": "https://github.com/quiclog/internet-drafts/issues/95",
      "state": "CLOSED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "All time values are encoded as floats, so it doesn't make any difference if we encode them in ms or in \u03bcs.\r\n\r\nRemoving the configuration option makes it easier to implement qlog parsers, since there's less state to carry around to parse events.",
      "createdAt": "2020-07-08T08:09:14Z",
      "updatedAt": "2020-07-11T20:49:16Z",
      "closedAt": "2020-07-11T20:49:16Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "While I agree it makes it more difficult to parse, I'm not sure we can remove this?\r\n\r\nWould you advocate for forcing implementation to log either ms or \u03bcs (and do x1000 or /1000 if they use the other internally) then? Currently, I think we have about 4 implementations doing \u03bcs. ",
          "createdAt": "2020-07-08T08:12:48Z",
          "updatedAt": "2020-07-08T08:12:48Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "It doesn't matter which one is removed. Currently, I'm logging in ms (just because it's the default).\r\nChanging this to \u03bcs would basically just being a change from calling `duration.Milliseconds()` to `duration.Microseconds()` (see https://github.com/lucas-clemente/quic-go/blob/master/qlog/event.go#L14). I assume it's the same for other implementations: You have one (language/implementaton-specific) time representation, and you convert that to whatever unit you export to qlog.",
          "createdAt": "2020-07-08T08:27:47Z",
          "updatedAt": "2020-07-08T08:27:47Z"
        }
      ]
    },
    {
      "number": 96,
      "id": "MDU6SXNzdWU2NTMxMDY1MTI=",
      "title": "in_recovery is a congestion_state_updated event and doesn't belong in metrics_updated",
      "url": "https://github.com/quiclog/internet-drafts/issues/96",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "indicating whether the CC has entered recovery can currently be done in two ways, should probably removed this from `metrics_updated`.\r\n\r\ncc @marten-seemann ",
      "createdAt": "2020-07-08T08:40:20Z",
      "updatedAt": "2020-07-08T09:29:59Z",
      "closedAt": "2020-07-08T09:29:59Z",
      "comments": []
    },
    {
      "number": 97,
      "id": "MDU6SXNzdWU2NTMxMTA2OTA=",
      "title": "Indicate (encoded) sizes",
      "url": "https://github.com/quiclog/internet-drafts/issues/97",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently we do not have many fields for indicating (compressed) sizes of things.\r\n\r\nWe have the `packet_size`, but this is in the wrong place (#40) and `payload_length` (though it's not well-defined whether packet_size includes the TLS suffix). \r\n\r\nHowever, we lack size indicators for datagrams and, especially frames. When creating the qvis packetization diagram, this was very annoying, as I'd have to reconstruct the encoded frame headers to properly render those (and even then there might be rounding issues if the sender didn't encode things in the absolute minimum of bytes allowed). \r\n\r\nIn comparison, wireshark output does include exact sizes for these fields for TCP/TLS/H2 and this was quite handy. \r\n\r\nAnother point: need some way to indicate the used TLS AEAD tag length (though that should be constant for the entire connection and depends on the used cipher IIUC. For TCP+TLS from Wireshark I also had to manually calculate this from the cipher definitions though, which was a PITA). \r\n\r\nFinally, make terminology consistent (_size vs _length)\r\n",
      "createdAt": "2020-07-08T08:46:33Z",
      "updatedAt": "2020-08-06T14:10:11Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "From a discussion on slack, most are in favor of adding AEAD length to the general packet_size field (one reasons is because the tag also contributes to cwnd calculation)",
          "createdAt": "2020-08-06T14:10:11Z",
          "updatedAt": "2020-08-06T14:10:11Z"
        }
      ]
    },
    {
      "number": 100,
      "id": "MDU6SXNzdWU2NTMxMTYzNDc=",
      "title": "make it possible to log persistent congestion",
      "url": "https://github.com/quiclog/internet-drafts/issues/100",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "This would be *really* important for generating metrics.\r\n\r\nLogically, it's a congestion event, but I'm not sure if it would probably make sense to add this as a state to the `congestion_state_updated` event, because once you declare persistent congestion, your congestion controller enters either slow start or congestion avoidance (not sure which one, haven't implemented persistent congestion yet), which are already `congestion_state_updated` events.",
      "createdAt": "2020-07-08T08:54:37Z",
      "updatedAt": "2020-07-08T08:54:37Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 101,
      "id": "MDU6SXNzdWU2NTMxMjM3NjI=",
      "title": "Combine category and event type",
      "url": "https://github.com/quiclog/internet-drafts/issues/101",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Currently, we have two separate fields for category and event type.\r\n\r\nIn a default JSON form, you'd have:\r\n```\r\n{\r\n    category: \"transport\",\r\n    type: \"parameters_set\"\r\n}\r\n```\r\n\r\nHowever, the event type names (e.g., `parameters_set`, `state_updated`) aren't unique to the categories at the moment. \r\n\r\nThis can lead to some issues:\r\n- Processing qlog requires you to always check the category as well (cannot just look at the event_type)\r\n- Makes them difficult to encode in formats like protobuf (e.g., https://github.com/quiclog/pcap2qlog/blob/binary/src/converters/types/qlog.proto#L54) (and I assume things like serde as well @LPardue?) because you cannot easily de-serialize based just on the type-name.\r\n\r\nOne solution would be to combine both into one field:\r\n```\r\n{\r\n    type: \"transport:parameters_set\"\r\n}\r\n```\r\n\r\nThis should make it easier to de-serialize just based on the type field (and make it into an enum) and easier to do a `switch` when processing events. \r\n\r\nAnother option would be to make sure each event has a unique name, but that would come down the pre-pending the category (or something similar) anyway afaict (e.g., `transport_parameters_set`), which is basically the same thing. \r\n\r\nI don't see many use cases where you'd only want to log one category (and thus could leave that out and just log the event names to save some space). I also don't know if compression would be that much worse overall if we'd combine the two into one field. If we'd move to regular JSON as the default (#89), it would probably save space, as then we'd only have a single field per-object instead of 2 (as in the examples above).\r\n\r\nWhat do people think? CC @LPardue  @marten-seemann @mpiraux @triplewy @mikkelfj @nibanks @jlaine\r\n\r\n \r\n",
      "createdAt": "2020-07-08T09:04:43Z",
      "updatedAt": "2020-09-05T16:18:38Z",
      "closedAt": "2020-09-05T16:18:38Z",
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Makes sense to me. I support making this change.",
          "createdAt": "2020-07-08T09:17:31Z",
          "updatedAt": "2020-07-08T09:17:31Z"
        }
      ]
    },
    {
      "number": 102,
      "id": "MDU6SXNzdWU2NTMxMjQ1OTY=",
      "title": "remove redundant length fields",
      "url": "https://github.com/quiclog/internet-drafts/issues/102",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Once we require hex-encoding for byte-fields (which, as far as I can see from other peoples' qlogs, is already the de-facto standard), we don't need length fields any more. This applies to the `scil` and `dcil` in the `Header` (as well as the proposed token length in #94), the `NewTokenFrame`, `NewConnectionIDFrame` (and probably also the QPACK events, but I'm not an expert on those).",
      "createdAt": "2020-07-08T09:05:51Z",
      "updatedAt": "2020-07-08T09:22:53Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "I kind of disagree, as I think an important use case is where you'd not want to log the raw values but only the lengths. \r\n\r\nWe can argue whether that's the case for the CIDs of course, but for tokens and qpack data (and, obviously, payloads), I think people should have the option to not log the raw values. \r\n\r\nThis flows from privacy/security concerns, as well as storage efficiency for larger deployments. \r\n\r\nI do think we could add text stating something like \"if you log the raw values in hex, the length field is redundant, and tools SHOULD be able to derive length from hex\"?",
          "createdAt": "2020-07-08T09:13:08Z",
          "updatedAt": "2020-07-08T09:13:08Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Good point. I can see why you want to log tokens (since they create a correlation spanning two separate connections).\r\n\r\n> I do think we could add text stating something like \"if you log the raw values in hex, the length field is redundant, and tools SHOULD be able to derive length from hex\"?\r\n\r\nI'd make this an even stronger statement: \"\"if you log the raw values in hex, the length field is redundant and SHOULD (or MUST?) be omitted. Tools MUST be able to derive length from hex\".",
          "createdAt": "2020-07-08T09:20:09Z",
          "updatedAt": "2020-07-08T09:22:53Z"
        }
      ]
    },
    {
      "number": 104,
      "id": "MDU6SXNzdWU2NTMxMzAxNTY=",
      "title": "consider removing StreamFrame.raw",
      "url": "https://github.com/quiclog/internet-drafts/issues/104",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "I can't see how logging the exact contents of a STREAM frame could facilitate debugging / analysis of QUIC connections.\r\n\r\nArguably, a transport-level logging system should not log application data exchanged, as this might expose user data to places where it doesn't belong. While qlog can't prevent implementations from adding additional JSON fields to existing events, it should not encourage this behavior.",
      "createdAt": "2020-07-08T09:14:00Z",
      "updatedAt": "2020-07-08T09:27:44Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "While I agree from a security/privacy perspective that it shouldn't be encouraged, I disagree somewhat about the non-usefulness in debugging (esp. if you go beyond just H3 and look at things like DoQ or MASQUE). \r\n\r\nTwo direct alternatives I can see at the moment:\r\n- Remove from QUIC STREAM, but keep it on H3 DATA (though this means people can't easily get this without also logging H3/app-layer events, which e.g., things like quant might not like)\r\n- Add a generic `raw` and/or `raw_payload` field for all events to use (much like what we have for `trigger`). This makes it less obvious per-event (so less encouraged you might say) while still keeping a common name for interoperable tooling. \r\n\r\nOverall though, I'd vote to just keep it as-is. While it shouldn't be encouraged, it should imo be -possible- to express in the format for specific use cases. I'd even go one step further and add an explicit `raw` field to the datagram_* events (especially `datagram_dropped`) as I've seen several times in the slack that packets aren't being decoded correctly and having the raw values available could help there. \r\n\r\nIf we do keep `raw`, I'd propose to work out a scheme to allow logging only the first x-bytes though (e.g., `raw: 0xABABCD...(436)` indicating there were 436 more bytes not logged). ",
          "createdAt": "2020-07-08T09:23:36Z",
          "updatedAt": "2020-07-08T09:27:44Z"
        }
      ]
    },
    {
      "number": 105,
      "id": "MDU6SXNzdWU2NTUwMDg1NDk=",
      "title": "Log Retry and Connection in same Qlog file, use ODCID for indentification",
      "url": "https://github.com/quiclog/internet-drafts/issues/105",
      "state": "OPEN",
      "author": "huitema",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Many implementations identify the Qlog file using the \"Initial DCID\" used by the client. In the case of Retry, this results in two Qlog traces: one identified by the original DCID that contains the first initial packet of the client and the Retry packet that the server produced, and a second one identified by the Initial DCID used in the subsequent connection.\r\n\r\nMy proposal is to use a single file for the retry and for the next connection, identified by the ODCID. But I wonder whether that's going to break something.",
      "createdAt": "2020-07-10T20:06:03Z",
      "updatedAt": "2020-07-12T01:08:58Z",
      "closedAt": null,
      "comments": [
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "The current spec kind of gives up on the issue. It says \"Note that this can make it difficult to match logs from different vantage points with each other. For example, from the client side, it is easy to log connections with version negotiation or stateless retry in the same trace, while on the server they would most likely be logged in separate traces.\" My point is that it can be solved on the server, if the server keys logging events with the ODCID.",
          "createdAt": "2020-07-10T20:11:56Z",
          "updatedAt": "2020-07-10T20:11:56Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> My point is that it can be solved on the server, if the server keys logging events with the ODCID.\r\n\r\nThat's correct, and it's a trivial thing to do. The server needs to encode the ODCID in the token anyway, to be able to send it in the transport parameters. In fact, that's [how quic-go does it](https://github.com/lucas-clemente/quic-go/blob/e7fa420e264c4d319625c6dbf7cdf1c7668e7535/server.go#L452-L456).",
          "createdAt": "2020-07-11T01:36:30Z",
          "updatedAt": "2020-07-11T01:36:46Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "@huitema I'm not 100% sure what the issue is here.\r\n\r\nThe current qlog text explicitly mentions using the ODCID as filename for qlog files as the \"lowest common denominator\". Furthermore, qlog files can contain multiple individual traces, as the `traces' property is an array. So it's already perfectly possible to log one trace for the first connection and then a second trace for the retried one after that. As @marten-seemann said, there are people doing just that already. Maybe the problem is in the wording? That you interpret a `trace' as a single file, and I interpret it as a list of events, where 1 file can contain multiple traces? \r\n\r\nAnother approach would be to use the `group_id' field and not set that to an ODCID but assign that a sort of internal server number based on IP+port or something to uniquely identify a client across attempts. That would work even for \"version negotiation\", as there you don't have the luxury of having a stable ODCID. I don't want to mandate this type of thing in the text though, since it all implies the server needs to keep extra state around to match up connections, and some (most?) won't want to do that (in production). \r\n\r\nIn short: I'm not sure what you'd like me to change to the text at this time. \r\n\r\n\r\n\r\n",
          "createdAt": "2020-07-11T15:53:59Z",
          "updatedAt": "2020-07-11T15:53:59Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> So it's already perfectly possible to log one trace for the first connection and then a second trace for the retried one after that. As @marten-seemann said, there are people doing just that already.\r\n\r\nActually, that's not what I'm doing. I'm not logging the first Intitial received and the Retry sent at all on the server side (only on the client side), since there's no reasonable way of logging events that happen outside of a connection (see #106).",
          "createdAt": "2020-07-12T01:08:58Z",
          "updatedAt": "2020-07-12T01:08:58Z"
        }
      ]
    },
    {
      "number": 106,
      "id": "MDU6SXNzdWU2NTUwMTI5OTY=",
      "title": "Logging events outside of connections",
      "url": "https://github.com/quiclog/internet-drafts/issues/106",
      "state": "CLOSED",
      "author": "huitema",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Servers typically notice events happening outside of connections: packets for which the DCID is not recognized, malformed Initial packets, etc. When debugging and looking at both client traces and server traces, it might be useful to match a client packet with an out-of-connection event at the server. I wonder how to do that within the qlog framework.",
      "createdAt": "2020-07-10T20:15:49Z",
      "updatedAt": "2020-09-05T16:18:06Z",
      "closedAt": "2020-09-05T16:18:06Z",
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I encountered the same issue. I think the root of the problem is that qlog is not streamable. Otherwise, you could just have a `server.qlog` and append every event that's not matched to any connection.\r\n\r\nSure, you can always play some tricks to work around this issue, like exporting a server qlog every one hour, for example. Downside is, of course, that you wouldn't have access to the events that happened with the last hour.\r\n\r\nBroadening the scope of this issue, I'd say the root cause is that qlog is trying to make it possible to have multiple traces per file. Sure, it's possible, but only by giving up on streamability. I'm not sure if that's a feature worth having though, considering how much complexity it creates for the format itself (and for parsers that want to support it). I'd actually prefer to have a format that looks something like this:\r\n```\r\n<header>\r\n   config options, vantage, other trace-related metadata\r\n</header>\r\n<events>\r\n\r\n<events>\r\n```\r\n",
          "createdAt": "2020-07-11T01:34:15Z",
          "updatedAt": "2020-07-11T01:34:15Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "So, here again (like with #105, which I see as similar to this, maybe read my reply there first) I'm not sure what the issue is.\r\n\r\nIf an event cannot be matched to a connection, it's difficult to log it together with other events without keeping extra state and doing some work. That is usually relatively easy though: you can e.g., match IP+port of a malformed initial to other proper packets from the same source and aggregate them in the same trace based on that. Then you do need to add extra logic for migration etc. but it's certainly possible I'd say? \r\n\r\nAnother option is as @marten-seemann mentions to have a single big `server.qlog' that includes all the events that don't nicely match to a connection (this could also include things you reply to with a version negotiation for example). I think Lars does this for quant. To me, it depends on how you want to debug your setup and how you approach these types of errors. I'm not sure if there needs to be more text on this? \r\n\r\n-----------\r\n\r\nI am however unsure about why this `server.qlog` wouldn't be possible with the JSON-based setup... each qlog event is self-contained (it's a single array with some values and in draft-02 will be a single object) and can trivially be appended to a file by prepending it with a `,`. \r\n\r\nThe thing that makes a JSON file that was \"streamed\" a bit more awkward is that you'd need to close it with a `]}]}' to be \"valid JSON\". However, qvis for example includes a fallback to a streaming parser (http://oboejs.com) to deal with files that don't do this and it seems to work perfectly fine (was initially added for cases where the endpoint shuts down unexpectedly and doesn't have time to print those final closing characters). Furthermore, this case is trivially fixed by a post-processor that adds those characters if they're not found at the end of a file. \r\n\r\nAm I missing something crucial in this discussion on streaming? Several people have brought this up (e.g., @martinthomson) but I have always considered this a non-issue. If we were switch to e.g., a csv-format, you'd have to write custom parsers for that as well (which would likely be slower than optimized, built-in JSON parsers), while now at least you get a free JSON parser if your files are well-formed (which, from my experience, most are or can be made to be). \r\n\r\nI also don't agree this is due to having support for multiple traces in one file. You can have just a single trace and use `group_id' to split things out later. The only thing this adds is the extra `]}`. Even if we left that out and stayed with JSON, you'd still be stuck with the other `]}' to close the `<events>` list and total object. \r\n\r\nFinally, especially for @marten-seemann, these might be moot points for draft-02, as that should make it much easier to use protobufs or similar instead of JSON. Still, I'd love some feedback on my points above, to make sure I'm not missing a key point in this discussion. \r\n\r\n",
          "createdAt": "2020-07-11T16:12:46Z",
          "updatedAt": "2020-07-11T16:12:46Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> I am however unsure about why this server.qlog wouldn't be possible with the JSON-based setup... each qlog event is self-contained (it's a single array with some values \r\n\r\nI'm not sure if I understand your point. I thought this is not the case. You can have multiple traces per file, so you'd have a file where events are appended at different locations within that file.\r\nThis is why I suggested getting rid of this corner case by saying that `1 qlog file == 1 trace`. That would simplify things quite a bit.\r\n\r\n> Am I missing something crucial in this discussion on streaming? Several people have brought this up (e.g., @martinthomson) but I have always considered this a non-issue.\r\n\r\nI'd be curious to hear what other peoples' requirements are here.\r\nFor me, streaming means that you have a valid at any given moment. With the qlog format that's not possible since you'll always be missing some closing `]` and `}`.\r\nImagine you want to build a tool like the UNIX command line utility `tail` for qlog, which outputs events as soon as they come in. This would be hard to do, as you're dealing with invalid JSON most of the time. \r\nThings are further complicated by the fact that JSON doesn't define the order of the fields within an object (for example, it would be valid qlog if `common_fields` comes after the `events`, and iirc, I've seen at least one qlog implementation write files like that).\r\n\r\n>  If we were switch to e.g., a csv-format, you'd have to write custom parsers for that as well (which would likely be slower than optimized, built-in JSON parsers), while now at least you get a free JSON parser if your files are well-formed (which, from my experience, most are or can be made to be).\r\n\r\nWhen I think of slow encoders, I think of JSON...\r\nThat being said, I don't think JSON buys us a lot here. The fact that qlogs have `common_fields` and `event_fields`, and that events are not exported as objects but as arrays makes it quite cumbersome to write a qlog exporter / parser. As I understand, the `event_fields` are effectively an attempt to build in some compression into the format, an attempt rendered moot by the fact that JSON is an inherently space-inefficient file format. gzipping a tyical qlog gives you a file smaller than 10% of the original file size... so I can't help to think of this as a premature optimization.",
          "createdAt": "2020-07-12T02:32:10Z",
          "updatedAt": "2020-07-12T02:32:10Z"
        },
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "+1 on premature optimization. I tripped on the event_fields when writing a python parser for the qlogs, and I see that others have similar issues. If you want to reduce volume, it would be simpler to just have shorter names for the variables.",
          "createdAt": "2020-07-12T02:37:29Z",
          "updatedAt": "2020-07-12T02:37:29Z"
        },
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "+1 on format too. I would very much like to be able to just add an event by appending the data at the end of the log file. I can do that with my binary logs, and then call a converter to produce the qlog and add the required set of curly and square brackets at the end. But with an actual qlog file, I would have to roll back the end of the file, remove the end brackets, write the event, and then add the brackets again. Kind of, not funny.\r\n",
          "createdAt": "2020-07-12T03:09:04Z",
          "updatedAt": "2020-07-12T03:09:04Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "I believe this was resolved with the introduction of the streaming NDJSON option (discussed in #109) and other options I outlined in https://github.com/quiclog/internet-drafts/issues/106#issuecomment-657087933. If not, feel free to re-open. \r\n\r\n\r\n",
          "createdAt": "2020-09-05T16:18:06Z",
          "updatedAt": "2020-09-05T16:18:06Z"
        }
      ]
    },
    {
      "number": 107,
      "id": "MDU6SXNzdWU2NTU1NjA4MzM=",
      "title": "Do we need an acked_packet event?",
      "url": "https://github.com/quiclog/internet-drafts/issues/107",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "We have a `lost_packet` event, but no corresponding `acked_packet` event. That event would fire when a packet is acknowledged and removed from the map of sent packets.\r\n\r\nA parser that wants to track when a packet is (first) acknowledged would have to build a map of sent packets, and remove packets from that map when they are either acknowledged, declared lost or when the packet number space is lost.",
      "createdAt": "2020-07-13T03:49:47Z",
      "updatedAt": "2020-08-03T15:09:41Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "So this is similar to \"do we need a `connection_closed` event?\" in that there's a question of how many \"redundant\" ways we can have to log the same event. \r\n\r\nFor connection close and packet acked, the reasoning initially was that those could be fully parsed from the corresponding frames inside `packet_sent` and `packet_received` and thus didn't need an extra event. \r\n\r\nI can now see the reasoning for wanting a specific `connection_closed` event (e.g., logging an internal reason that you don't send over the wire), but I'm not sure what a separate `acked_packet` event would give us. For some use cases it would make it easier (if you're not parsing frames inside the `packet_*` events), but in most, I feel it would make things more complicated (do I need to log both? Do I skip logging of ACK frames if I do `acked_packet`? etc.). I do see that receipt of an ACK frame is potentially different from adding/removing to the internal packet datastructure (given duplicate acks), but not sure that should be exposed in a default way in qlog (we have many more of those type of `implied` actions that are expected to happen upon frame processing. Though there is some precedent of exposing that type of thing, e.g., in `data_moved`). In most cases, it was expected that tools could derive this type of action from the logged frames (e.g., like how I track retransmissions and gaps in streams in qvis: those are also not explicitly logged).",
          "createdAt": "2020-07-13T14:15:51Z",
          "updatedAt": "2020-07-13T14:18:11Z"
        },
        {
          "author": "nibanks",
          "authorAssociation": "NONE",
          "body": "Personally, I agree with @marten-seemann. MsQuic logs (to [ETW](https://docs.microsoft.com/en-us/windows/win32/etw/event-tracing-portal)) events or state changes for things. For qlog, we then have a converter. We don't log the contents of every packet or frame. For packets, we track when they're created, acknowledged, dropped, assumed lost, deemed spuriously lost, etc. IMO, tools that process the logs should not have to know how to process a logged ACK frame to then go calculate which in-flight packets have now been acknowledged. Another reason, is that our logging system doesn't assume full state was contained within the log file. The logs may have been started mid run or the fixed fixed log buffer rolled over. In those cases, you wouldn't necessarily have the created event for a packet and wouldn't know what is already in-flight.",
          "createdAt": "2020-07-13T14:30:59Z",
          "updatedAt": "2020-07-13T14:30:59Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Paraphrasing an argument I made in a Slack conversation:\r\n \r\nI can see @rmarx's argument that logging both the ACK frame and `packet_acked` is kind of redundant, given that a parser can build a copy of the packet map. However, I\u2019d rather log a `packet_acked` event instaed of the content of ACK frames. I care more about the state changes than about what redundant ACK ranges get serialized on the wire.",
          "createdAt": "2020-08-03T15:09:41Z",
          "updatedAt": "2020-08-03T15:09:41Z"
        }
      ]
    },
    {
      "number": 109,
      "id": "MDU6SXNzdWU2Njc2MjkwNjI=",
      "title": "Add streaming option",
      "url": "https://github.com/quiclog/internet-drafts/issues/109",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Current thinking is to allow http://ndjson.org and to log the qlog header information as a separate object on top. \r\nThis would remove the possibility to log multiple traces in the same qlog file if it uses the streaming option, so users would need to use `group_id` in those cases.\r\n\r\nWould also require the addition of a \"format\" field to the qlog metadata header (though we probably need that anyway if we want to keep support for other JSON-based optimizations like `event_fields`.\r\n\r\nWe first need to check how this plays with oboe.js in qvis though, to see if we need to add another parser in the mix. \r\n\r\nSee also #106 and #2 for more discussion.\r\n\r\nFor comparison:\r\n```\r\nDraft-01 would look like this (if, like me, JSON serializers would be too lazy to add quotes):\r\n{\r\n    version: draft-01,\r\n    traces: [\r\n        {\r\n             vantage_point: client,\r\n             common_fields: [ ... ],\r\n             event_fields: [ ... ], \r\n             events: [\r\n                 [ 55, transport, packet_sent, { ... } ],\r\n                 [ 66, transport, packet_received, { ... } ],\r\n                 ...\r\n             ]\r\n        }\r\n    ]\r\n}\r\n\r\nDraft-02 would be more like:\r\n{\r\n    version: draft-02,\r\n    format: \"ndjson\",\r\n    trace: {\r\n             vantage_point: client,\r\n             common_fields: [ ... ]\r\n     }\r\n}\r\n{ time: 55, name: \"transport:packet_sent\", data: { ... } }\r\n{ time: 66, name: \"transport:packet_received\", data: { ... } }\r\n...\r\n```\r\n\r\nCC @marten-seemann @huitema",
      "createdAt": "2020-07-29T07:49:26Z",
      "updatedAt": "2020-09-05T16:25:57Z",
      "closedAt": "2020-09-05T16:25:57Z",
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I like this idea. In the interest of keeping parsers simple, I would be supportive of completely replacing the current qlog format with this.\r\nAs I've said elsewhere, I don't think that the complexity that comes with putting multiple traces into the same file justifies the complexity.",
          "createdAt": "2020-07-29T08:00:15Z",
          "updatedAt": "2020-07-29T08:00:15Z"
        },
        {
          "author": "nibanks",
          "authorAssociation": "NONE",
          "body": "I agree with @marten-seemann. We should optimize for the primary case, which is definitely single machine/trace.",
          "createdAt": "2020-07-29T11:12:46Z",
          "updatedAt": "2020-07-29T11:12:46Z"
        },
        {
          "author": "huitema",
          "authorAssociation": "NONE",
          "body": "Streaming would definitely be useful for me too. Currently, I have to use a two stage approach to solve that: stream-supporting binary log, then convert to qlog once I am sure the end is reached. And yes, all usage is single machine/trace.",
          "createdAt": "2020-07-29T18:11:00Z",
          "updatedAt": "2020-07-29T18:11:00Z"
        },
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "support this, would be good to see a short example of the difference between new and old formats so I can start planning",
          "createdAt": "2020-07-29T23:33:32Z",
          "updatedAt": "2020-07-29T23:33:32Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "I've updated the issue with an example, PTAL. \r\n\r\nI'm highly skeptical that that type of thing would be de-serializable automatically with something like @LPardue's serde, but a girl can dream. ",
          "createdAt": "2020-07-30T08:58:37Z",
          "updatedAt": "2020-07-30T08:58:37Z"
        },
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "Thanks. FWIW, the current qlog format is effectively not deserializable using serde either, so nothing would be lost.\r\n\r\nedit: hmm, actually the new format might make it more straightforward to use a semi-automated deserializer",
          "createdAt": "2020-07-30T09:03:36Z",
          "updatedAt": "2020-07-30T09:05:52Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Initial support for \"newline delimited JSON\" qlog streaming is now in qvis per https://github.com/quiclog/qvis/commit/8504c0e4581b28c985f97eff701589b866bde4b9\r\n\r\nAn example draft-02 NDJSON file can be found in attachment (should load just fine in the live qvis atm).\r\nNow that I'm certain this can be done without major performance issues, I will move forward with this for qlog draft-02. \r\n\r\nIn the meantime, further feedback based on the example file is of course welcome. \r\n\r\n[newline_delimited.zip](https://github.com/quiclog/internet-drafts/files/5137476/newline_delimited.zip)\r\n\r\n",
          "createdAt": "2020-08-27T16:01:36Z",
          "updatedAt": "2020-08-27T16:01:36Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "This issues should have been resolved by the choice of NDJSON as streaming format as of https://github.com/quiclog/internet-drafts/commit/f5db7cdc8cd0cf37bfe5f1b0b4c54fc56ffc5f28",
          "createdAt": "2020-09-05T16:25:57Z",
          "updatedAt": "2020-09-05T16:25:57Z"
        }
      ]
    },
    {
      "number": 110,
      "id": "MDU6SXNzdWU2Nzk4MTM1MDI=",
      "title": "push_allowed may be misleading",
      "url": "https://github.com/quiclog/internet-drafts/issues/110",
      "state": "CLOSED",
      "author": "dtikhonov",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "The \"push_allowed\" data element is `parameters_set` event is odd in that other data elements log actual settings values while \"push_allowed\" is derived from the value of `MAX_PUSH_ID` frame.\r\n\r\nIn addition, for pushing to be allowed, a second condition is necessary:  There must be enough unidirectional stream credit.  For example, if only three unidirectional streams are allowed, it effectively means that pushing is not possible, as those will be taken by the control and the two QPACK streams.",
      "createdAt": "2020-08-16T19:41:52Z",
      "updatedAt": "2020-09-07T10:21:24Z",
      "closedAt": "2020-09-07T10:21:24Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "I agree this is unclear.\r\nThe thinking was that you'd be able to see the MAX_PUSH_ID frame when that's logged as part of `frame_created` or `frame_parsed` and the `push_allowed` would be a more high-level indicator if applications don't log the frames individually (e.g., to save space).\r\n\r\nGiven this, would you advocate:\r\n1. removing `push_allowed` altogether\r\n2. properly defining `push_allowed` as meaning both MAX_PUSH_ID > 0 AND unidirectional credit at the same time\r\n3. replacing `push_allowed` with a `max_push_id` field\r\n",
          "createdAt": "2020-08-19T13:12:50Z",
          "updatedAt": "2020-08-19T13:12:50Z"
        },
        {
          "author": "dtikhonov",
          "authorAssociation": "NONE",
          "body": "Well, `MAX_PUSH_ID` is already logged, so we don't need (3).  (2) may be onerous to log because now the implementation needs to add checks in at least two places and log `push_allowed`...  Let the tool do it!\r\n\r\nThus, I vote for (1).",
          "createdAt": "2020-08-20T12:49:21Z",
          "updatedAt": "2020-08-20T12:49:21Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Was removed and explanation text added. ",
          "createdAt": "2020-09-07T10:21:24Z",
          "updatedAt": "2020-09-07T10:21:24Z"
        }
      ]
    },
    {
      "number": 111,
      "id": "MDU6SXNzdWU2Nzk4MTU5OTY=",
      "title": "data_moved describes three layers, but only two are specified",
      "url": "https://github.com/quiclog/internet-drafts/issues/111",
      "state": "OPEN",
      "author": "dtikhonov",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "> Used to indicate when data moves between the HTTP/3 and the transport layer (e.g., passing from H3 to QUIC stream buffers and vice versa) or between HTTP/3 and the actual user application on top (e.g., a browser engine). This helps make clear the flow of data, how long data remains in various buffers and the overheads introduced by HTTP/3's framing layer.\r\n\r\nThe paragraph above describes three layers: transport, HTTP/3, and user application.\r\n\r\nThe definition allows for only two layers:\r\n\r\n```\r\n    from?:\"application\"|\"transport\",\r\n    to?:\"application\"|\"transport\",\r\n```\r\n\r\nIn addition, the definition of \"application\" is ambiguous: it could be the application protocol (HTTP/3) or the user application.",
      "createdAt": "2020-08-16T19:58:10Z",
      "updatedAt": "2020-09-07T13:40:36Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "So, the current thinking is that this is always an HTTP level event, so there is no need to specify the \"third\" layer, as it is always HTTP. \r\n\"application\" in this case will always be the user application, not the application protocol (which, I agree, is not entirely clear or obvious).\r\n\r\nGiven this explanation, would you push for:\r\n1. making this a more generic event by adding the \"third layer\" as well (not sure which category we would use for this event then though)\r\n2. re-naming \"application\" to something else here to separate it from HTTP3/application protocol more clearly\r\n3. keeping it as is but adding text to explain it more clearly\r\n",
          "createdAt": "2020-08-19T13:10:27Z",
          "updatedAt": "2020-08-19T13:10:27Z"
        },
        {
          "author": "dtikhonov",
          "authorAssociation": "NONE",
          "body": "I'd vote for (1), with the three layers being:\r\n1. transport (or QUIC);\r\n2. HTTP/3; and\r\n3. application.\r\n\r\nNote that the data may skip the HTTP/3 layer ([lsquic](https://github.com/litespeedtech/lsquic), for example, tries hard to avoid intermediate buffering) altogether, so the event should be able to describe that as well.",
          "createdAt": "2020-08-20T12:56:08Z",
          "updatedAt": "2020-08-20T12:56:08Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Could you give an example of how data can skip the H3 layer in practice? I can understand this if you're for example using raw DATAGRAM frames in QUIC that don't use the HTTP/3 DATAGRAM stuff, but the moment you're carrying H3, you still need to strip out the frame headers in the H3 layer, no? ",
          "createdAt": "2020-08-20T13:07:48Z",
          "updatedAt": "2020-08-20T13:07:48Z"
        },
        {
          "author": "dtikhonov",
          "authorAssociation": "NONE",
          "body": "Here is how reading from an HTTP/3 stream works in lsquic (in the best-case scenario):\r\n\r\n1. The packet is decrypted and the decrypted payload is stored in a new memory object.  (This is data copy number 1.)\r\n2. The packet is parsed.  This creates _STREAM_ frame marker objects.\r\n3. If, as a result of a new marker, reading from a stream becomes possible, the user is notified via an \"on stream write\" event.  Note that *in the case of HTTP/3 stream, special magic happens internally*: the stream is fast-forwarded through the framing.  _DATA_ frames are processed and discarded.  You can peek at [how it's done in the `read_data_frames()` function](https://github.com/litespeedtech/lsquic/blob/v2.19.5/src/liblsquic/lsquic_stream.c#L1360L1363).\r\n4. The user then reads data from stream using one of two ways.\r\n   - **copy read**.  In this case, `lsquic_stream_read()` or `lsquic_stream_readv()` is called and data is copied into user-supplied buffer(s).  (This is data copy number 2.)\r\n   - **zero copy**.  User can call `lsquic_stream_readf()` with a callback.  This callback will then be given a pointer to the data from the  first copy (in Step 1).  See [line 1367](https://github.com/litespeedtech/lsquic/blob/v2.19.5/src/liblsquic/lsquic_stream.c#L1367) -- this is where this callback is called.  Here, the user can copy the data or process it directly.\r\n\r\nNote that there was no copy from transport layer into any intermediate layer.  So, no, stripping out frame headers does not mean that data must be copied.",
          "createdAt": "2020-08-20T14:20:34Z",
          "updatedAt": "2020-08-20T14:20:34Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Is tied to #65 ",
          "createdAt": "2020-09-07T13:40:36Z",
          "updatedAt": "2020-09-07T13:40:36Z"
        }
      ]
    },
    {
      "number": 112,
      "id": "MDU6SXNzdWU2Nzk4MTczNTI=",
      "title": "QPACK encoder and decoder each has its own state",
      "url": "https://github.com/quiclog/internet-drafts/issues/112",
      "state": "CLOSED",
      "author": "dtikhonov",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Each peer has two QPACK states: encoder and decoder.  `qpack`'s `state_updated` and `dynamic_table_updated` events do not make such differentiation, making the specification incomplete.  For example:\r\n\r\n> This event is emitted when one or more entries are added or evicted from QPACK's dynamic table.\r\n\r\nWhich dynamic table is meant: encoder or decoder?",
      "createdAt": "2020-08-16T20:06:37Z",
      "updatedAt": "2020-09-07T10:17:26Z",
      "closedAt": "2020-09-07T10:17:26Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Closed by adding an \"owner\" field to `dynamic_state_updated`.\r\n\r\nOther qpack events either already had this field or are split out into created/parsed or encoded/decoded events, which makes this implicit. ",
          "createdAt": "2020-09-07T10:17:26Z",
          "updatedAt": "2020-09-07T10:17:26Z"
        }
      ]
    },
    {
      "number": 113,
      "id": "MDU6SXNzdWU2Nzk4MTc2MzE=",
      "title": "QPACK: dynamic table entries are \"inserted,\" not \"added\"",
      "url": "https://github.com/quiclog/internet-drafts/issues/113",
      "state": "OPEN",
      "author": "dtikhonov",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "The draft should use the QPACK terminology.",
      "createdAt": "2020-08-16T20:08:19Z",
      "updatedAt": "2020-09-07T10:11:24Z",
      "closedAt": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "So, I'm not entirely sure what to do here... I based myself mainly on this section, which does use \"added\"/\"evicted\": \r\nhttps://tools.ietf.org/html/draft-ietf-quic-qpack-16#section-3.2.2\r\n\r\nHowever, I agree that in other places, \"inserted\" is used for the same/similar operations. I have changed it to \"inserted\" in qlog, as I trust @dtikhonov's good judgement, but wonder if this should be made consistent in the QPACK text? \r\n\r\nSimilarly, several diagrams refer to the \"dropping point\" or \"dropped count\", which I -assume- is the same as \"evicted\" or \"eviction index\" etc. The \"dropping point\" is not mentioned in the running text anywhere in any case. Maybe that should also be made more consistent?\r\n\r\nCC'ing @afrind on this. If needed, I can make issues in the quicwg repo for this. \r\n",
          "createdAt": "2020-09-07T10:11:23Z",
          "updatedAt": "2020-09-07T10:11:23Z"
        }
      ]
    },
    {
      "number": 114,
      "id": "MDU6SXNzdWU2Nzk4MTgxMjA=",
      "title": "qpack.instruction_sent -- which \"sent\" time is meant?",
      "url": "https://github.com/quiclog/internet-drafts/issues/114",
      "state": "CLOSED",
      "author": "dtikhonov",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "> This event is emitted when a QPACK instruction (both decoder and encoder) is sent.\r\n\r\nA QPACK instruction is:\r\n- written to the encoder (or decoder) stream, which is then\r\n- packetized, and\r\n- the packet (or packets!) is sent.\r\n\r\nWhich of the three events above is meant is not clear.",
      "createdAt": "2020-08-16T20:11:32Z",
      "updatedAt": "2020-09-07T10:04:05Z",
      "closedAt": "2020-09-07T10:04:05Z",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Resolved by renaming `instruction_sent/received` to `instruction_created/parsed`, as an analogy to the HTTP/3 `frame_created/parsed` events. \r\n\r\nThanks for noticing this @dtikhonov!",
          "createdAt": "2020-09-07T10:04:05Z",
          "updatedAt": "2020-09-07T10:04:05Z"
        }
      ]
    },
    {
      "number": 115,
      "id": "MDU6SXNzdWU2ODE4ODYxODI=",
      "title": "QPACK: check data types",
      "url": "https://github.com/quiclog/internet-drafts/issues/115",
      "state": "CLOSED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Commit 128f3840cf951e8032c25896683b369cb92d13e6 adds proper data type definitions (e.g., uint64 instead of just \"number\", bytes instead of \"hex string\") to the qlog event fields.\r\n\r\nThese were a bit difficult to figure out for QPACK events (with the whole \"prefixed integer\" stuff), so I've put everything at uint64 for now. While this is the \"safest\" option, elsewhere in the document I've taken the approach of choosing the smallest \"likely\" integer size for a given value. One example is the datagram_size: it's unlikely we'll see datagrams larger than uint32 in practice (you could even argue uint16?), so this field is not a uint64. Similarly, the length of a QUIC STREAM frame is also just uint32, not 64. This approach should help make binary qlog derivations more optimized. \r\n\r\nAs such, I was hoping someone with more QPACK experience could help indicate the \"minimal\" data type size for the different fields. \r\n\r\nA secondary aspect is the values logged for \"name\" and \"value\" (e.g., in InsertWithoutNameReferenceInstruction). I'm not sure if these should be readable strings (e.g., \"Content-Encoding\" or their hex/byte equivalents (e.g., \"abcde1234\"). For debugging it makes more sense if they are the readable strings, though I'm not sure it's logical to treat them that way inside a QPACK implementation. Alternative is the make the datatype for those `string | bytes` (one of both) and let the logger/tools figure it out. \r\n\r\nSo, TLDR:\r\n1. Which QPACK number-esque fields can do with less than 64 bits in practice?\r\n2. Which data type should name and value fields have in qlog? \r\n\r\nI was hoping for some input on this from maybe @dtikhonov, @lpardue, @afrind. Thanks in advance!\r\n\r\n\r\n\r\n",
      "createdAt": "2020-08-19T14:27:50Z",
      "updatedAt": "2020-09-07T10:04:54Z",
      "closedAt": "2020-09-07T10:04:53Z",
      "comments": [
        {
          "author": "LPardue",
          "authorAssociation": "NONE",
          "body": "I don't have answers sorry, only a question. Have you considered making the name string and value string optional?",
          "createdAt": "2020-08-19T16:28:05Z",
          "updatedAt": "2020-08-19T16:28:05Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "@LPardue yes, and most already are, but it seems I missed a few. Will be fixed, thanks for reporting. ",
          "createdAt": "2020-08-19T16:34:08Z",
          "updatedAt": "2020-08-19T16:34:08Z"
        },
        {
          "author": "dtikhonov",
          "authorAssociation": "NONE",
          "body": "> Similarly, the length of a QUIC STREAM frame is also just uint32, not 64.\r\n\r\nI looked at the latest version of the draft and stream IDs in the \"qpack\" section are all uint64...",
          "createdAt": "2020-08-20T13:09:47Z",
          "updatedAt": "2020-08-20T13:09:47Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "> > Similarly, the length of a QUIC STREAM frame is also just uint32, not 64.\r\n> \r\n> I looked at the latest version of the draft and stream IDs in the \"qpack\" section are all uint64...\r\n\r\nyes, because the IDs can conceivably go higher than 32. However, the raw_length of the STREAM frame (https://quiclog.github.io/internet-drafts/draft-marx-qlog-event-definitions-quic-h3.html#name-streamframe) can never span more than just the single packet, and I assume individual packets won't be larger than 32 bits in practice. (the \"length\" field there should then probably also be uint32 instead of 64, though the offset should remain at 64). \r\n",
          "createdAt": "2020-08-20T13:18:48Z",
          "updatedAt": "2020-08-20T13:18:48Z"
        },
        {
          "author": "dtikhonov",
          "authorAssociation": "NONE",
          "body": "Oh, I see where I went wrong! :-)\r\n\r\nThe updated \"qpack\" types in the draft look OK.",
          "createdAt": "2020-08-20T13:59:24Z",
          "updatedAt": "2020-08-20T13:59:24Z"
        },
        {
          "author": "afrind",
          "authorAssociation": "NONE",
          "body": "Seems like you could probably reduce the table capacity and table size to 32 bits, as I have a hard time seeing someone reserving up to 4GB for this purpose.  That said, it is possible in the protocol, so if you want to be able to handle even the crazies I guess you have to stick to 64.  ",
          "createdAt": "2020-08-20T15:47:06Z",
          "updatedAt": "2020-08-20T15:47:06Z"
        },
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Thanks all for the review.\r\n\r\nName and value (and their lengths) are now always optional and some uint64's have become uint32's (we do not want to encourage the crazies ;)) ",
          "createdAt": "2020-09-07T10:04:53Z",
          "updatedAt": "2020-09-07T10:04:53Z"
        }
      ]
    },
    {
      "number": 116,
      "id": "MDU6SXNzdWU2OTQ4NjY2MDE=",
      "title": "Add certificate verification events",
      "url": "https://github.com/quiclog/internet-drafts/issues/116",
      "state": "OPEN",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "In real deployments, getting some feedback on certificate validation events is useful (e.g., these are included in NetLog).\r\n\r\nI wonder if these can be done as triggers... but on what event would those triggers be logged then? Maybe on a connection close if validation fails? What if it succeeds? \r\n\r\nAs I've seen some discussion about these things being async and sometimes depend on remote fetches (e.g., for OCSP), it would probably be useful to have separate events though. \r\n\r\nI need to get a better overview of the types of events though to decide on how to log this. Probably add something simple for draft-02 and extend in -03. ",
      "createdAt": "2020-09-07T08:37:28Z",
      "updatedAt": "2020-09-07T08:37:28Z",
      "closedAt": null,
      "comments": []
    }
  ],
  "pulls": [
    {
      "number": 12,
      "id": "MDExOlB1bGxSZXF1ZXN0MzAwMDI4NTY1",
      "title": "Fixed typo in \"version_negotiation\" enum member",
      "url": "https://github.com/quiclog/internet-drafts/pull/12",
      "state": "MERGED",
      "author": "jlaine",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2019-07-22T20:34:38Z",
      "updatedAt": "2019-07-22T21:12:06Z",
      "closedAt": "2019-07-22T21:12:06Z",
      "mergedAt": "2019-07-22T21:12:06Z",
      "mergedBy": "rmarx",
      "comments": [],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "52267e9675f5fd06f5d507a1792aa601ea0bdb81",
      "headRepository": "jlaine/internet-drafts",
      "headRefName": "negotiation-typo",
      "headRefOid": "22e20c5e072d32d98f4e432effec3e2b49d6c869",
      "mergeCommit": {
        "oid": "9bc53886d90472c68000652c2a1c83a9a6c338a5"
      }
    },
    {
      "number": 18,
      "id": "MDExOlB1bGxSZXF1ZXN0MzEwMzM1NzE2",
      "title": "Rename \"type\" property to \"packet_type\" in packet events",
      "url": "https://github.com/quiclog/internet-drafts/pull/18",
      "state": "MERGED",
      "author": "jlaine",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2019-08-23T10:53:01Z",
      "updatedAt": "2019-08-23T12:30:44Z",
      "closedAt": "2019-08-23T12:30:44Z",
      "mergedAt": "2019-08-23T12:30:44Z",
      "mergedBy": "rmarx",
      "comments": [],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "3b12117f096ab7fa7927516a39a2fd17b392d8d6",
      "headRepository": "jlaine/internet-drafts",
      "headRefName": "packet_type",
      "headRefOid": "f629b74ec44e95158b4594873c338435d07fc108",
      "mergeCommit": {
        "oid": "886a41e3afc7634500ed382350508a1fe5c224b7"
      }
    },
    {
      "number": 38,
      "id": "MDExOlB1bGxSZXF1ZXN0MzY0NDMzMDYy",
      "title": "add the HANDSHAKE_DONE frame",
      "url": "https://github.com/quiclog/internet-drafts/pull/38",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Fixes #37.",
      "createdAt": "2020-01-18T14:47:55Z",
      "updatedAt": "2020-01-18T15:31:42Z",
      "closedAt": "2020-01-18T15:31:42Z",
      "mergedAt": "2020-01-18T15:31:42Z",
      "mergedBy": "rmarx",
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Done.",
          "createdAt": "2020-01-18T15:29:31Z",
          "updatedAt": "2020-01-18T15:29:31Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0OTQ3OTg4",
          "commit": {
            "abbreviatedOid": "5ec9531"
          },
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "state": "CHANGES_REQUESTED",
          "body": "Could you also add HandshakeDoneFrame to the QuicFrame type in appendix A.4 (https://tools.ietf.org/html/draft-marx-qlog-event-definitions-quic-h3-01#appendix-A.4)? ",
          "createdAt": "2020-01-18T15:18:51Z",
          "updatedAt": "2020-01-18T15:18:51Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0OTQ4NTEy",
          "commit": {
            "abbreviatedOid": "189dd7b"
          },
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2020-01-18T15:31:31Z",
          "updatedAt": "2020-01-18T15:31:31Z",
          "comments": []
        }
      ],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "e5c946c65db20624d4f06d5c9102d6154fc8c847",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "handshake-done",
      "headRefOid": "189dd7bbf606d0b8400d91260e704c9c328b6aa3",
      "mergeCommit": {
        "oid": "b50635531bbbbcd059d9670b0b97f55069defb4f"
      }
    },
    {
      "number": 41,
      "id": "MDExOlB1bGxSZXF1ZXN0MzY0NDk4MTI4",
      "title": "move packet_size to events, move packet_type to PacketHeader",
      "url": "https://github.com/quiclog/internet-drafts/pull/41",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Fixes #40.",
      "createdAt": "2020-01-19T05:31:38Z",
      "updatedAt": "2020-01-19T05:31:44Z",
      "closedAt": null,
      "mergedAt": null,
      "mergedBy": null,
      "comments": [],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "b50635531bbbbcd059d9670b0b97f55069defb4f",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "udp-vs-quic-header",
      "headRefOid": "ce9b1690e05a31048d39ec171721febb1922d618",
      "mergeCommit": null
    },
    {
      "number": 61,
      "id": "MDExOlB1bGxSZXF1ZXN0Mzg1MjMwMjI3",
      "title": "rename header_decrypt_error to header_parse_error",
      "url": "https://github.com/quiclog/internet-drafts/pull/61",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Fixes #60.",
      "createdAt": "2020-03-08T05:53:58Z",
      "updatedAt": "2020-03-08T05:53:58Z",
      "closedAt": null,
      "mergedAt": null,
      "mergedBy": null,
      "comments": [],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "b50635531bbbbcd059d9670b0b97f55069defb4f",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "header-decrypt-error",
      "headRefOid": "359513a3bb236747fcbd71ee1f506e12199293ad",
      "mergeCommit": null
    },
    {
      "number": 63,
      "id": "MDExOlB1bGxSZXF1ZXN0Mzg3NjMyNDIw",
      "title": "rename idle_timeout to max_idle_timeout",
      "url": "https://github.com/quiclog/internet-drafts/pull/63",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "See https://github.com/quicwg/base-drafts/pull/3099.",
      "createdAt": "2020-03-13T07:30:34Z",
      "updatedAt": "2020-03-13T08:13:43Z",
      "closedAt": "2020-03-13T08:13:43Z",
      "mergedAt": "2020-03-13T08:13:43Z",
      "mergedBy": "rmarx",
      "comments": [],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "b50635531bbbbcd059d9670b0b97f55069defb4f",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "max-idle-timeout",
      "headRefOid": "a007af093fb3f7f77c82d415a6f366f2dc9b2098",
      "mergeCommit": {
        "oid": "89d2a71667fbdf7888c9139583c84bc0a9690e0f"
      }
    },
    {
      "number": 67,
      "id": "MDExOlB1bGxSZXF1ZXN0MzkxODExNzMz",
      "title": "rename max_packet_size to max_udp_payload_size",
      "url": "https://github.com/quiclog/internet-drafts/pull/67",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "See https://github.com/quicwg/base-drafts/pull/3473.",
      "createdAt": "2020-03-21T04:03:25Z",
      "updatedAt": "2020-03-21T12:32:22Z",
      "closedAt": "2020-03-21T12:32:21Z",
      "mergedAt": "2020-03-21T12:32:21Z",
      "mergedBy": "rmarx",
      "comments": [],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "89d2a71667fbdf7888c9139583c84bc0a9690e0f",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "max-udp-payload-size",
      "headRefOid": "8c8e39e1dafd3ced236d384d20539a7a2dec25d2",
      "mergeCommit": {
        "oid": "eeebaa4826d2596f552dedac95b4d01998d6e735"
      }
    },
    {
      "number": 68,
      "id": "MDExOlB1bGxSZXF1ZXN0MzkzODIxMTU2",
      "title": "Add stateless reset support",
      "url": "https://github.com/quiclog/internet-drafts/pull/68",
      "state": "MERGED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Partially fixes #64.\r\n\r\nMain change is adding `stateless_reset` as a PacketType.\r\n\r\nThe `stateless_reset_token` is added as a field to packet_sent and packet_received.\r\nAny of the \"unpredictable bits\" can be logged as `raw_encrypted`\r\n\r\nAdded a trigger to `connection_state_update` to indicate if a connection goes into draining/closing because a valid stateless_reset was received.\r\n\r\nBackwards compatibility break: renamed `reset_token` to `stateless_reset_token` on the NewConnectionID frame definition (for consistency). \r\n\r\nBeen thinking about a way to signal an \"invalid\" stateless reset, but figured there is no way to distinguish it from an otherwise garbage datagram, so decided it was impossible. Not 100% sure about the \"looping case\" where you continually lower the packet size. Maybe that's solved by adding a generic \"packet_size\" trigger value to `packet_dropped` though?",
      "createdAt": "2020-03-25T20:50:37Z",
      "updatedAt": "2020-03-28T10:24:21Z",
      "closedAt": "2020-03-28T10:24:21Z",
      "mergedAt": "2020-03-28T10:24:20Z",
      "mergedBy": "rmarx",
      "comments": [
        {
          "author": "kazu-yamamoto",
          "authorAssociation": "NONE",
          "body": "LGTM!",
          "createdAt": "2020-03-27T02:54:01Z",
          "updatedAt": "2020-03-27T02:54:01Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgyMDE3NjE5",
          "commit": {
            "abbreviatedOid": "6ff25a4"
          },
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2020-03-26T13:40:51Z",
          "updatedAt": "2020-03-26T13:40:51Z",
          "comments": []
        }
      ],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "eeebaa4826d2596f552dedac95b4d01998d6e735",
      "headRepository": "quiclog/internet-drafts",
      "headRefName": "stateless_reset",
      "headRefOid": "6ff25a4b44ee3a5f146381c737a5de3c64e14c78",
      "mergeCommit": {
        "oid": "edf676c342bf73c168a27a4af6b6f45af58518ba"
      }
    },
    {
      "number": 70,
      "id": "MDExOlB1bGxSZXF1ZXN0Mzk1MDU5NTUz",
      "title": "update transport errors",
      "url": "https://github.com/quiclog/internet-drafts/pull/70",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2020-03-28T07:26:37Z",
      "updatedAt": "2020-03-28T11:00:33Z",
      "closedAt": "2020-03-28T11:00:33Z",
      "mergedAt": "2020-03-28T11:00:33Z",
      "mergedBy": "rmarx",
      "comments": [],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "eeebaa4826d2596f552dedac95b4d01998d6e735",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "update-transport-errors",
      "headRefOid": "26a19a8f72339cac08eb277f45285fdd244981eb",
      "mergeCommit": {
        "oid": "2a4dfd91598685b31daade04cf99db4bdffd2109"
      }
    },
    {
      "number": 71,
      "id": "MDExOlB1bGxSZXF1ZXN0Mzk1MDY0ODQy",
      "title": "add a packet number space field to loss_timer_set and loss_timer_expired",
      "url": "https://github.com/quiclog/internet-drafts/pull/71",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #69.",
      "createdAt": "2020-03-28T08:27:33Z",
      "updatedAt": "2020-03-28T10:29:32Z",
      "closedAt": "2020-03-28T10:29:31Z",
      "mergedAt": "2020-03-28T10:29:31Z",
      "mergedBy": "rmarx",
      "comments": [],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "eeebaa4826d2596f552dedac95b4d01998d6e735",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "loss-timer-events-pn-space",
      "headRefOid": "da1743e05116cf2f0f0390ea24c54bf66b2df0c0",
      "mergeCommit": {
        "oid": "d4a976e57f271d237d1818e5e56c62b6b60fd38b"
      }
    },
    {
      "number": 73,
      "id": "MDExOlB1bGxSZXF1ZXN0Mzk1MDc5MjQ3",
      "title": "Merge loss_timer events",
      "url": "https://github.com/quiclog/internet-drafts/pull/73",
      "state": "MERGED",
      "author": "rmarx",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "Fixes #69.\r\n\r\nNot 100% sure this is enough for all the latest draft-27(+) changes though, added a TODO in the draft for that purpose. \r\n\r\nAlso added a packet_number_space field to metrics_updated to be flexible for implementations that don't want to implement all types of events. It's not a great fit at that location, but it seemed the most synergistic without having a completely new event type.\r\n\r\nPTAL @marten-seemann ",
      "createdAt": "2020-03-28T10:55:36Z",
      "updatedAt": "2020-03-29T11:23:59Z",
      "closedAt": "2020-03-29T11:23:59Z",
      "mergedAt": "2020-03-29T11:23:59Z",
      "mergedBy": "rmarx",
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgzMzg4MzI3",
          "commit": {
            "abbreviatedOid": "1833887"
          },
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2020-03-29T08:59:00Z",
          "updatedAt": "2020-03-29T09:01:06Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "There's no single point in time where the packet number space changes for loss recovery purposes. On the contrary, both client and server will have packets from different packet number spaces in flight during every QUIC handshake.\r\n\r\nTherefore I'd suggest to remove this field.",
              "createdAt": "2020-03-29T08:59:00Z",
              "updatedAt": "2020-03-29T11:10:15Z"
            },
            {
              "originalPosition": 64,
              "body": "Why do we need to export a trigger, if this is the only possible reason for canceling the timer anyway?",
              "createdAt": "2020-03-29T09:00:48Z",
              "updatedAt": "2020-03-29T11:10:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgzMzk5MDAx",
          "commit": {
            "abbreviatedOid": "11720e0"
          },
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2020-03-29T11:13:37Z",
          "updatedAt": "2020-03-29T11:13:37Z",
          "comments": []
        }
      ],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "d4a976e57f271d237d1818e5e56c62b6b60fd38b",
      "headRepository": "quiclog/internet-drafts",
      "headRefName": "loss_timer",
      "headRefOid": "11720e0eb6c63f44526e7546c705aa419a7bf6ef",
      "mergeCommit": {
        "oid": "a68c214abaa655138ad5935bf3a1021091bfc5a5"
      }
    },
    {
      "number": 77,
      "id": "MDExOlB1bGxSZXF1ZXN0NDAxNzczNjIw",
      "title": "add more packet_dropped triggers",
      "url": "https://github.com/quiclog/internet-drafts/pull/77",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2020-04-10T06:13:27Z",
      "updatedAt": "2020-04-12T12:10:18Z",
      "closedAt": "2020-04-12T12:10:18Z",
      "mergedAt": "2020-04-12T12:10:18Z",
      "mergedBy": "rmarx",
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "Per discussion on slack, `unexpected_version_negotiation` and `unexpected_retry` can be merged into `unexpected_packet`, given that we have a `packet_type` field for the disambiguation. ",
          "createdAt": "2020-04-12T11:41:05Z",
          "updatedAt": "2020-04-12T11:41:05Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@rmarx Done. Updated the PR.",
          "createdAt": "2020-04-12T11:56:16Z",
          "updatedAt": "2020-04-12T11:56:16Z"
        }
      ],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "a68c214abaa655138ad5935bf3a1021091bfc5a5",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "more-drop-triggers",
      "headRefOid": "af1614c49eace48bf29b911790a9db0095b458ca",
      "mergeCommit": {
        "oid": "9af27705fd0eed8f38465ebf34afeedc9e93d857"
      }
    },
    {
      "number": 81,
      "id": "MDExOlB1bGxSZXF1ZXN0NDAzMDgyODg0",
      "title": "add a supported_versions array to packet_sent and packet_received",
      "url": "https://github.com/quiclog/internet-drafts/pull/81",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Partial fix for #75.",
      "createdAt": "2020-04-14T09:51:56Z",
      "updatedAt": "2020-04-14T14:40:18Z",
      "closedAt": "2020-04-14T14:40:17Z",
      "mergedAt": "2020-04-14T14:40:17Z",
      "mergedBy": "rmarx",
      "comments": [],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "9af27705fd0eed8f38465ebf34afeedc9e93d857",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "log-supported-versions",
      "headRefOid": "d02e59eeb7ca2df7787b9818b584629d01608cd9",
      "mergeCommit": {
        "oid": "b0880b8a5ff7a9e856e321ff12465fd4be15cf5f"
      }
    },
    {
      "number": 82,
      "id": "MDExOlB1bGxSZXF1ZXN0NDAzMTEzNzAw",
      "title": "add a version_negotiation trigger for the closed connection_state_updated event",
      "url": "https://github.com/quiclog/internet-drafts/pull/82",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2020-04-14T10:57:24Z",
      "updatedAt": "2020-04-14T14:39:52Z",
      "closedAt": "2020-04-14T14:39:52Z",
      "mergedAt": "2020-04-14T14:39:52Z",
      "mergedBy": "rmarx",
      "comments": [],
      "reviews": [],
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "9af27705fd0eed8f38465ebf34afeedc9e93d857",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "version-mismatch",
      "headRefOid": "573c8386c29bed76851f66c40158eb4165fb35a3",
      "mergeCommit": {
        "oid": "05c45b5d9e2195535a610c28f9c4a27d88444c5d"
      }
    },
    {
      "number": 83,
      "id": "MDExOlB1bGxSZXF1ZXN0NDAzNTMwMzQ5",
      "title": "add a timeout trigger for the closed connection_state_updated event",
      "url": "https://github.com/quiclog/internet-drafts/pull/83",
      "state": "OPEN",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2020-04-15T04:12:58Z",
      "updatedAt": "2020-04-15T07:43:32Z",
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "b0880b8a5ff7a9e856e321ff12465fd4be15cf5f",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "connection-state-timeout",
      "headRefOid": "08e4ca6944c39f416b951adba3dd03d30af326a7",
      "closedAt": null,
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I'm unsure about this PR. In my implementation, I have an idle timeout as well as a handshake timeout (the handshake timeout way shorter than the idle timeout).\r\nWould it make sense to add a value for both? Or a separate field for trigger details?",
          "createdAt": "2020-04-15T07:43:32Z",
          "updatedAt": "2020-04-15T07:43:32Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 84,
      "id": "MDExOlB1bGxSZXF1ZXN0NDAzNjUzMDA0",
      "title": "add a packet_size field to the packet_buffered event",
      "url": "https://github.com/quiclog/internet-drafts/pull/84",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Fixes #76.",
      "createdAt": "2020-04-15T09:44:14Z",
      "updatedAt": "2020-07-22T07:48:09Z",
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "b0880b8a5ff7a9e856e321ff12465fd4be15cf5f",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "packet-buffered-packet-size",
      "headRefOid": "e17dbba7a86e0c28814ed03b1d9833b2c0c2750a",
      "closedAt": "2020-07-22T07:48:08Z",
      "mergedAt": "2020-07-22T07:48:08Z",
      "mergedBy": "rmarx",
      "mergeCommit": {
        "oid": "fc2eec4b31c9484f8891323017acdc20bae816b3"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 87,
      "id": "MDExOlB1bGxSZXF1ZXN0NDIyMzcxOTQz",
      "title": "Authenticate connection IDs",
      "url": "https://github.com/quiclog/internet-drafts/pull/87",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Adds the transport parameters added in https://github.com/quicwg/base-drafts/pull/3499.",
      "createdAt": "2020-05-24T06:41:19Z",
      "updatedAt": "2020-07-22T07:48:29Z",
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "b0880b8a5ff7a9e856e321ff12465fd4be15cf5f",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "authenticate-connection-ids",
      "headRefOid": "3f2ea8e74fb7a36fb692f4d96951de1656f13102",
      "closedAt": "2020-07-22T07:48:29Z",
      "mergedAt": "2020-07-22T07:48:28Z",
      "mergedBy": "rmarx",
      "mergeCommit": {
        "oid": "09f447a17f183f02f8e0ada7cd0b834c3fd3963a"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 92,
      "id": "MDExOlB1bGxSZXF1ZXN0NDIzNTYwODU1",
      "title": "add a packet_dropped trigger for duplicate packets",
      "url": "https://github.com/quiclog/internet-drafts/pull/92",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "See https://quicwg.org/base-drafts/draft-ietf-quic-transport.html#name-packet-numbers.",
      "createdAt": "2020-05-27T02:03:51Z",
      "updatedAt": "2020-07-22T07:46:53Z",
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "d6433e1e93343aae970163d13549ad8361535330",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "packet-drop-duplicate",
      "headRefOid": "4278731985a41594c118fa093912a44e9d4e4ed4",
      "closedAt": "2020-07-22T07:46:53Z",
      "mergedAt": "2020-07-22T07:46:53Z",
      "mergedBy": "rmarx",
      "mergeCommit": {
        "oid": "c93c66d7dafdba32aa415e720a84643ab4835f86"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 93,
      "id": "MDExOlB1bGxSZXF1ZXN0NDMzMDA5OTUz",
      "title": "rename server_busy to connection_refused",
      "url": "https://github.com/quiclog/internet-drafts/pull/93",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2020-06-11T11:19:22Z",
      "updatedAt": "2020-06-11T15:06:15Z",
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "b0880b8a5ff7a9e856e321ff12465fd4be15cf5f",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "connection-refused",
      "headRefOid": "8d9475002611699c8381081fa90b95e51ea22f72",
      "closedAt": "2020-06-11T15:06:15Z",
      "mergedAt": "2020-06-11T15:06:15Z",
      "mergedBy": "rmarx",
      "mergeCommit": {
        "oid": "b8a2fd89f64b95e026a68dfcfb879d6c695e1514"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 98,
      "id": "MDExOlB1bGxSZXF1ZXN0NDQ2MDkxNDgx",
      "title": "max_ack_delay is a constant, and logged in the transport parameters_set",
      "url": "https://github.com/quiclog/internet-drafts/pull/98",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2020-07-08T08:47:07Z",
      "updatedAt": "2020-07-08T09:30:40Z",
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "b8a2fd89f64b95e026a68dfcfb879d6c695e1514",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "remove-max-ack-delay-from-metrics-update",
      "headRefOid": "408d46cd7076062970ad2898cba4079091629cff",
      "closedAt": "2020-07-08T09:30:40Z",
      "mergedAt": "2020-07-08T09:30:40Z",
      "mergedBy": "rmarx",
      "mergeCommit": {
        "oid": "11edf486b69a96720646b436789d33d4621a02e2"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 99,
      "id": "MDExOlB1bGxSZXF1ZXN0NDQ2MDkyNjE0",
      "title": "remove in_recovery from metrics_update",
      "url": "https://github.com/quiclog/internet-drafts/pull/99",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Fixes #96.",
      "createdAt": "2020-07-08T08:49:13Z",
      "updatedAt": "2020-07-08T09:29:59Z",
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "b8a2fd89f64b95e026a68dfcfb879d6c695e1514",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "remove-in-recovery-from-metrics-update",
      "headRefOid": "a348778bab9d89e583a49e4262d3ab5be20c896d",
      "closedAt": "2020-07-08T09:29:59Z",
      "mergedAt": "2020-07-08T09:29:59Z",
      "mergedBy": "rmarx",
      "mergeCommit": {
        "oid": "2b49fb613be515fdfde9eff5e1f66c455499fd35"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 103,
      "id": "MDExOlB1bGxSZXF1ZXN0NDQ2MTAzNDQ2",
      "title": "remove the UnknownFrame QUIC frame type",
      "url": "https://github.com/quiclog/internet-drafts/pull/103",
      "state": "CLOSED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Other than H3 frames, QUIC frames are not TLV encoded. It's therefore not possible to parse a frame type that your implementation doesn't understand. In fact, it's a protocol violation to even send such a frame.",
      "createdAt": "2020-07-08T09:08:43Z",
      "updatedAt": "2020-07-08T09:38:57Z",
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "b8a2fd89f64b95e026a68dfcfb879d6c695e1514",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "remove-unknown-frame",
      "headRefOid": "76957f1589918be259181e40d9a9b6b810ceb9b2",
      "closedAt": "2020-07-08T09:38:56Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "rmarx",
          "authorAssociation": "MEMBER",
          "body": "I'm not sure I follow you on this one.\r\n\r\nHere, the use case is for the receiver. They see an incoming frame with an unknown type, which, according to the spec, leads to a connection close:\r\n\r\n> An endpoint MUST treat the receipt of a frame of unknown type as a\r\n   connection error of type FRAME_ENCODING_ERROR.\r\n\r\nThis definition is to allow the endpoint to log more detailed info (or at least the raw value for manual dissection during debugging) about the unexpected frame before closing the connection. ",
          "createdAt": "2020-07-08T09:34:51Z",
          "updatedAt": "2020-07-08T09:34:51Z"
        },
        {
          "author": "marten-seemann",
          "authorAssociation": "CONTRIBUTOR",
          "body": "That makes sense. Must have missed that, sorry.",
          "createdAt": "2020-07-08T09:38:56Z",
          "updatedAt": "2020-07-08T09:38:56Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 108,
      "id": "MDExOlB1bGxSZXF1ZXN0NDQ5NDM2MjM3",
      "title": "add an \"invalid_initial\" trigger to the packet_dropped event",
      "url": "https://github.com/quiclog/internet-drafts/pull/108",
      "state": "MERGED",
      "author": "marten-seemann",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "There are two conditions an Initial packet has to fulfill:\r\n1. It's DCID must be >= 8 bytes long.\r\n2. The size of the datagram must be >= 1200.\r\n\r\nNone of the other triggers for the `packet_dropped` event seem appropriate for this.",
      "createdAt": "2020-07-15T12:02:04Z",
      "updatedAt": "2020-07-22T07:45:42Z",
      "baseRepository": "quiclog/internet-drafts",
      "baseRefName": "master",
      "baseRefOid": "fae11164cbfc22ff5aa6a763eae746e2e15abd00",
      "headRepository": "marten-seemann/internet-drafts",
      "headRefName": "add-invalid-initial-drop-trigger",
      "headRefOid": "0a8175658bf0aa7ef8d9ebd4754deb1a52c49844",
      "closedAt": "2020-07-22T07:45:42Z",
      "mergedAt": "2020-07-22T07:45:42Z",
      "mergedBy": "rmarx",
      "mergeCommit": {
        "oid": "d6433e1e93343aae970163d13549ad8361535330"
      },
      "comments": [],
      "reviews": []
    }
  ]
}